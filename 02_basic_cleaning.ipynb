{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2b1e2d",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing for YouTube Weight Stigma Research\n",
    "\n",
    "This notebook implements the data cleaning and preprocessing pipeline for YouTube comments collected in the weight stigma research study. The cleaning process ensures data quality and prepares the dataset for subsequent analysis.\n",
    "\n",
    "## Cleaning Steps Overview\n",
    "\n",
    "The data cleaning pipeline includes:\n",
    "\n",
    "1. **Text Length Filtering**: Remove extremely short and long comments\n",
    "2. **Emoji-Only Comment Removal**: Filter out comments containing only emojis\n",
    "3. **Data Structure Normalization**: Fix nested data structures\n",
    "4. **Duplicate Detection**: Remove dupli|cate comments using content-based hashing\n",
    "5. **Language Detection**: Identify and filter Portuguese content\n",
    "6. **Video Language Validation**: Ensure video titles are also in Portuguese\n",
    "7. **Data Export**: Save cleaned dataset for further analysis\n",
    "\n",
    "## Input Data\n",
    "\n",
    "- **Source**: Raw YouTube comments from `01_get_data_api.ipynb`\n",
    "- **Expected location**: `../data/raw/20250417_youtube_comments.parquet`\n",
    "- **Format**: Parquet file with comment metadata\n",
    "\n",
    "## Output Data\n",
    "\n",
    "- **Destination**: `../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet`\n",
    "- **Content**: Cleaned Portuguese comments ready for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddf902",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98aa8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries loaded and configuration set\n",
      "ğŸ“ Input file: ../data/raw/20250417_youtube_comments.parquet\n",
      "ğŸ“ Output file: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "ğŸ¯ Target language: pt\n",
      "âœ… Input file found: 20250417_youtube_comments.parquet\n",
      "ğŸ“Š File size: 127.5 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Configuration class for data cleaning\n",
    "class CleaningConfig:\n",
    "    \"\"\"Configuration for data cleaning pipeline.\"\"\"\n",
    "\n",
    "    # File paths\n",
    "    DATA_DIR = Path(\"../data\")\n",
    "    RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "    INTERMEDIATE_DATA_DIR = DATA_DIR / \"intermediate\"\n",
    "\n",
    "    # Expected input file (from data collection notebook)\n",
    "    INPUT_FILE = RAW_DATA_DIR / \"20250417_youtube_comments.parquet\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = INTERMEDIATE_DATA_DIR / \"20250417_youtube_comments_pt_cleaned1.parquet\"\n",
    "\n",
    "    # Cleaning parameters\n",
    "    MIN_TEXT_PERCENTILE = 0.1  # Remove bottom 10% shortest comments\n",
    "    MAX_TEXT_PERCENTILE = 0.999  # Remove top 0.1% longest comments\n",
    "\n",
    "    # Language detection\n",
    "    TARGET_LANGUAGE = \"pt\"  # Portuguese\n",
    "    BATCH_SIZE = 784  # For language detection processing\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        cls.INTERMEDIATE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create directories\n",
    "CleaningConfig.create_directories()\n",
    "\n",
    "print(\"âœ… Libraries loaded and configuration set\")\n",
    "print(f\"ğŸ“ Input file: {CleaningConfig.INPUT_FILE}\")\n",
    "print(f\"ğŸ“ Output file: {CleaningConfig.OUTPUT_FILE}\")\n",
    "print(f\"ğŸ¯ Target language: {CleaningConfig.TARGET_LANGUAGE}\")\n",
    "\n",
    "# Verify input file exists\n",
    "if CleaningConfig.INPUT_FILE.exists():\n",
    "    print(f\"âœ… Input file found: {CleaningConfig.INPUT_FILE.name}\")\n",
    "    file_size = CleaningConfig.INPUT_FILE.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"ğŸ“Š File size: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"âŒ Input file not found: {CleaningConfig.INPUT_FILE}\")\n",
    "    print(\"Please run the data collection notebook (01_get_data_api.ipynb) first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8abe6bf",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38690d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:02:57,750 - INFO - Loading data from: ../data/raw/20250417_youtube_comments.parquet\n",
      "2025-07-23 18:03:00,651 - INFO - âœ… Successfully loaded 593,509 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” RAW DATA OVERVIEW\n",
      "==================================================\n",
      "ğŸ“Š Dataset shape: (593509, 19)\n",
      "ğŸ“ Total comments: 593,509\n",
      "ğŸ¥ Unique videos: 1,850\n",
      "ğŸ‘¥ Unique authors: 512,630\n",
      "\n",
      "ğŸ“‹ Column Information:\n",
      "   video_id: object, 0 missing (0.0000)\n",
      "   channelId: object, 0 missing (0.0000)\n",
      "   videoId: object, 0 missing (0.0000)\n",
      "   textDisplay: object, 0 missing (0.0000)\n",
      "   textOriginal: object, 0 missing (0.0000)\n",
      "   authorDisplayName: object, 0 missing (0.0000)\n",
      "   authorProfileImageUrl: object, 0 missing (0.0000)\n",
      "   authorChannelUrl: object, 0 missing (0.0000)\n",
      "   authorChannelId: object, 0 missing (0.0000)\n",
      "   canRate: bool, 0 missing (0.0000)\n",
      "   viewerRating: object, 0 missing (0.0000)\n",
      "   likeCount: float64, 0 missing (0.0000)\n",
      "   publishedAt: datetime64[ns, UTC], 0 missing (0.0000)\n",
      "   updatedAt: datetime64[ns, UTC], 0 missing (0.0000)\n",
      "   author: object, 593,509 missing (100.0000)\n",
      "   comment: object, 593,509 missing (100.0000)\n",
      "   date: object, 593,509 missing (100.0000)\n",
      "   likes: object, 593,509 missing (100.0000)\n",
      "   video_title: object, 0 missing (0.0000)\n",
      "\n",
      "ğŸ’¾ Memory usage: 771.4 MB\n",
      "\n",
      "ğŸ“„ Sample data (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b0af8601-bd61-46de-8137-c14c7d6138fe",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o polÃ­cia chupando a buda do Romer",
         "Haahahahahahahahhahh o polÃ­cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "{'value': 'UCNhXx9ev5RtEiyGsVjMuTOA'}",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢",
         "ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢",
         "@SophialouiseSouza",
         "https://yt3.ggpht.com/rvxbmQyDslI2p4RzecqWzruSVGDGiCAX3IqSL2c1TKB0ht7en1fm2Y8wvkVSGYPbDga4bkkUDGo=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@SophialouiseSouza",
         "{'value': 'UCdQkFElArWumsJTS7FxUWcQ'}",
         "True",
         "none",
         "2.0",
         "2024-12-28 21:47:13+00:00",
         "2024-12-28 21:47:13+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons"
        ],
        [
         "2",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chupada tridimensional ğŸ˜",
         "Chupada tridimensional ğŸ˜",
         "@capivagiota",
         "https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWVh8m8DRQ63IdH3cGONpwB8Ws18JztpBasiZCV97pGCr5lINSpeA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@capivagiota",
         "{'value': 'UCaA27fdWlD7VZn6keN5Gb2w'}",
         "True",
         "none",
         "123.0",
         "2024-12-28 21:53:52+00:00",
         "2024-12-28 21:53:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o polÃ­cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o polÃ­cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>{'value': 'UCNhXx9ev5RtEiyGsVjMuTOA'}</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢</td>\n",
       "      <td>ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢</td>\n",
       "      <td>@SophialouiseSouza</td>\n",
       "      <td>https://yt3.ggpht.com/rvxbmQyDslI2p4RzecqWzruS...</td>\n",
       "      <td>http://www.youtube.com/@SophialouiseSouza</td>\n",
       "      <td>{'value': 'UCdQkFElArWumsJTS7FxUWcQ'}</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2024-12-28 21:47:13+00:00</td>\n",
       "      <td>2024-12-28 21:47:13+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chupada tridimensional ğŸ˜</td>\n",
       "      <td>Chupada tridimensional ğŸ˜</td>\n",
       "      <td>@capivagiota</td>\n",
       "      <td>https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...</td>\n",
       "      <td>http://www.youtube.com/@capivagiota</td>\n",
       "      <td>{'value': 'UCaA27fdWlD7VZn6keN5Gb2w'}</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "2  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o polÃ­cia chupando a buda...   \n",
       "1                   ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢   \n",
       "2                           Chupada tridimensional ğŸ˜   \n",
       "\n",
       "                                        textOriginal   authorDisplayName  \\\n",
       "0  Haahahahahahahahhahh o polÃ­cia chupando a buda...   @evelynsoares4467   \n",
       "1                   ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢  @SophialouiseSouza   \n",
       "2                           Chupada tridimensional ğŸ˜        @capivagiota   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/rvxbmQyDslI2p4RzecqWzruS...   \n",
       "2  https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...   \n",
       "\n",
       "                            authorChannelUrl  \\\n",
       "0   http://www.youtube.com/@evelynsoares4467   \n",
       "1  http://www.youtube.com/@SophialouiseSouza   \n",
       "2        http://www.youtube.com/@capivagiota   \n",
       "\n",
       "                         authorChannelId  canRate viewerRating  likeCount  \\\n",
       "0  {'value': 'UCNhXx9ev5RtEiyGsVjMuTOA'}     True         none        0.0   \n",
       "1  {'value': 'UCdQkFElArWumsJTS7FxUWcQ'}     True         none        2.0   \n",
       "2  {'value': 'UCaA27fdWlD7VZn6keN5Gb2w'}     True         none      123.0   \n",
       "\n",
       "                publishedAt                 updatedAt author comment  date  \\\n",
       "0 2024-12-28 21:38:37+00:00 2024-12-28 21:38:37+00:00   None    None  None   \n",
       "1 2024-12-28 21:47:13+00:00 2024-12-28 21:47:13+00:00   None    None  None   \n",
       "2 2024-12-28 21:53:52+00:00 2024-12-28 21:53:52+00:00   None    None  None   \n",
       "\n",
       "  likes                         video_title  \n",
       "0  None  Tony Gordo Ã© Incriminado #simpsons  \n",
       "1  None  Tony Gordo Ã© Incriminado #simpsons  \n",
       "2  None  Tony Gordo Ã© Incriminado #simpsons  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_explore_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw data and provide initial exploration.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the raw data file\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with raw data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        logger.info(f\"âœ… Successfully loaded {len(df):,} records\")\n",
    "\n",
    "        # Initial data exploration\n",
    "        print(\"ğŸ” RAW DATA OVERVIEW\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "        print(f\"ğŸ“ Total comments: {len(df):,}\")\n",
    "        print(f\"ğŸ¥ Unique videos: {df['video_id'].nunique():,}\")\n",
    "        print(f\"ğŸ‘¥ Unique authors: {df['authorDisplayName'].nunique():,}\")\n",
    "\n",
    "        # Check data types and missing values\n",
    "        print(f\"\\nğŸ“‹ Column Information:\")\n",
    "        for col in df.columns:\n",
    "            missing_count = df[col].isna().sum()\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            print(f\"   {col}: {df[col].dtype}, {missing_count:,} missing ({missing_pct:.4f})\")\n",
    "\n",
    "        # Memory usage\n",
    "        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "        print(f\"\\nğŸ’¾ Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Load the raw data\n",
    "df = load_and_explore_data(CleaningConfig.INPUT_FILE)\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nğŸ“„ Sample data (first 3 rows):\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44451ff1",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis and Filtering\n",
    "\n",
    "Remove extremely short and long comments to focus on meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689334fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:04,601 - INFO - Analyzing comment text lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ TEXT LENGTH ANALYSIS\n",
      "==================================================\n",
      "ğŸ“Š Text Length Statistics:\n",
      "   count: 593509 characters\n",
      "   mean: 68 characters\n",
      "   std: 174 characters\n",
      "   min: 0 characters\n",
      "   1%: 2 characters\n",
      "   2%: 3 characters\n",
      "   3%: 3 characters\n",
      "   5%: 5 characters\n",
      "   10%: 9 characters\n",
      "   20%: 16 characters\n",
      "   30%: 22 characters\n",
      "   40%: 30 characters\n",
      "   50%: 38 characters\n",
      "   60%: 49 characters\n",
      "   70%: 64 characters\n",
      "   80%: 88 characters\n",
      "   90%: 141 characters\n",
      "   95%: 213 characters\n",
      "   99%: 500 characters\n",
      "   99.9%: 1488 characters\n",
      "   max: 62137 characters\n",
      "\n",
      "ğŸ“ Sample Comments by Length:\n",
      "\n",
      "ğŸ”¹ Very short (â‰¤9 chars) - 66,181 comments:\n",
      "     1. 'O jogo' (6 chars)\n",
      "     2. 'ğŸ«µğŸ¤¨' (2 chars)\n",
      "     3. 'ğŸ’€' (1 chars)\n",
      "\n",
      "ğŸ”¹ Very long (â‰¥1488 chars) - 594 comments:\n",
      "     1. 'Ele tÃ¡ certo! Mulher diz : \n",
      "nÃ£o quero cara desempregado, nÃ£o gosto de homem sem barba, nÃ£o gosto de ...' (2617 chars)\n",
      "     2. 'Olha, SE a Maya aprendeu isso, ela propositadamente esqueceu todo o resto da aula, onde qualquer fac...' (1742 chars)\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_lengths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze comment text lengths and provide detailed statistics.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with textDisplay column\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with text length statistics\n",
    "    \"\"\"\n",
    "    logger.info(\"Analyzing comment text lengths\")\n",
    "\n",
    "    # Calculate text lengths\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis[\"len_text\"] = df_analysis[\"textDisplay\"].str.len()\n",
    "\n",
    "    print(\"ğŸ“ TEXT LENGTH ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Comprehensive statistics\n",
    "    length_stats = df_analysis[\"len_text\"].describe(percentiles=[0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999])\n",
    "\n",
    "    print(\"ğŸ“Š Text Length Statistics:\")\n",
    "    for stat, value in length_stats.items():\n",
    "        print(f\"   {stat}: {value:.0f} characters\")\n",
    "\n",
    "    # Show examples of different lengths\n",
    "    print(f\"\\nğŸ“ Sample Comments by Length:\")\n",
    "\n",
    "    # Very short comments\n",
    "    short_comments = df_analysis[df_analysis[\"len_text\"] <= length_stats[\"10%\"]]\n",
    "    if len(short_comments) > 0:\n",
    "        print(f\"\\nğŸ”¹ Very short (â‰¤{length_stats['10%']:.0f} chars) - {len(short_comments):,} comments:\")\n",
    "        for i, comment in enumerate(short_comments[\"textDisplay\"].head(3)):\n",
    "            print(f\"     {i + 1}. '{comment}' ({len(comment)} chars)\")\n",
    "\n",
    "    # Very long comments\n",
    "    long_comments = df_analysis[df_analysis[\"len_text\"] >= length_stats[\"99.9%\"]]\n",
    "    if len(long_comments) > 0:\n",
    "        print(f\"\\nğŸ”¹ Very long (â‰¥{length_stats['99.9%']:.0f} chars) - {len(long_comments):,} comments:\")\n",
    "        for i, comment in enumerate(long_comments[\"textDisplay\"].head(2)):\n",
    "            preview = comment[:100] + \"...\" if len(comment) > 100 else comment\n",
    "            print(f\"     {i + 1}. '{preview}' ({len(comment)} chars)\")\n",
    "\n",
    "    return df_analysis\n",
    "\n",
    "\n",
    "# Analyze text lengths\n",
    "df = analyze_text_lengths(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8a0c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:05,003 - INFO - Filtering comments by text length (keeping 10.0% to 99.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ TEXT LENGTH FILTERING\n",
      "==================================================\n",
      "ğŸ“Š Original dataset: 593,509 comments\n",
      "ğŸ“ Length thresholds:\n",
      "   Minimum (10.0% percentile): 9 characters\n",
      "   Maximum (99.9% percentile): 1488 characters\n",
      "\n",
      "ğŸ“Š Filtering Results:\n",
      "   ğŸ—‘ï¸ Removed 66,181 too short comments (11.2%)\n",
      "   ğŸ—‘ï¸ Removed 594 too long comments (0.1%)\n",
      "   âœ… Kept 526,734 comments (88.7%)\n",
      "   ğŸ“‰ Total removed: 66,775 comments (11.3%)\n"
     ]
    }
   ],
   "source": [
    "def filter_by_text_length(df: pd.DataFrame, min_percentile: float = 0.1, max_percentile: float = 0.999) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter comments by text length using percentile thresholds.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with len_text column\n",
    "        min_percentile: Minimum percentile threshold (remove bottom X%)\n",
    "        max_percentile: Maximum percentile threshold (remove top X%)\n",
    "\n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering comments by text length (keeping {min_percentile:.1%} to {max_percentile:.1%})\")\n",
    "\n",
    "    # Calculate thresholds\n",
    "    min_length = df[\"len_text\"].quantile(min_percentile)\n",
    "    max_length = df[\"len_text\"].quantile(max_percentile)\n",
    "\n",
    "    print(f\"ğŸ“ TEXT LENGTH FILTERING\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Original dataset: {len(df):,} comments\")\n",
    "    print(f\"ğŸ“ Length thresholds:\")\n",
    "    print(f\"   Minimum ({min_percentile:.1%} percentile): {min_length:.0f} characters\")\n",
    "    print(f\"   Maximum ({max_percentile:.1%} percentile): {max_length:.0f} characters\")\n",
    "\n",
    "    # Apply filters\n",
    "    original_count = len(df)\n",
    "\n",
    "    # Remove too short\n",
    "    too_short = len(df[df[\"len_text\"] <= min_length])\n",
    "    df_filtered = df[df[\"len_text\"] > min_length].copy()\n",
    "\n",
    "    # Remove too long\n",
    "    too_long = len(df_filtered[df_filtered[\"len_text\"] >= max_length])\n",
    "    df_filtered = df_filtered[df_filtered[\"len_text\"] < max_length].copy()\n",
    "\n",
    "    # Clean up and reset index\n",
    "    df_filtered = df_filtered.drop(columns=[\"len_text\"])\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Report results\n",
    "    final_count = len(df_filtered)\n",
    "    removed_count = original_count - final_count\n",
    "\n",
    "    print(f\"\\nğŸ“Š Filtering Results:\")\n",
    "    print(f\"   ğŸ—‘ï¸ Removed {too_short:,} too short comments ({too_short / original_count:.1%})\")\n",
    "    print(f\"   ğŸ—‘ï¸ Removed {too_long:,} too long comments ({too_long / original_count:.1%})\")\n",
    "    print(f\"   âœ… Kept {final_count:,} comments ({final_count / original_count:.1%})\")\n",
    "    print(f\"   ğŸ“‰ Total removed: {removed_count:,} comments ({removed_count / original_count:.1%})\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# Apply text length filtering\n",
    "df = filter_by_text_length(df, CleaningConfig.MIN_TEXT_PERCENTILE, CleaningConfig.MAX_TEXT_PERCENTILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b46cfd",
   "metadata": {},
   "source": [
    "## 4. Emoji-Only Comment Removal\n",
    "\n",
    "Filter out comments that contain only emojis or emoji-like characters to focus on textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e603f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:05,740 - INFO - Removing emoji-only comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜€ EMOJI-ONLY COMMENT REMOVAL\n",
      "==================================================\n",
      "ğŸ“Š Dataset before filtering: 526,734 comments\n",
      "ğŸ˜€ Emoji-only comments found: 6,076 (1.1535)\n",
      "ğŸ“ Text comments kept: 520,658 (98.8465)\n",
      "\n",
      "ğŸ“‹ Sample emoji-only comments:\n",
      "   1. 'ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢'\n",
      "   2. 'ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©ğŸ’©'\n",
      "   3. 'ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚'\n",
      "   4. 'ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£'\n",
      "   5. 'ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢ğŸ˜¢'\n"
     ]
    }
   ],
   "source": [
    "def remove_emoji_only_comments(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Remove comments that consist only of emojis or emoji-like characters.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with textDisplay column\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (filtered_dataframe, emoji_only_dataframe)\n",
    "    \"\"\"\n",
    "    logger.info(\"Removing emoji-only comments\")\n",
    "\n",
    "    def is_emoji_only(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if text contains only emojis and whitespace.\n",
    "\n",
    "        Covers multiple Unicode ranges for emojis and symbols.\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            return False\n",
    "\n",
    "        # Remove whitespace for analysis\n",
    "        text_clean = text.strip()\n",
    "        if not text_clean:\n",
    "            return False\n",
    "\n",
    "        # Check if all characters are emojis/symbols\n",
    "        emoji_ranges = [\n",
    "            (0x1F600, 0x1F64F),  # Emoticons\n",
    "            (0x1F300, 0x1F5FF),  # Misc Symbols and Pictographs\n",
    "            (0x1F680, 0x1F6FF),  # Transport and Map Symbols\n",
    "            (0x1F1E0, 0x1F1FF),  # Regional Indicator Symbols\n",
    "            (0x2600, 0x26FF),  # Misc Symbols\n",
    "            (0x2700, 0x27BF),  # Dingbats\n",
    "            (0xFE00, 0xFE0F),  # Variation Selectors\n",
    "            (0x1F900, 0x1F9FF),  # Supplemental Symbols and Pictographs\n",
    "        ]\n",
    "\n",
    "        for char in text_clean:\n",
    "            char_code = ord(char)\n",
    "            is_emoji = any(start <= char_code <= end for start, end in emoji_ranges)\n",
    "            is_whitespace = char.isspace()\n",
    "\n",
    "            if not (is_emoji or is_whitespace):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    print(\"ğŸ˜€ EMOJI-ONLY COMMENT REMOVAL\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Dataset before filtering: {len(df):,} comments\")\n",
    "\n",
    "    # Apply emoji detection\n",
    "    emoji_mask = df[\"textDisplay\"].apply(is_emoji_only)\n",
    "\n",
    "    # Separate emoji-only and text comments\n",
    "    df_emoji_only = df[emoji_mask].copy()\n",
    "    df_filtered = df[~emoji_mask].copy()\n",
    "\n",
    "    print(f\"ğŸ˜€ Emoji-only comments found: {len(df_emoji_only):,} ({len(df_emoji_only) / len(df) * 100:.4f})\")\n",
    "    print(f\"ğŸ“ Text comments kept: {len(df_filtered):,} ({len(df_filtered) / len(df) * 100:.4f})\")\n",
    "\n",
    "    # Show examples of emoji-only comments\n",
    "    if len(df_emoji_only) > 0:\n",
    "        print(f\"\\nğŸ“‹ Sample emoji-only comments:\")\n",
    "        samples = df_emoji_only[\"textDisplay\"].head(5)\n",
    "        for i, comment in enumerate(samples, 1):\n",
    "            print(f\"   {i}. '{comment}'\")\n",
    "\n",
    "    return df_filtered.reset_index(drop=True), df_emoji_only.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Remove emoji-only comments\n",
    "df, df_emoji = remove_emoji_only_comments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674a92f",
   "metadata": {},
   "source": [
    "## 5. Data Structure Normalization\n",
    "\n",
    "Fix nested data structures and normalize the dataset schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389c643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:07,580 - INFO - Normalizing data structures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ DATA STRUCTURE NORMALIZATION\n",
      "==================================================\n",
      "ğŸ“‹ Normalizing authorChannelId structure...\n",
      "   Found nested structure, extracting 'value' field\n",
      "\n",
      "ğŸ“Š Data types after normalization:\n",
      "   video_id: object\n",
      "   channelId: object\n",
      "   videoId: object\n",
      "   textDisplay: object\n",
      "   textOriginal: object\n",
      "   authorDisplayName: object\n",
      "   authorProfileImageUrl: object\n",
      "   authorChannelUrl: object\n",
      "   authorChannelId: object\n",
      "   canRate: bool\n",
      "   viewerRating: object\n",
      "   likeCount: float64\n",
      "   publishedAt: datetime64[ns, UTC]\n",
      "   updatedAt: datetime64[ns, UTC]\n",
      "   author: object\n",
      "   comment: object\n",
      "   date: object\n",
      "   likes: object\n",
      "   video_title: object\n",
      "\n",
      "ğŸ“„ Sample normalized data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d388df32-98e4-46df-95a5-501829ca6e65",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o polÃ­cia chupando a buda do Romer",
         "Haahahahahahahahhahh o polÃ­cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chupada tridimensional ğŸ˜",
         "Chupada tridimensional ğŸ˜",
         "@capivagiota",
         "https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWVh8m8DRQ63IdH3cGONpwB8Ws18JztpBasiZCV97pGCr5lINSpeA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@capivagiota",
         "UCaA27fdWlD7VZn6keN5Gb2w",
         "True",
         "none",
         "123.0",
         "2024-12-28 21:53:52+00:00",
         "2024-12-28 21:53:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o polÃ­cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o polÃ­cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chupada tridimensional ğŸ˜</td>\n",
       "      <td>Chupada tridimensional ğŸ˜</td>\n",
       "      <td>@capivagiota</td>\n",
       "      <td>https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...</td>\n",
       "      <td>http://www.youtube.com/@capivagiota</td>\n",
       "      <td>UCaA27fdWlD7VZn6keN5Gb2w</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o polÃ­cia chupando a buda...   \n",
       "1                           Chupada tridimensional ğŸ˜   \n",
       "\n",
       "                                        textOriginal  authorDisplayName  \\\n",
       "0  Haahahahahahahahhahh o polÃ­cia chupando a buda...  @evelynsoares4467   \n",
       "1                           Chupada tridimensional ğŸ˜       @capivagiota   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...   \n",
       "\n",
       "                           authorChannelUrl           authorChannelId  \\\n",
       "0  http://www.youtube.com/@evelynsoares4467  UCNhXx9ev5RtEiyGsVjMuTOA   \n",
       "1       http://www.youtube.com/@capivagiota  UCaA27fdWlD7VZn6keN5Gb2w   \n",
       "\n",
       "   canRate viewerRating  likeCount               publishedAt  \\\n",
       "0     True         none        0.0 2024-12-28 21:38:37+00:00   \n",
       "1     True         none      123.0 2024-12-28 21:53:52+00:00   \n",
       "\n",
       "                  updatedAt author comment  date likes  \\\n",
       "0 2024-12-28 21:38:37+00:00   None    None  None  None   \n",
       "1 2024-12-28 21:53:52+00:00   None    None  None  None   \n",
       "\n",
       "                          video_title  \n",
       "0  Tony Gordo Ã© Incriminado #simpsons  \n",
       "1  Tony Gordo Ã© Incriminado #simpsons  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_data_structures(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize nested data structures in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with nested structures\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with normalized structures\n",
    "    \"\"\"\n",
    "    logger.info(\"Normalizing data structures\")\n",
    "\n",
    "    print(\"ğŸ”§ DATA STRUCTURE NORMALIZATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    # Fix authorChannelId structure (if it's nested)\n",
    "    if \"authorChannelId\" in df_normalized.columns:\n",
    "        print(f\"ğŸ“‹ Normalizing authorChannelId structure...\")\n",
    "\n",
    "        # Check if it's a nested structure\n",
    "        sample_value = df_normalized[\"authorChannelId\"].dropna().iloc[0] if len(df_normalized[\"authorChannelId\"].dropna()) > 0 else None\n",
    "\n",
    "        if isinstance(sample_value, dict):\n",
    "            print(f\"   Found nested structure, extracting 'value' field\")\n",
    "            df_normalized[\"authorChannelId\"] = df_normalized[\"authorChannelId\"].apply(lambda x: x.get(\"value\") if isinstance(x, dict) and \"value\" in x else x)\n",
    "        else:\n",
    "            print(f\"   Structure already normalized\")\n",
    "\n",
    "    # Check for other nested structures\n",
    "    print(f\"\\nğŸ“Š Data types after normalization:\")\n",
    "    for col in df_normalized.columns:\n",
    "        dtype = df_normalized[col].dtype\n",
    "        print(f\"   {col}: {dtype}\")\n",
    "\n",
    "        # Check for remaining nested structures\n",
    "        if dtype == \"object\":\n",
    "            sample_vals = df_normalized[col].dropna().head(3)\n",
    "            has_dict = any(isinstance(val, dict) for val in sample_vals)\n",
    "            has_list = any(isinstance(val, list) for val in sample_vals)\n",
    "\n",
    "            if has_dict or has_list:\n",
    "                print(f\"      âš ï¸ Contains nested structures (dict: {has_dict}, list: {has_list})\")\n",
    "\n",
    "    # Display sample of normalized data\n",
    "    print(f\"\\nğŸ“„ Sample normalized data:\")\n",
    "    if len(df_normalized) > 0:\n",
    "        display(df_normalized.head(2))\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "\n",
    "# Normalize data structures\n",
    "df = normalize_data_structures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58edf4",
   "metadata": {},
   "source": [
    "## 6. Duplicate Detection and Removal\n",
    "\n",
    "Identify and remove duplicate comments using content-based hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbec88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:12,544 - INFO - Detecting and removing duplicate comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DUPLICATE DETECTION AND REMOVAL\n",
      "==================================================\n",
      "ğŸ“Š Dataset before deduplication: 520,658 comments\n",
      "ğŸ”’ Creating content-based hashes...\n",
      "ğŸ” Found 5,413 duplicate comments (0.0104)\n",
      "\n",
      "ğŸ“‹ Sample duplicate comments:\n",
      "\n",
      "   Duplicate group 1 (2 instances):\n",
      "      Text: 'VÃ­deo completo: https://youtu.be/hnetjD-gje4'\n",
      "      Videos: ['-6Qxw7CpQvQ', '-6Qxw7CpQvQ']\n",
      "\n",
      "   Duplicate group 2 (2 instances):\n",
      "      Text: 'Eu tb n pegaria gorda CaraÃ­, mas eu n ia ficar meia hora dando explicaÃ§Ã£o sobre isso nÃ© bixo kkkkkkk...'\n",
      "      Videos: ['-6Qxw7CpQvQ', '-6Qxw7CpQvQ']\n",
      "\n",
      "   Duplicate group 3 (2 instances):\n",
      "      Text: '\"tenho 18 anos e me cuido\" falou o cara que vocÃª olha de lado parece um quadrado do Minecraft, tem c...'\n",
      "      Videos: ['-6Qxw7CpQvQ', '-6Qxw7CpQvQ']\n",
      "\n",
      "ğŸ“Š Deduplication Results:\n",
      "   ğŸ—‘ï¸ Removed: 5,413 duplicate comments (0.0104)\n",
      "   âœ… Kept: 515,245 unique comments (0.9896)\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate comments using content-based hashing.\n",
    "\n",
    "    Creates a unique identifier based on video_id, comment text, and author URL\n",
    "    to identify and remove duplicate comments.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with comment data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    logger.info(\"Detecting and removing duplicate comments\")\n",
    "\n",
    "    print(\"ğŸ” DUPLICATE DETECTION AND REMOVAL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    original_count = len(df)\n",
    "    print(f\"ğŸ“Š Dataset before deduplication: {original_count:,} comments\")\n",
    "\n",
    "    df_dedup = df.copy()\n",
    "\n",
    "    # Create unique identifier for each comment\n",
    "    # Combine video_id + text + author to detect duplicates\n",
    "    def create_comment_hash(row):\n",
    "        \"\"\"Create a unique hash for a comment.\"\"\"\n",
    "        try:\n",
    "            # Handle potential missing values\n",
    "            video_id = str(row.get(\"video_id\", \"\"))\n",
    "            text = str(row.get(\"textDisplay\", \"\"))\n",
    "            author_url = str(row.get(\"authorChannelUrl\", \"\"))\n",
    "\n",
    "            # Create composite string\n",
    "            composite = video_id + text + author_url\n",
    "\n",
    "            # Generate MD5 hash\n",
    "            return hashlib.md5(composite.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating hash: {str(e)}\")\n",
    "            return str(hash(str(row)))  # Fallback hash\n",
    "\n",
    "    # Apply hashing\n",
    "    print(\"ğŸ”’ Creating content-based hashes...\")\n",
    "    df_dedup[\"comment_uuid\"] = df_dedup.apply(create_comment_hash, axis=1)\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicate_count = df_dedup[\"comment_uuid\"].duplicated().sum()\n",
    "    print(f\"ğŸ” Found {duplicate_count:,} duplicate comments ({duplicate_count / original_count:.4f})\")\n",
    "\n",
    "    if duplicate_count > 0:\n",
    "        # Show examples of duplicates\n",
    "        print(f\"\\nğŸ“‹ Sample duplicate comments:\")\n",
    "        duplicate_hashes = df_dedup[df_dedup[\"comment_uuid\"].duplicated(keep=False)][\"comment_uuid\"].unique()[:3]\n",
    "\n",
    "        for i, hash_val in enumerate(duplicate_hashes, 1):\n",
    "            duplicates = df_dedup[df_dedup[\"comment_uuid\"] == hash_val]\n",
    "            print(f\"\\n   Duplicate group {i} ({len(duplicates)} instances):\")\n",
    "            print(f\"      Text: '{duplicates.iloc[0]['textDisplay'][:100]}{'...' if len(duplicates.iloc[0]['textDisplay']) > 100 else ''}'\")\n",
    "            print(f\"      Videos: {duplicates['video_id'].tolist()}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_dedup = df_dedup.drop_duplicates(subset=[\"comment_uuid\"])\n",
    "    df_dedup = df_dedup.drop(columns=[\"comment_uuid\"])  # Remove helper column\n",
    "    df_dedup.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_count = len(df_dedup)\n",
    "    removed_count = original_count - final_count\n",
    "\n",
    "    print(f\"\\nğŸ“Š Deduplication Results:\")\n",
    "    print(f\"   ğŸ—‘ï¸ Removed: {removed_count:,} duplicate comments ({removed_count / original_count:.4f})\")\n",
    "    print(f\"   âœ… Kept: {final_count:,} unique comments ({final_count / original_count:.4f})\")\n",
    "\n",
    "    return df_dedup\n",
    "\n",
    "\n",
    "# Remove duplicates\n",
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17ed9d",
   "metadata": {},
   "source": [
    "## 7. Language Detection and Filtering\n",
    "\n",
    "Identify Portuguese comments and filter out content in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3880ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:22,369 - INFO - Setting up language detection pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ LANGUAGE DETECTION SETUP\n",
      "==================================================\n",
      "ğŸ“¦ Loading language detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Language detection model loaded successfully\n",
      "ğŸ§ª Test detection:\n",
      "   Text: 'OlÃ¡, como vocÃª estÃ¡?'\n",
      "   Detected: pt (confidence: 0.996)\n",
      "âœ… Portuguese detection working correctly\n"
     ]
    }
   ],
   "source": [
    "def setup_language_detection():\n",
    "    \"\"\"\n",
    "    Set up the language detection pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Language detection pipeline\n",
    "    \"\"\"\n",
    "    logger.info(\"Setting up language detection pipeline\")\n",
    "\n",
    "    try:\n",
    "        from transformers import pipeline\n",
    "\n",
    "        print(\"ğŸŒ LANGUAGE DETECTION SETUP\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"ğŸ“¦ Loading language detection model...\")\n",
    "\n",
    "        # Use XLM-RoBERTa for multilingual language detection\n",
    "        pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "            device=1,  # Use CPU (set to 0 for GPU if available)\n",
    "        )\n",
    "\n",
    "        print(\"âœ… Language detection model loaded successfully\")\n",
    "\n",
    "        # Test the pipeline\n",
    "        test_result = pipe(\"OlÃ¡, como vocÃª estÃ¡?\", top_k=1, truncation=True)\n",
    "        detected_lang = test_result[0][\"label\"]\n",
    "        confidence = test_result[0][\"score\"]\n",
    "\n",
    "        print(f\"ğŸ§ª Test detection:\")\n",
    "        print(f\"   Text: 'OlÃ¡, como vocÃª estÃ¡?'\")\n",
    "        print(f\"   Detected: {detected_lang} (confidence: {confidence:.3f})\")\n",
    "\n",
    "        if detected_lang == \"pt\":\n",
    "            print(\"âœ… Portuguese detection working correctly\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Unexpected result - please verify model\")\n",
    "\n",
    "        return pipe\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"âŒ Error: transformers library not installed\")\n",
    "        print(\"Please install with: pip install transformers torch\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up language detection: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Set up language detection\n",
    "language_detector = setup_language_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a366b315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:37,941 - INFO - Detecting languages for 515,245 comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ COMMENT LANGUAGE DETECTION\n",
      "==================================================\n",
      "ğŸ“Š Processing 515,245 comments in batches of 784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b512aa0b5234fbfb2d7bef52520b05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detecting languages:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Language Distribution:\n",
      "   pt: 240,028 (46.5852)\n",
      "   es: 154,493 (29.9844)\n",
      "   en: 47,658 (9.2496)\n",
      "   it: 22,683 (4.4024)\n",
      "   sw: 18,754 (3.6398)\n",
      "   hi: 11,844 (2.2987)\n",
      "   ur: 7,902 (1.5336)\n",
      "   tr: 3,022 (0.5865)\n",
      "   bg: 2,600 (0.5046)\n",
      "   ru: 1,428 (0.2771)\n",
      "\n",
      "ğŸŒ Sample non-Portuguese comments:\n",
      "\n",
      "   ES examples:\n",
      "      1. 'Ele esta correto'\n",
      "      2. 'Cara escroto sem comentarios'\n",
      "\n",
      "   EN examples:\n",
      "      1. 'Famoso sugar baby'\n",
      "      2. 'New money is so classless.'\n",
      "\n",
      "   IT examples:\n",
      "      1. 'Como se ele fosse magro KKKKKKKKKKKKKKKK'\n",
      "      2. 'Esse cara n tem 18'\n"
     ]
    }
   ],
   "source": [
    "def detect_comment_languages(df: pd.DataFrame, pipe, batch_size: int = 784) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect languages for all comments in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with textDisplay column\n",
    "        pipe: Language detection pipeline\n",
    "        batch_size: Number of texts to process in each batch\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with language column added\n",
    "    \"\"\"\n",
    "    logger.info(f\"Detecting languages for {len(df):,} comments\")\n",
    "\n",
    "    print(\"ğŸŒ COMMENT LANGUAGE DETECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Processing {len(df):,} comments in batches of {batch_size}\")\n",
    "\n",
    "    df_lang = df.copy()\n",
    "    language_results = []\n",
    "\n",
    "    try:\n",
    "        # Process in batches with progress bar\n",
    "        for batch_start in tqdm(range(0, len(df_lang), batch_size), desc=\"Detecting languages\"):\n",
    "            batch_end = min(batch_start + batch_size, len(df_lang))\n",
    "            batch_texts = df_lang.iloc[batch_start:batch_end][\"textDisplay\"].tolist()\n",
    "\n",
    "            # Clean texts (handle potential None values)\n",
    "            batch_texts = [str(text) if text is not None else \"\" for text in batch_texts]\n",
    "\n",
    "            # Detect languages for batch\n",
    "            batch_results = pipe(batch_texts, top_k=1, truncation=True, batch_size=min(batch_size, len(batch_texts)))\n",
    "\n",
    "            # Extract language labels\n",
    "            batch_languages = [result[0][\"label\"] for result in batch_results]\n",
    "            language_results.extend(batch_languages)\n",
    "\n",
    "        # Add language column\n",
    "        df_lang[\"language\"] = language_results\n",
    "\n",
    "        # Language distribution\n",
    "        lang_counts = df_lang[\"language\"].value_counts()\n",
    "        print(f\"\\nğŸ“Š Language Distribution:\")\n",
    "        for lang, count in lang_counts.head(10).items():\n",
    "            percentage = (count / len(df_lang)) * 100\n",
    "            print(f\"   {lang}: {count:,} ({percentage:.4f})\")\n",
    "\n",
    "        # Show examples of non-Portuguese content\n",
    "        non_pt = df_lang[df_lang[\"language\"] != \"pt\"]\n",
    "        if len(non_pt) > 0:\n",
    "            print(f\"\\nğŸŒ Sample non-Portuguese comments:\")\n",
    "            sample_languages = non_pt[\"language\"].value_counts().head(3)\n",
    "\n",
    "            for lang in sample_languages.index:\n",
    "                sample_comments = non_pt[non_pt[\"language\"] == lang][\"textDisplay\"].head(2)\n",
    "                print(f\"\\n   {lang.upper()} examples:\")\n",
    "                for i, comment in enumerate(sample_comments, 1):\n",
    "                    preview = comment[:100] + \"...\" if len(comment) > 100 else comment\n",
    "                    print(f\"      {i}. '{preview}'\")\n",
    "\n",
    "        return df_lang\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in language detection: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Detect languages for all comments\n",
    "df = detect_comment_languages(df, language_detector, CleaningConfig.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974d04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:37:38,025 - INFO - Filtering comments for language: pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‡§ğŸ‡· PORTUGUESE COMMENT FILTERING\n",
      "==================================================\n",
      "ğŸ“Š Dataset before language filtering: 515,245 comments\n",
      "ğŸ“ˆ Portuguese comments: 240,028 (0.4659)\n",
      "ğŸ“ˆ Other languages: 275,217 (0.5341)\n",
      "\n",
      "ğŸ“Š Filtering Results:\n",
      "   âœ… Kept: 240,028 Portuguese comments (0.4659)\n",
      "   ğŸ—‘ï¸ Removed: 275,217 non-Portuguese comments (0.5341)\n"
     ]
    }
   ],
   "source": [
    "def filter_portuguese_comments(df: pd.DataFrame, target_language: str = \"pt\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter comments to keep only the target language.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with language column\n",
    "        target_language: Language code to keep (default: \"pt\" for Portuguese)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with only target language comments\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering comments for language: {target_language}\")\n",
    "\n",
    "    print(f\"ğŸ‡§ğŸ‡· PORTUGUESE COMMENT FILTERING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    original_count = len(df)\n",
    "    print(f\"ğŸ“Š Dataset before language filtering: {original_count:,} comments\")\n",
    "\n",
    "    # Show distribution before filtering\n",
    "    lang_dist = df[\"language\"].value_counts()\n",
    "    pt_count = lang_dist.get(target_language, 0)\n",
    "    print(f\"ğŸ“ˆ Portuguese comments: {pt_count:,} ({pt_count / original_count:.4f})\")\n",
    "    print(f\"ğŸ“ˆ Other languages: {original_count - pt_count:,} ({(original_count - pt_count) / original_count:.4f})\")\n",
    "\n",
    "    # Filter for Portuguese comments\n",
    "    df_pt = df[df[\"language\"] == target_language].copy()\n",
    "    df_pt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_count = len(df_pt)\n",
    "    removed_count = original_count - final_count\n",
    "\n",
    "    print(f\"\\nğŸ“Š Filtering Results:\")\n",
    "    print(f\"   âœ… Kept: {final_count:,} Portuguese comments ({final_count / original_count:.4f})\")\n",
    "    print(f\"   ğŸ—‘ï¸ Removed: {removed_count:,} non-Portuguese comments ({removed_count / original_count:.4f})\")\n",
    "\n",
    "    return df_pt\n",
    "\n",
    "\n",
    "# Filter for Portuguese comments only\n",
    "df = filter_portuguese_comments(df, CleaningConfig.TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731644be",
   "metadata": {},
   "source": [
    "## 8. Video Language Va|lidation\n",
    "\n",
    "Verify that the video titles are also in Portuguese to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8071296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:38:38,014 - INFO - Validating video title languages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¥ VIDEO LANGUAGE VALIDATION\n",
      "==================================================\n",
      "ğŸ¬ Unique videos to check: 1,619\n",
      "ğŸŒ Detecting languages for video titles...\n",
      "\n",
      "ğŸ“Š Video Title Language Distribution:\n",
      "   pt: 1,204 videos (74.3669)\n",
      "   es: 196 videos (12.1062)\n",
      "   it: 117 videos (7.2267)\n",
      "   hi: 28 videos (1.7295)\n",
      "   en: 27 videos (1.6677)\n",
      "   sw: 18 videos (1.1118)\n",
      "   ur: 8 videos (0.4941)\n",
      "   de: 6 videos (0.3706)\n",
      "   tr: 5 videos (0.3088)\n",
      "   bg: 4 videos (0.2471)\n",
      "   nl: 4 videos (0.2471)\n",
      "   ru: 2 videos (0.1235)\n",
      "\n",
      "ğŸŒ Sample non-PT video titles:\n",
      "\n",
      "   ES examples:\n",
      "      1. -7bTGqiS34w: 'RaÃºl de Molina se entera que Lili Estefan perdiÃ³ a una conocida en el Jet Set | ...'\n",
      "      2. -v6VfrcNzB0: 'ğŸ’¥Era ACOSADO por GORDO pero un ISEKAI lo CONVIRTIÃ“ en TODO UN PAPUCHO | TEMPORAD...'\n",
      "\n",
      "   IT examples:\n",
      "      1. 1e9OzpsQg1k: 'Mi Gorda Bella capitulo 112'\n",
      "      2. 1u1gR2RJIzE: 'Mi papÃ¡ es un fan!'\n",
      "\n",
      "   HI examples:\n",
      "      1. 2SSDY47Ecrw: 'TU MADRE ESTÃ TAN GORDA'\n",
      "      2. 6jmg_XexH8U: 'SOU A BARBIE GORDA ğŸ’ª #roblox #barbie #fy #viraliza #robloxshorts #memeblox'\n",
      "\n",
      "ğŸ“Š Video Language Filtering:\n",
      "   âœ… PT videos: 1,204 (0.7437)\n",
      "   ğŸ—‘ï¸ Other languages: 415\n",
      "\n",
      "ğŸ“Š Comment Filtering by Video Language:\n",
      "   âœ… Comments kept: 191,946 (0.7997)\n",
      "   ğŸ—‘ï¸ Comments removed: 48,082 (0.2003)\n"
     ]
    }
   ],
   "source": [
    "def validate_video_languages(df: pd.DataFrame, pipe, target_language: str = \"pt\") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate that video titles are in the target language.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with video_id and video_title columns\n",
    "        pipe: Language detection pipeline\n",
    "        target_language: Expected language for videos\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (filtered_dataframe, list_of_target_language_video_ids)\n",
    "    \"\"\"\n",
    "    logger.info(\"Validating video title languages\")\n",
    "\n",
    "    print(\"ğŸ¥ VIDEO LANGUAGE VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get unique videos\n",
    "    videos = df[[\"video_id\", \"video_title\"]].drop_duplicates()\n",
    "    videos.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"ğŸ¬ Unique videos to check: {len(videos):,}\")\n",
    "\n",
    "    # Detect languages for video titles\n",
    "    print(f\"ğŸŒ Detecting languages for video titles...\")\n",
    "\n",
    "    video_titles = videos[\"video_title\"].tolist()\n",
    "    # Clean titles (handle potential None values)\n",
    "    video_titles = [str(title) if title is not None else \"\" for title in video_titles]\n",
    "\n",
    "    video_language_results = pipe(video_titles, top_k=1, truncation=True, batch_size=512)\n",
    "\n",
    "    videos[\"language\"] = [result[0][\"label\"] for result in video_language_results]\n",
    "\n",
    "    # Language distribution for videos\n",
    "    video_lang_dist = videos[\"language\"].value_counts()\n",
    "    print(f\"\\nğŸ“Š Video Title Language Distribution:\")\n",
    "    for lang, count in video_lang_dist.items():\n",
    "        percentage = (count / len(videos)) * 100\n",
    "        print(f\"   {lang}: {count:,} videos ({percentage:.4f})\")\n",
    "\n",
    "    # Show examples of non-target language videos\n",
    "    non_target_videos = videos[videos[\"language\"] != target_language]\n",
    "    if len(non_target_videos) > 0:\n",
    "        print(f\"\\nğŸŒ Sample non-{target_language.upper()} video titles:\")\n",
    "        for lang in non_target_videos[\"language\"].value_counts().head(3).index:\n",
    "            sample_videos = non_target_videos[non_target_videos[\"language\"] == lang]\n",
    "            print(f\"\\n   {lang.upper()} examples:\")\n",
    "            for i, (_, row) in enumerate(sample_videos.head(2).iterrows(), 1):\n",
    "                title_preview = row[\"video_title\"][:80] + \"...\" if len(str(row[\"video_title\"])) > 80 else row[\"video_title\"]\n",
    "                print(f\"      {i}. {row['video_id']}: '{title_preview}'\")\n",
    "\n",
    "    # Filter for target language videos\n",
    "    target_videos = videos[videos[\"language\"] == target_language]\n",
    "    target_video_ids = target_videos[\"video_id\"].tolist()\n",
    "\n",
    "    print(f\"\\nğŸ“Š Video Language Filtering:\")\n",
    "    print(f\"   âœ… {target_language.upper()} videos: {len(target_video_ids):,} ({len(target_video_ids) / len(videos):.4f})\")\n",
    "    print(f\"   ğŸ—‘ï¸ Other languages: {len(videos) - len(target_video_ids):,}\")\n",
    "\n",
    "    # Filter comments to only include target language videos\n",
    "    original_comment_count = len(df)\n",
    "    df_filtered = df[df[\"video_id\"].isin(target_video_ids)].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_comment_count = len(df_filtered)\n",
    "    removed_comments = original_comment_count - final_comment_count\n",
    "\n",
    "    print(f\"\\nğŸ“Š Comment Filtering by Video Language:\")\n",
    "    print(f\"   âœ… Comments kept: {final_comment_count:,} ({final_comment_count / original_comment_count:.4f})\")\n",
    "    print(f\"   ğŸ—‘ï¸ Comments removed: {removed_comments:,} ({removed_comments / original_comment_count:.4f})\")\n",
    "\n",
    "    return df_filtered, target_video_ids\n",
    "\n",
    "\n",
    "# Validate video languages and filter accordingly\n",
    "df, portuguese_video_ids = validate_video_languages(df, language_detector, CleaningConfig.TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d07ca",
   "metadata": {},
   "source": [
    "## 9. Final Data Summary and Export\n",
    "\n",
    "Review the cleaned dataset and export for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7853ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:39:38,016 - INFO - Generating final dataset summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š FINAL CLEANED DATASET SUMMARY\n",
      "==================================================\n",
      "ğŸ“ˆ Dataset Overview:\n",
      "   Total comments: 191,946\n",
      "   Unique videos: 1,204\n",
      "   Unique authors: 163,664\n",
      "   Dataset shape: (191946, 20)\n",
      "   Memory usage: 255.06 MB\n",
      "\n",
      "ğŸ“… Temporal Coverage:\n",
      "   Earliest comment: 2006-11-24 20:16:56+00:00\n",
      "   Latest comment: 2025-04-17 11:46:21+00:00\n",
      "   Time span: 6718 days\n",
      "\n",
      "ğŸ“ Comment Length Statistics:\n",
      "   Mean: 91.09 characters\n",
      "   Median: 55.00 characters\n",
      "   Std: 117.04 characters\n",
      "   Range: 10 - 1487 characters\n",
      "\n",
      "ğŸ¥ Most Commented Videos (Top 5):\n",
      "   1. S4pDpA-g7hE: 9,418 comments\n",
      "      'Pirado - JoÃ£o Gordo X Dado Dolabella'\n",
      "   2. 3JK3MbRhjUg: 5,927 comments\n",
      "      'ROMANTIZANDO A OBESIDADE.'\n",
      "   3. Fn54EKtAbTc: 5,873 comments\n",
      "      'Super OraÃ§Ã£o Contra Inveja, Olho Gordo e Mau Olhado'\n",
      "   4. YeGperfZ7QY: 5,742 comments\n",
      "      'COMECEI A TREINAR Eâ€¦. #viral #emagrecimento #youtube #obesid...'\n",
      "   5. Y0NWZKORge0: 3,752 comments\n",
      "      'Mulher mais gorda do Brasil pesa 360 quilos'\n",
      "\n",
      "ğŸ” Data Quality Assessment:\n",
      "   author: 191,946 missing (100.0000)\n",
      "   comment: 191,946 missing (100.0000)\n",
      "   date: 191,946 missing (100.0000)\n",
      "   likes: 191,946 missing (100.0000)\n",
      "\n",
      "ğŸŒ Language Distribution:\n",
      "   pt: 191,946 (100.0000)\n",
      "\n",
      "ğŸ“„ Final Cleaned Dataset (sample):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c31b5f0a-b04e-440f-b403-bf25727df1e5",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o polÃ­cia chupando a buda do Romer",
         "Haahahahahahahahhahh o polÃ­cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons",
         "pt"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chefe wigol deu um beijo grego no homer skksks",
         "Chefe wigol deu um beijo grego no homer skksks",
         "@MrLopess00",
         "https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO5lyhiApXfoR1zZJKsEBbZjpx-DQR3Rt_ajq96pk7PIl-26av=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@MrLopess00",
         "UCtrByOsq8kIDCQfSXfq3IKw",
         "True",
         "none",
         "447.0",
         "2024-12-29 02:00:55+00:00",
         "2024-12-29 02:00:55+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons",
         "pt"
        ],
        [
         "2",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem era Batedor de Carteiras ?",
         "Quem era Batedor de Carteiras ?",
         "@mateuss.santossilva5059",
         "https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_2IMVBe6FpUQbhn3VYstkZM0AGckW3SDAPtmlRlxw4Y9GOUlO=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@mateuss.santossilva5059",
         "UCIY2M7NurJ728_H4Cs5zmQA",
         "True",
         "none",
         "5.0",
         "2024-12-29 12:53:19+00:00",
         "2024-12-29 12:53:19+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons",
         "pt"
        ],
        [
         "3",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"GraÃ§as a deus que essa coisa estÃ¡ do nosso ladoğŸ˜¨\" kskskskksjsks",
         "\"GraÃ§as a deus que essa coisa estÃ¡ do nosso ladoğŸ˜¨\" kskskskksjsks",
         "@Ray._Ryan000",
         "https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX-ohqT8ffBd6vJTL5z8Rd2J8UK1CuBIftClOTcDqU_3wzrP2i7g=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Ray._Ryan000",
         "UCr5gdJ-I9wpcBXWdSR6T5iw",
         "True",
         "none",
         "1677.0",
         "2024-12-29 16:16:06+00:00",
         "2024-12-29 16:17:20+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons",
         "pt"
        ],
        [
         "4",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "ğŸ¤¨ tÃ¡ estranho isso",
         "ğŸ¤¨ tÃ¡ estranho isso",
         "@darkgacha5649",
         "https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOHyOkDBA0ibu_cM1LtoM-Ar3C_3fH8CJtikMtCSMztWt7nhu-tvA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@darkgacha5649",
         "UCZnl2qgkPiF-SYyoPmTbzBw",
         "True",
         "none",
         "0.0",
         "2024-12-29 18:03:52+00:00",
         "2024-12-29 18:03:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo Ã© Incriminado #simpsons",
         "pt"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o polÃ­cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o polÃ­cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chefe wigol deu um beijo grego no homer skksks</td>\n",
       "      <td>Chefe wigol deu um beijo grego no homer skksks</td>\n",
       "      <td>@MrLopess00</td>\n",
       "      <td>https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO...</td>\n",
       "      <td>http://www.youtube.com/@MrLopess00</td>\n",
       "      <td>UCtrByOsq8kIDCQfSXfq3IKw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>447.0</td>\n",
       "      <td>2024-12-29 02:00:55+00:00</td>\n",
       "      <td>2024-12-29 02:00:55+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Quem era Batedor de Carteiras ?</td>\n",
       "      <td>Quem era Batedor de Carteiras ?</td>\n",
       "      <td>@mateuss.santossilva5059</td>\n",
       "      <td>https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_...</td>\n",
       "      <td>http://www.youtube.com/@mateuss.santossilva5059</td>\n",
       "      <td>UCIY2M7NurJ728_H4Cs5zmQA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024-12-29 12:53:19+00:00</td>\n",
       "      <td>2024-12-29 12:53:19+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>\"GraÃ§as a deus que essa coisa estÃ¡ do nosso la...</td>\n",
       "      <td>\"GraÃ§as a deus que essa coisa estÃ¡ do nosso la...</td>\n",
       "      <td>@Ray._Ryan000</td>\n",
       "      <td>https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX...</td>\n",
       "      <td>http://www.youtube.com/@Ray._Ryan000</td>\n",
       "      <td>UCr5gdJ-I9wpcBXWdSR6T5iw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>2024-12-29 16:16:06+00:00</td>\n",
       "      <td>2024-12-29 16:17:20+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>ğŸ¤¨ tÃ¡ estranho isso</td>\n",
       "      <td>ğŸ¤¨ tÃ¡ estranho isso</td>\n",
       "      <td>@darkgacha5649</td>\n",
       "      <td>https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOH...</td>\n",
       "      <td>http://www.youtube.com/@darkgacha5649</td>\n",
       "      <td>UCZnl2qgkPiF-SYyoPmTbzBw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-29 18:03:52+00:00</td>\n",
       "      <td>2024-12-29 18:03:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo Ã© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "2  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "3  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "4  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o polÃ­cia chupando a buda...   \n",
       "1     Chefe wigol deu um beijo grego no homer skksks   \n",
       "2                    Quem era Batedor de Carteiras ?   \n",
       "3  \"GraÃ§as a deus que essa coisa estÃ¡ do nosso la...   \n",
       "4                                 ğŸ¤¨ tÃ¡ estranho isso   \n",
       "\n",
       "                                        textOriginal  \\\n",
       "0  Haahahahahahahahhahh o polÃ­cia chupando a buda...   \n",
       "1     Chefe wigol deu um beijo grego no homer skksks   \n",
       "2                    Quem era Batedor de Carteiras ?   \n",
       "3  \"GraÃ§as a deus que essa coisa estÃ¡ do nosso la...   \n",
       "4                                 ğŸ¤¨ tÃ¡ estranho isso   \n",
       "\n",
       "          authorDisplayName  \\\n",
       "0         @evelynsoares4467   \n",
       "1               @MrLopess00   \n",
       "2  @mateuss.santossilva5059   \n",
       "3             @Ray._Ryan000   \n",
       "4            @darkgacha5649   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO...   \n",
       "2  https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_...   \n",
       "3  https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX...   \n",
       "4  https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOH...   \n",
       "\n",
       "                                  authorChannelUrl           authorChannelId  \\\n",
       "0         http://www.youtube.com/@evelynsoares4467  UCNhXx9ev5RtEiyGsVjMuTOA   \n",
       "1               http://www.youtube.com/@MrLopess00  UCtrByOsq8kIDCQfSXfq3IKw   \n",
       "2  http://www.youtube.com/@mateuss.santossilva5059  UCIY2M7NurJ728_H4Cs5zmQA   \n",
       "3             http://www.youtube.com/@Ray._Ryan000  UCr5gdJ-I9wpcBXWdSR6T5iw   \n",
       "4            http://www.youtube.com/@darkgacha5649  UCZnl2qgkPiF-SYyoPmTbzBw   \n",
       "\n",
       "   canRate viewerRating  likeCount               publishedAt  \\\n",
       "0     True         none        0.0 2024-12-28 21:38:37+00:00   \n",
       "1     True         none      447.0 2024-12-29 02:00:55+00:00   \n",
       "2     True         none        5.0 2024-12-29 12:53:19+00:00   \n",
       "3     True         none     1677.0 2024-12-29 16:16:06+00:00   \n",
       "4     True         none        0.0 2024-12-29 18:03:52+00:00   \n",
       "\n",
       "                  updatedAt author comment  date likes  \\\n",
       "0 2024-12-28 21:38:37+00:00   None    None  None  None   \n",
       "1 2024-12-29 02:00:55+00:00   None    None  None  None   \n",
       "2 2024-12-29 12:53:19+00:00   None    None  None  None   \n",
       "3 2024-12-29 16:17:20+00:00   None    None  None  None   \n",
       "4 2024-12-29 18:03:52+00:00   None    None  None  None   \n",
       "\n",
       "                          video_title language  \n",
       "0  Tony Gordo Ã© Incriminado #simpsons       pt  \n",
       "1  Tony Gordo Ã© Incriminado #simpsons       pt  \n",
       "2  Tony Gordo Ã© Incriminado #simpsons       pt  \n",
       "3  Tony Gordo Ã© Incriminado #simpsons       pt  \n",
       "4  Tony Gordo Ã© Incriminado #simpsons       pt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_final_summary(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of the cleaned dataset.\n",
    "\n",
    "    Args:\n",
    "        df: Final cleaned DataFrame\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with summary statistics\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating final dataset summary\")\n",
    "\n",
    "    print(\"ğŸ“Š FINAL CLEANED DATASET SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    summary = {\n",
    "        \"total_comments\": len(df),\n",
    "        \"unique_videos\": df[\"video_id\"].nunique(),\n",
    "        \"unique_authors\": df[\"authorDisplayName\"].nunique(),\n",
    "        \"date_range\": {\"earliest\": df[\"publishedAt\"].min() if \"publishedAt\" in df.columns else None, \"latest\": df[\"publishedAt\"].max() if \"publishedAt\" in df.columns else None},\n",
    "        \"dataset_shape\": df.shape,\n",
    "        \"memory_usage_mb\": df.memory_usage(deep=True).sum() / (1024 * 1024),\n",
    "    }\n",
    "\n",
    "    print(f\"ğŸ“ˆ Dataset Overview:\")\n",
    "    print(f\"   Total comments: {summary['total_comments']:,}\")\n",
    "    print(f\"   Unique videos: {summary['unique_videos']:,}\")\n",
    "    print(f\"   Unique authors: {summary['unique_authors']:,}\")\n",
    "    print(f\"   Dataset shape: {summary['dataset_shape']}\")\n",
    "    print(f\"   Memory usage: {summary['memory_usage_mb']:.2f} MB\")\n",
    "\n",
    "    # Temporal coverage\n",
    "    if summary[\"date_range\"][\"earliest\"] and summary[\"date_range\"][\"latest\"]:\n",
    "        time_span = summary[\"date_range\"][\"latest\"] - summary[\"date_range\"][\"earliest\"]\n",
    "        print(f\"\\nğŸ“… Temporal Coverage:\")\n",
    "        print(f\"   Earliest comment: {summary['date_range']['earliest']}\")\n",
    "        print(f\"   Latest comment: {summary['date_range']['latest']}\")\n",
    "        print(f\"   Time span: {time_span.days} days\")\n",
    "        summary[\"time_span_days\"] = time_span.days\n",
    "\n",
    "    # Comment length statistics\n",
    "    if \"textDisplay\" in df.columns:\n",
    "        df_temp = df.copy()\n",
    "        df_temp[\"comment_length\"] = df_temp[\"textDisplay\"].str.len()\n",
    "        length_stats = df_temp[\"comment_length\"].describe()\n",
    "\n",
    "        print(f\"\\nğŸ“ Comment Length Statistics:\")\n",
    "        print(f\"   Mean: {length_stats['mean']:.2f} characters\")\n",
    "        print(f\"   Median: {length_stats['50%']:.2f} characters\")\n",
    "        print(f\"   Std: {length_stats['std']:.2f} characters\")\n",
    "        print(f\"   Range: {length_stats['min']:.0f} - {length_stats['max']:.0f} characters\")\n",
    "\n",
    "        summary[\"comment_length\"] = {\"mean\": length_stats[\"mean\"], \"median\": length_stats[\"50%\"], \"std\": length_stats[\"std\"], \"min\": length_stats[\"min\"], \"max\": length_stats[\"max\"]}\n",
    "\n",
    "    # Top videos by comment count\n",
    "    print(f\"\\nğŸ¥ Most Commented Videos (Top 5):\")\n",
    "    top_videos = df[\"video_id\"].value_counts().head()\n",
    "    for i, (video_id, count) in enumerate(top_videos.items(), 1):\n",
    "        title = df[df[\"video_id\"] == video_id][\"video_title\"].iloc[0] if \"video_title\" in df.columns else \"Unknown\"\n",
    "        title_preview = title[:60] + \"...\" if len(str(title)) > 60 else title\n",
    "        print(f\"   {i}. {video_id}: {count:,} comments\")\n",
    "        print(f\"      '{title_preview}'\")\n",
    "\n",
    "    summary[\"top_videos\"] = top_videos.to_dict()\n",
    "\n",
    "    # Data quality assessment\n",
    "    print(f\"\\nğŸ” Data Quality Assessment:\")\n",
    "    missing_stats = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        missing_stats[col] = {\"count\": missing_count, \"percentage\": missing_pct}\n",
    "        if missing_count > 0:\n",
    "            print(f\"   {col}: {missing_count:,} missing ({missing_pct:.4f})\")\n",
    "\n",
    "    if not any(stats[\"count\"] > 0 for stats in missing_stats.values()):\n",
    "        print(\"   âœ… No missing values detected\")\n",
    "\n",
    "    summary[\"missing_data\"] = missing_stats\n",
    "\n",
    "    # Language confirmation\n",
    "    if \"language\" in df.columns:\n",
    "        lang_dist = df[\"language\"].value_counts()\n",
    "        print(f\"\\nğŸŒ Language Distribution:\")\n",
    "        for lang, count in lang_dist.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"   {lang}: {count:,} ({pct:.4f})\")\n",
    "        summary[\"language_distribution\"] = lang_dist.to_dict()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Generate final summary\n",
    "final_summary = generate_final_summary(df)\n",
    "\n",
    "# Display final cleaned dataset\n",
    "print(f\"\\nğŸ“„ Final Cleaned Dataset (sample):\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f35d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:40:38,017 - INFO - Exporting cleaned data to: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ EXPORTING CLEANED DATA\n",
      "==================================================\n",
      "âœ… Dataset exported successfully\n",
      "ğŸ“ File: 20250417_youtube_comments_pt_cleaned1.parquet\n",
      "ğŸ“Š Size: 47.1 MB\n",
      "ğŸ“ˆ Records: 191,946\n",
      "ğŸ“‹ Metadata exported: 20250417_youtube_comments_pt_cleaned1.json\n",
      "ğŸ’¾ CSV backup created: 20250417_youtube_comments_pt_cleaned1.csv (104.9 MB)\n",
      "\n",
      "âœ… Export completed successfully!\n",
      "ğŸ“ Files created:\n",
      "   â€¢ 20250417_youtube_comments_pt_cleaned1.parquet (main dataset)\n",
      "   â€¢ 20250417_youtube_comments_pt_cleaned1.json (metadata)\n",
      "   â€¢ 20250417_youtube_comments_pt_cleaned1.csv (CSV backup)\n",
      "\n",
      "ğŸ‰ DATA CLEANING PIPELINE COMPLETED!\n",
      "ğŸ“Š Final dataset: 191,946 Portuguese comments from 1,204 videos\n",
      "ğŸ“ Output file: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "â–¶ï¸ Ready for analysis in subsequent notebooks!\n"
     ]
    }
   ],
   "source": [
    "def export_cleaned_data(df: pd.DataFrame, output_path: Path, summary: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Export the cleaned dataset with comprehensive metadata.\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned DataFrame to export\n",
    "        output_path: Path for the output file\n",
    "        summary: Summary statistics dictionary\n",
    "    \"\"\"\n",
    "    logger.info(f\"Exporting cleaned data to: {output_path}\")\n",
    "\n",
    "    print(\"ğŸ’¾ EXPORTING CLEANED DATA\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Export main dataset\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "        print(f\"âœ… Dataset exported successfully\")\n",
    "        print(f\"ğŸ“ File: {output_path.name}\")\n",
    "        print(f\"ğŸ“Š Size: {file_size_mb:.1f} MB\")\n",
    "        print(f\"ğŸ“ˆ Records: {len(df):,}\")\n",
    "\n",
    "        # Export summary metadata\n",
    "        metadata_path = output_path.with_suffix(\".json\")\n",
    "\n",
    "        # Prepare metadata for JSON serialization\n",
    "        export_metadata = {\n",
    "            \"export_info\": {\"timestamp\": pd.Timestamp.now().isoformat(), \"file_name\": output_path.name, \"file_size_mb\": file_size_mb, \"format\": \"parquet\"},\n",
    "            \"dataset_summary\": summary,\n",
    "            \"cleaning_pipeline\": {\n",
    "                \"steps\": [\"Text length filtering (10th to 99.9th percentile)\", \"Emoji-only comment removal\", \"Data structure normalization\", \"Duplicate detection and removal (content-based hashing)\", \"Language detection and Portuguese filtering\", \"Video language validation\"],\n",
    "                \"target_language\": CleaningConfig.TARGET_LANGUAGE,\n",
    "                \"min_text_percentile\": CleaningConfig.MIN_TEXT_PERCENTILE,\n",
    "                \"max_text_percentile\": CleaningConfig.MAX_TEXT_PERCENTILE,\n",
    "            },\n",
    "            \"column_descriptions\": {\n",
    "                \"video_id\": \"YouTube video identifier\",\n",
    "                \"textDisplay\": \"Comment text content (Portuguese only)\",\n",
    "                \"authorDisplayName\": \"Comment author display name\",\n",
    "                \"authorChannelUrl\": \"Author channel URL\",\n",
    "                \"authorChannelId\": \"Author channel identifier\",\n",
    "                \"publishedAt\": \"Comment publication timestamp\",\n",
    "                \"updatedAt\": \"Comment last update timestamp\",\n",
    "                \"likeCount\": \"Number of likes on the comment\",\n",
    "                \"video_title\": \"Title of the YouTube video (Portuguese)\",\n",
    "                \"language\": \"Detected language code (pt for Portuguese)\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Handle datetime serialization\n",
    "        import json\n",
    "\n",
    "        def json_serializer(obj):\n",
    "            if isinstance(obj, pd.Timestamp):\n",
    "                return obj.isoformat()\n",
    "            elif hasattr(obj, \"isoformat\"):\n",
    "                return obj.isoformat()\n",
    "            return str(obj)\n",
    "\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_metadata, f, indent=2, ensure_ascii=False, default=json_serializer)\n",
    "\n",
    "        print(f\"ğŸ“‹ Metadata exported: {metadata_path.name}\")\n",
    "\n",
    "        # Export CSV backup\n",
    "        csv_path = output_path.with_suffix(\".csv\")\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "        csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "        print(f\"ğŸ’¾ CSV backup created: {csv_path.name} ({csv_size_mb:.1f} MB)\")\n",
    "\n",
    "        print(f\"\\nâœ… Export completed successfully!\")\n",
    "        print(f\"ğŸ“ Files created:\")\n",
    "        print(f\"   â€¢ {output_path.name} (main dataset)\")\n",
    "        print(f\"   â€¢ {metadata_path.name} (metadata)\")\n",
    "        print(f\"   â€¢ {csv_path.name} (CSV backup)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Export the cleaned dataset\n",
    "export_cleaned_data(df, CleaningConfig.OUTPUT_FILE, final_summary)\n",
    "\n",
    "print(f\"\\nğŸ‰ DATA CLEANING PIPELINE COMPLETED!\")\n",
    "print(f\"ğŸ“Š Final dataset: {len(df):,} Portuguese comments from {df['video_id'].nunique():,} videos\")\n",
    "print(f\"ğŸ“ Output file: {CleaningConfig.OUTPUT_FILE}\")\n",
    "print(f\"â–¶ï¸ Ready for analysis in subsequent notebooks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaffc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e170f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_youtube_weight_stigma_1e733e1bf2b6c34f6bcb8483dce2a479",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
