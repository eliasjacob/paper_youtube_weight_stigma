{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2b1e2d",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing for YouTube Weight Stigma Research\n",
    "\n",
    "This notebook implements the data cleaning and preprocessing pipeline for YouTube comments collected in the weight stigma research study. The cleaning process ensures data quality and prepares the dataset for subsequent analysis.\n",
    "\n",
    "## Cleaning Steps Overview\n",
    "\n",
    "The data cleaning pipeline includes:\n",
    "\n",
    "1. **Text Length Filtering**: Remove extremely short and long comments\n",
    "2. **Emoji-Only Comment Removal**: Filter out comments containing only emojis\n",
    "3. **Data Structure Normalization**: Fix nested data structures\n",
    "4. **Duplicate Detection**: Remove dupli|cate comments using content-based hashing\n",
    "5. **Language Detection**: Identify and filter Portuguese content\n",
    "6. **Video Language Validation**: Ensure video titles are also in Portuguese\n",
    "7. **Data Export**: Save cleaned dataset for further analysis\n",
    "\n",
    "## Input Data\n",
    "\n",
    "- **Source**: Raw YouTube comments from `01_get_data_api.ipynb`\n",
    "- **Expected location**: `../data/raw/20250417_youtube_comments.parquet`\n",
    "- **Format**: Parquet file with comment metadata\n",
    "\n",
    "## Output Data\n",
    "\n",
    "- **Destination**: `../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet`\n",
    "- **Content**: Cleaned Portuguese comments ready for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddf902",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a98aa8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded and configuration set\n",
      "üìÅ Input file: ../data/raw/20250417_youtube_comments.parquet\n",
      "üìÅ Output file: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "üéØ Target language: pt\n",
      "‚úÖ Input file found: 20250417_youtube_comments.parquet\n",
      "üìä File size: 127.5 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Configuration class for data cleaning\n",
    "class CleaningConfig:\n",
    "    \"\"\"Configuration for data cleaning pipeline.\"\"\"\n",
    "\n",
    "    # File paths\n",
    "    DATA_DIR = Path(\"../data\")\n",
    "    RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "    INTERMEDIATE_DATA_DIR = DATA_DIR / \"intermediate\"\n",
    "\n",
    "    # Expected input file (from data collection notebook)\n",
    "    INPUT_FILE = RAW_DATA_DIR / \"20250417_youtube_comments.parquet\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = INTERMEDIATE_DATA_DIR / \"20250417_youtube_comments_pt_cleaned1.parquet\"\n",
    "\n",
    "    # Cleaning parameters\n",
    "    MIN_TEXT_PERCENTILE = 0.1  # Remove bottom 10% shortest comments\n",
    "    MAX_TEXT_PERCENTILE = 0.999  # Remove top 0.1% longest comments\n",
    "\n",
    "    # Language detection\n",
    "    TARGET_LANGUAGE = \"pt\"  # Portuguese\n",
    "    BATCH_SIZE = 784  # For language detection processing\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        cls.INTERMEDIATE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create directories\n",
    "CleaningConfig.create_directories()\n",
    "\n",
    "print(\"‚úÖ Libraries loaded and configuration set\")\n",
    "print(f\"üìÅ Input file: {CleaningConfig.INPUT_FILE}\")\n",
    "print(f\"üìÅ Output file: {CleaningConfig.OUTPUT_FILE}\")\n",
    "print(f\"üéØ Target language: {CleaningConfig.TARGET_LANGUAGE}\")\n",
    "\n",
    "# Verify input file exists\n",
    "if CleaningConfig.INPUT_FILE.exists():\n",
    "    print(f\"‚úÖ Input file found: {CleaningConfig.INPUT_FILE.name}\")\n",
    "    file_size = CleaningConfig.INPUT_FILE.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"üìä File size: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Input file not found: {CleaningConfig.INPUT_FILE}\")\n",
    "    print(\"Please run the data collection notebook (01_get_data_api.ipynb) first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8abe6bf",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38690d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:02:57,750 - INFO - Loading data from: ../data/raw/20250417_youtube_comments.parquet\n",
      "2025-07-23 18:03:00,651 - INFO - ‚úÖ Successfully loaded 593,509 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç RAW DATA OVERVIEW\n",
      "==================================================\n",
      "üìä Dataset shape: (593509, 19)\n",
      "üìù Total comments: 593,509\n",
      "üé• Unique videos: 1,850\n",
      "üë• Unique authors: 512,630\n",
      "\n",
      "üìã Column Information:\n",
      "   video_id: object, 0 missing (0.0000)\n",
      "   channelId: object, 0 missing (0.0000)\n",
      "   videoId: object, 0 missing (0.0000)\n",
      "   textDisplay: object, 0 missing (0.0000)\n",
      "   textOriginal: object, 0 missing (0.0000)\n",
      "   authorDisplayName: object, 0 missing (0.0000)\n",
      "   authorProfileImageUrl: object, 0 missing (0.0000)\n",
      "   authorChannelUrl: object, 0 missing (0.0000)\n",
      "   authorChannelId: object, 0 missing (0.0000)\n",
      "   canRate: bool, 0 missing (0.0000)\n",
      "   viewerRating: object, 0 missing (0.0000)\n",
      "   likeCount: float64, 0 missing (0.0000)\n",
      "   publishedAt: datetime64[ns, UTC], 0 missing (0.0000)\n",
      "   updatedAt: datetime64[ns, UTC], 0 missing (0.0000)\n",
      "   author: object, 593,509 missing (100.0000)\n",
      "   comment: object, 593,509 missing (100.0000)\n",
      "   date: object, 593,509 missing (100.0000)\n",
      "   likes: object, 593,509 missing (100.0000)\n",
      "   video_title: object, 0 missing (0.0000)\n",
      "\n",
      "üíæ Memory usage: 771.4 MB\n",
      "\n",
      "üìÑ Sample data (first 3 rows):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b0af8601-bd61-46de-8137-c14c7d6138fe",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "{'value': 'UCNhXx9ev5RtEiyGsVjMuTOA'}",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢",
         "üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢",
         "@SophialouiseSouza",
         "https://yt3.ggpht.com/rvxbmQyDslI2p4RzecqWzruSVGDGiCAX3IqSL2c1TKB0ht7en1fm2Y8wvkVSGYPbDga4bkkUDGo=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@SophialouiseSouza",
         "{'value': 'UCdQkFElArWumsJTS7FxUWcQ'}",
         "True",
         "none",
         "2.0",
         "2024-12-28 21:47:13+00:00",
         "2024-12-28 21:47:13+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons"
        ],
        [
         "2",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chupada tridimensional üòé",
         "Chupada tridimensional üòé",
         "@capivagiota",
         "https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWVh8m8DRQ63IdH3cGONpwB8Ws18JztpBasiZCV97pGCr5lINSpeA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@capivagiota",
         "{'value': 'UCaA27fdWlD7VZn6keN5Gb2w'}",
         "True",
         "none",
         "123.0",
         "2024-12-28 21:53:52+00:00",
         "2024-12-28 21:53:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>{'value': 'UCNhXx9ev5RtEiyGsVjMuTOA'}</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢</td>\n",
       "      <td>üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢</td>\n",
       "      <td>@SophialouiseSouza</td>\n",
       "      <td>https://yt3.ggpht.com/rvxbmQyDslI2p4RzecqWzruS...</td>\n",
       "      <td>http://www.youtube.com/@SophialouiseSouza</td>\n",
       "      <td>{'value': 'UCdQkFElArWumsJTS7FxUWcQ'}</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2024-12-28 21:47:13+00:00</td>\n",
       "      <td>2024-12-28 21:47:13+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chupada tridimensional üòé</td>\n",
       "      <td>Chupada tridimensional üòé</td>\n",
       "      <td>@capivagiota</td>\n",
       "      <td>https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...</td>\n",
       "      <td>http://www.youtube.com/@capivagiota</td>\n",
       "      <td>{'value': 'UCaA27fdWlD7VZn6keN5Gb2w'}</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "2  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1                   üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢   \n",
       "2                           Chupada tridimensional üòé   \n",
       "\n",
       "                                        textOriginal   authorDisplayName  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   @evelynsoares4467   \n",
       "1                   üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢  @SophialouiseSouza   \n",
       "2                           Chupada tridimensional üòé        @capivagiota   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/rvxbmQyDslI2p4RzecqWzruS...   \n",
       "2  https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...   \n",
       "\n",
       "                            authorChannelUrl  \\\n",
       "0   http://www.youtube.com/@evelynsoares4467   \n",
       "1  http://www.youtube.com/@SophialouiseSouza   \n",
       "2        http://www.youtube.com/@capivagiota   \n",
       "\n",
       "                         authorChannelId  canRate viewerRating  likeCount  \\\n",
       "0  {'value': 'UCNhXx9ev5RtEiyGsVjMuTOA'}     True         none        0.0   \n",
       "1  {'value': 'UCdQkFElArWumsJTS7FxUWcQ'}     True         none        2.0   \n",
       "2  {'value': 'UCaA27fdWlD7VZn6keN5Gb2w'}     True         none      123.0   \n",
       "\n",
       "                publishedAt                 updatedAt author comment  date  \\\n",
       "0 2024-12-28 21:38:37+00:00 2024-12-28 21:38:37+00:00   None    None  None   \n",
       "1 2024-12-28 21:47:13+00:00 2024-12-28 21:47:13+00:00   None    None  None   \n",
       "2 2024-12-28 21:53:52+00:00 2024-12-28 21:53:52+00:00   None    None  None   \n",
       "\n",
       "  likes                         video_title  \n",
       "0  None  Tony Gordo √© Incriminado #simpsons  \n",
       "1  None  Tony Gordo √© Incriminado #simpsons  \n",
       "2  None  Tony Gordo √© Incriminado #simpsons  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_explore_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load raw data and provide initial exploration.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the raw data file\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with raw data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading data from: {file_path}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        logger.info(f\"‚úÖ Successfully loaded {len(df):,} records\")\n",
    "\n",
    "        # Initial data exploration\n",
    "        print(\"üîç RAW DATA OVERVIEW\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìä Dataset shape: {df.shape}\")\n",
    "        print(f\"üìù Total comments: {len(df):,}\")\n",
    "        print(f\"üé• Unique videos: {df['video_id'].nunique():,}\")\n",
    "        print(f\"üë• Unique authors: {df['authorDisplayName'].nunique():,}\")\n",
    "\n",
    "        # Check data types and missing values\n",
    "        print(f\"\\nüìã Column Information:\")\n",
    "        for col in df.columns:\n",
    "            missing_count = df[col].isna().sum()\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            print(f\"   {col}: {df[col].dtype}, {missing_count:,} missing ({missing_pct:.4f})\")\n",
    "\n",
    "        # Memory usage\n",
    "        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "        print(f\"\\nüíæ Memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Load the raw data\n",
    "df = load_and_explore_data(CleaningConfig.INPUT_FILE)\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nüìÑ Sample data (first 3 rows):\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44451ff1",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis and Filtering\n",
    "\n",
    "Remove extremely short and long comments to focus on meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689334fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:04,601 - INFO - Analyzing comment text lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè TEXT LENGTH ANALYSIS\n",
      "==================================================\n",
      "üìä Text Length Statistics:\n",
      "   count: 593509 characters\n",
      "   mean: 68 characters\n",
      "   std: 174 characters\n",
      "   min: 0 characters\n",
      "   1%: 2 characters\n",
      "   2%: 3 characters\n",
      "   3%: 3 characters\n",
      "   5%: 5 characters\n",
      "   10%: 9 characters\n",
      "   20%: 16 characters\n",
      "   30%: 22 characters\n",
      "   40%: 30 characters\n",
      "   50%: 38 characters\n",
      "   60%: 49 characters\n",
      "   70%: 64 characters\n",
      "   80%: 88 characters\n",
      "   90%: 141 characters\n",
      "   95%: 213 characters\n",
      "   99%: 500 characters\n",
      "   99.9%: 1488 characters\n",
      "   max: 62137 characters\n",
      "\n",
      "üìù Sample Comments by Length:\n",
      "\n",
      "üîπ Very short (‚â§9 chars) - 66,181 comments:\n",
      "     1. 'O jogo' (6 chars)\n",
      "     2. 'ü´µü§®' (2 chars)\n",
      "     3. 'üíÄ' (1 chars)\n",
      "\n",
      "üîπ Very long (‚â•1488 chars) - 594 comments:\n",
      "     1. 'Ele t√° certo! Mulher diz : \n",
      "n√£o quero cara desempregado, n√£o gosto de homem sem barba, n√£o gosto de ...' (2617 chars)\n",
      "     2. 'Olha, SE a Maya aprendeu isso, ela propositadamente esqueceu todo o resto da aula, onde qualquer fac...' (1742 chars)\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_lengths(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze comment text lengths and provide detailed statistics.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with textDisplay column\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with text length statistics\n",
    "    \"\"\"\n",
    "    logger.info(\"Analyzing comment text lengths\")\n",
    "\n",
    "    # Calculate text lengths\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis[\"len_text\"] = df_analysis[\"textDisplay\"].str.len()\n",
    "\n",
    "    print(\"üìè TEXT LENGTH ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Comprehensive statistics\n",
    "    length_stats = df_analysis[\"len_text\"].describe(percentiles=[0.01, 0.02, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999])\n",
    "\n",
    "    print(\"üìä Text Length Statistics:\")\n",
    "    for stat, value in length_stats.items():\n",
    "        print(f\"   {stat}: {value:.0f} characters\")\n",
    "\n",
    "    # Show examples of different lengths\n",
    "    print(f\"\\nüìù Sample Comments by Length:\")\n",
    "\n",
    "    # Very short comments\n",
    "    short_comments = df_analysis[df_analysis[\"len_text\"] <= length_stats[\"10%\"]]\n",
    "    if len(short_comments) > 0:\n",
    "        print(f\"\\nüîπ Very short (‚â§{length_stats['10%']:.0f} chars) - {len(short_comments):,} comments:\")\n",
    "        for i, comment in enumerate(short_comments[\"textDisplay\"].head(3)):\n",
    "            print(f\"     {i + 1}. '{comment}' ({len(comment)} chars)\")\n",
    "\n",
    "    # Very long comments\n",
    "    long_comments = df_analysis[df_analysis[\"len_text\"] >= length_stats[\"99.9%\"]]\n",
    "    if len(long_comments) > 0:\n",
    "        print(f\"\\nüîπ Very long (‚â•{length_stats['99.9%']:.0f} chars) - {len(long_comments):,} comments:\")\n",
    "        for i, comment in enumerate(long_comments[\"textDisplay\"].head(2)):\n",
    "            preview = comment[:100] + \"...\" if len(comment) > 100 else comment\n",
    "            print(f\"     {i + 1}. '{preview}' ({len(comment)} chars)\")\n",
    "\n",
    "    return df_analysis\n",
    "\n",
    "\n",
    "# Analyze text lengths\n",
    "df = analyze_text_lengths(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8a0c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:05,003 - INFO - Filtering comments by text length (keeping 10.0% to 99.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè TEXT LENGTH FILTERING\n",
      "==================================================\n",
      "üìä Original dataset: 593,509 comments\n",
      "üìê Length thresholds:\n",
      "   Minimum (10.0% percentile): 9 characters\n",
      "   Maximum (99.9% percentile): 1488 characters\n",
      "\n",
      "üìä Filtering Results:\n",
      "   üóëÔ∏è Removed 66,181 too short comments (11.2%)\n",
      "   üóëÔ∏è Removed 594 too long comments (0.1%)\n",
      "   ‚úÖ Kept 526,734 comments (88.7%)\n",
      "   üìâ Total removed: 66,775 comments (11.3%)\n"
     ]
    }
   ],
   "source": [
    "def filter_by_text_length(df: pd.DataFrame, min_percentile: float = 0.1, max_percentile: float = 0.999) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter comments by text length using percentile thresholds.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with len_text column\n",
    "        min_percentile: Minimum percentile threshold (remove bottom X%)\n",
    "        max_percentile: Maximum percentile threshold (remove top X%)\n",
    "\n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering comments by text length (keeping {min_percentile:.1%} to {max_percentile:.1%})\")\n",
    "\n",
    "    # Calculate thresholds\n",
    "    min_length = df[\"len_text\"].quantile(min_percentile)\n",
    "    max_length = df[\"len_text\"].quantile(max_percentile)\n",
    "\n",
    "    print(f\"üìè TEXT LENGTH FILTERING\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Original dataset: {len(df):,} comments\")\n",
    "    print(f\"üìê Length thresholds:\")\n",
    "    print(f\"   Minimum ({min_percentile:.1%} percentile): {min_length:.0f} characters\")\n",
    "    print(f\"   Maximum ({max_percentile:.1%} percentile): {max_length:.0f} characters\")\n",
    "\n",
    "    # Apply filters\n",
    "    original_count = len(df)\n",
    "\n",
    "    # Remove too short\n",
    "    too_short = len(df[df[\"len_text\"] <= min_length])\n",
    "    df_filtered = df[df[\"len_text\"] > min_length].copy()\n",
    "\n",
    "    # Remove too long\n",
    "    too_long = len(df_filtered[df_filtered[\"len_text\"] >= max_length])\n",
    "    df_filtered = df_filtered[df_filtered[\"len_text\"] < max_length].copy()\n",
    "\n",
    "    # Clean up and reset index\n",
    "    df_filtered = df_filtered.drop(columns=[\"len_text\"])\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Report results\n",
    "    final_count = len(df_filtered)\n",
    "    removed_count = original_count - final_count\n",
    "\n",
    "    print(f\"\\nüìä Filtering Results:\")\n",
    "    print(f\"   üóëÔ∏è Removed {too_short:,} too short comments ({too_short / original_count:.1%})\")\n",
    "    print(f\"   üóëÔ∏è Removed {too_long:,} too long comments ({too_long / original_count:.1%})\")\n",
    "    print(f\"   ‚úÖ Kept {final_count:,} comments ({final_count / original_count:.1%})\")\n",
    "    print(f\"   üìâ Total removed: {removed_count:,} comments ({removed_count / original_count:.1%})\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# Apply text length filtering\n",
    "df = filter_by_text_length(df, CleaningConfig.MIN_TEXT_PERCENTILE, CleaningConfig.MAX_TEXT_PERCENTILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b46cfd",
   "metadata": {},
   "source": [
    "## 4. Emoji-Only Comment Removal\n",
    "\n",
    "Filter out comments that contain only emojis or emoji-like characters to focus on textual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e603f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:05,740 - INFO - Removing emoji-only comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÄ EMOJI-ONLY COMMENT REMOVAL\n",
      "==================================================\n",
      "üìä Dataset before filtering: 526,734 comments\n",
      "üòÄ Emoji-only comments found: 6,076 (1.1535)\n",
      "üìù Text comments kept: 520,658 (98.8465)\n",
      "\n",
      "üìã Sample emoji-only comments:\n",
      "   1. 'üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢'\n",
      "   2. 'üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©üí©'\n",
      "   3. 'üòÇüòÇüòÇüòÇüòÇüòÇüòÇüòÇüòÇüòÇ'\n",
      "   4. 'ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£ü§£'\n",
      "   5. 'üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢üò¢'\n"
     ]
    }
   ],
   "source": [
    "def remove_emoji_only_comments(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Remove comments that consist only of emojis or emoji-like characters.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with textDisplay column\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (filtered_dataframe, emoji_only_dataframe)\n",
    "    \"\"\"\n",
    "    logger.info(\"Removing emoji-only comments\")\n",
    "\n",
    "    def is_emoji_only(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if text contains only emojis and whitespace.\n",
    "\n",
    "        Covers multiple Unicode ranges for emojis and symbols.\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text.strip() == \"\":\n",
    "            return False\n",
    "\n",
    "        # Remove whitespace for analysis\n",
    "        text_clean = text.strip()\n",
    "        if not text_clean:\n",
    "            return False\n",
    "\n",
    "        # Check if all characters are emojis/symbols\n",
    "        emoji_ranges = [\n",
    "            (0x1F600, 0x1F64F),  # Emoticons\n",
    "            (0x1F300, 0x1F5FF),  # Misc Symbols and Pictographs\n",
    "            (0x1F680, 0x1F6FF),  # Transport and Map Symbols\n",
    "            (0x1F1E0, 0x1F1FF),  # Regional Indicator Symbols\n",
    "            (0x2600, 0x26FF),  # Misc Symbols\n",
    "            (0x2700, 0x27BF),  # Dingbats\n",
    "            (0xFE00, 0xFE0F),  # Variation Selectors\n",
    "            (0x1F900, 0x1F9FF),  # Supplemental Symbols and Pictographs\n",
    "        ]\n",
    "\n",
    "        for char in text_clean:\n",
    "            char_code = ord(char)\n",
    "            is_emoji = any(start <= char_code <= end for start, end in emoji_ranges)\n",
    "            is_whitespace = char.isspace()\n",
    "\n",
    "            if not (is_emoji or is_whitespace):\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    print(\"üòÄ EMOJI-ONLY COMMENT REMOVAL\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Dataset before filtering: {len(df):,} comments\")\n",
    "\n",
    "    # Apply emoji detection\n",
    "    emoji_mask = df[\"textDisplay\"].apply(is_emoji_only)\n",
    "\n",
    "    # Separate emoji-only and text comments\n",
    "    df_emoji_only = df[emoji_mask].copy()\n",
    "    df_filtered = df[~emoji_mask].copy()\n",
    "\n",
    "    print(f\"üòÄ Emoji-only comments found: {len(df_emoji_only):,} ({len(df_emoji_only) / len(df) * 100:.4f})\")\n",
    "    print(f\"üìù Text comments kept: {len(df_filtered):,} ({len(df_filtered) / len(df) * 100:.4f})\")\n",
    "\n",
    "    # Show examples of emoji-only comments\n",
    "    if len(df_emoji_only) > 0:\n",
    "        print(f\"\\nüìã Sample emoji-only comments:\")\n",
    "        samples = df_emoji_only[\"textDisplay\"].head(5)\n",
    "        for i, comment in enumerate(samples, 1):\n",
    "            print(f\"   {i}. '{comment}'\")\n",
    "\n",
    "    return df_filtered.reset_index(drop=True), df_emoji_only.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Remove emoji-only comments\n",
    "df, df_emoji = remove_emoji_only_comments(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b674a92f",
   "metadata": {},
   "source": [
    "## 5. Data Structure Normalization\n",
    "\n",
    "Fix nested data structures and normalize the dataset schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389c643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:07,580 - INFO - Normalizing data structures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß DATA STRUCTURE NORMALIZATION\n",
      "==================================================\n",
      "üìã Normalizing authorChannelId structure...\n",
      "   Found nested structure, extracting 'value' field\n",
      "\n",
      "üìä Data types after normalization:\n",
      "   video_id: object\n",
      "   channelId: object\n",
      "   videoId: object\n",
      "   textDisplay: object\n",
      "   textOriginal: object\n",
      "   authorDisplayName: object\n",
      "   authorProfileImageUrl: object\n",
      "   authorChannelUrl: object\n",
      "   authorChannelId: object\n",
      "   canRate: bool\n",
      "   viewerRating: object\n",
      "   likeCount: float64\n",
      "   publishedAt: datetime64[ns, UTC]\n",
      "   updatedAt: datetime64[ns, UTC]\n",
      "   author: object\n",
      "   comment: object\n",
      "   date: object\n",
      "   likes: object\n",
      "   video_title: object\n",
      "\n",
      "üìÑ Sample normalized data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "d388df32-98e4-46df-95a5-501829ca6e65",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chupada tridimensional üòé",
         "Chupada tridimensional üòé",
         "@capivagiota",
         "https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWVh8m8DRQ63IdH3cGONpwB8Ws18JztpBasiZCV97pGCr5lINSpeA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@capivagiota",
         "UCaA27fdWlD7VZn6keN5Gb2w",
         "True",
         "none",
         "123.0",
         "2024-12-28 21:53:52+00:00",
         "2024-12-28 21:53:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chupada tridimensional üòé</td>\n",
       "      <td>Chupada tridimensional üòé</td>\n",
       "      <td>@capivagiota</td>\n",
       "      <td>https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...</td>\n",
       "      <td>http://www.youtube.com/@capivagiota</td>\n",
       "      <td>UCaA27fdWlD7VZn6keN5Gb2w</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>2024-12-28 21:53:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1                           Chupada tridimensional üòé   \n",
       "\n",
       "                                        textOriginal  authorDisplayName  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...  @evelynsoares4467   \n",
       "1                           Chupada tridimensional üòé       @capivagiota   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/Rk5mblie0y248pftSyVfoqWV...   \n",
       "\n",
       "                           authorChannelUrl           authorChannelId  \\\n",
       "0  http://www.youtube.com/@evelynsoares4467  UCNhXx9ev5RtEiyGsVjMuTOA   \n",
       "1       http://www.youtube.com/@capivagiota  UCaA27fdWlD7VZn6keN5Gb2w   \n",
       "\n",
       "   canRate viewerRating  likeCount               publishedAt  \\\n",
       "0     True         none        0.0 2024-12-28 21:38:37+00:00   \n",
       "1     True         none      123.0 2024-12-28 21:53:52+00:00   \n",
       "\n",
       "                  updatedAt author comment  date likes  \\\n",
       "0 2024-12-28 21:38:37+00:00   None    None  None  None   \n",
       "1 2024-12-28 21:53:52+00:00   None    None  None  None   \n",
       "\n",
       "                          video_title  \n",
       "0  Tony Gordo √© Incriminado #simpsons  \n",
       "1  Tony Gordo √© Incriminado #simpsons  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_data_structures(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize nested data structures in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with nested structures\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with normalized structures\n",
    "    \"\"\"\n",
    "    logger.info(\"Normalizing data structures\")\n",
    "\n",
    "    print(\"üîß DATA STRUCTURE NORMALIZATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    df_normalized = df.copy()\n",
    "\n",
    "    # Fix authorChannelId structure (if it's nested)\n",
    "    if \"authorChannelId\" in df_normalized.columns:\n",
    "        print(f\"üìã Normalizing authorChannelId structure...\")\n",
    "\n",
    "        # Check if it's a nested structure\n",
    "        sample_value = df_normalized[\"authorChannelId\"].dropna().iloc[0] if len(df_normalized[\"authorChannelId\"].dropna()) > 0 else None\n",
    "\n",
    "        if isinstance(sample_value, dict):\n",
    "            print(f\"   Found nested structure, extracting 'value' field\")\n",
    "            df_normalized[\"authorChannelId\"] = df_normalized[\"authorChannelId\"].apply(lambda x: x.get(\"value\") if isinstance(x, dict) and \"value\" in x else x)\n",
    "        else:\n",
    "            print(f\"   Structure already normalized\")\n",
    "\n",
    "    # Check for other nested structures\n",
    "    print(f\"\\nüìä Data types after normalization:\")\n",
    "    for col in df_normalized.columns:\n",
    "        dtype = df_normalized[col].dtype\n",
    "        print(f\"   {col}: {dtype}\")\n",
    "\n",
    "        # Check for remaining nested structures\n",
    "        if dtype == \"object\":\n",
    "            sample_vals = df_normalized[col].dropna().head(3)\n",
    "            has_dict = any(isinstance(val, dict) for val in sample_vals)\n",
    "            has_list = any(isinstance(val, list) for val in sample_vals)\n",
    "\n",
    "            if has_dict or has_list:\n",
    "                print(f\"      ‚ö†Ô∏è Contains nested structures (dict: {has_dict}, list: {has_list})\")\n",
    "\n",
    "    # Display sample of normalized data\n",
    "    print(f\"\\nüìÑ Sample normalized data:\")\n",
    "    if len(df_normalized) > 0:\n",
    "        display(df_normalized.head(2))\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "\n",
    "# Normalize data structures\n",
    "df = normalize_data_structures(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58edf4",
   "metadata": {},
   "source": [
    "## 6. Duplicate Detection and Removal\n",
    "\n",
    "Identify and remove duplicate comments using content-based hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bbec88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:12,544 - INFO - Detecting and removing duplicate comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç DUPLICATE DETECTION AND REMOVAL\n",
      "==================================================\n",
      "üìä Dataset before deduplication: 520,658 comments\n",
      "üîí Creating content-based hashes...\n",
      "üîç Found 5,413 duplicate comments (0.0104)\n",
      "\n",
      "üìã Sample duplicate comments:\n",
      "\n",
      "   Duplicate group 1 (2 instances):\n",
      "      Text: 'V√≠deo completo: https://youtu.be/hnetjD-gje4'\n",
      "      Videos: ['-6Qxw7CpQvQ', '-6Qxw7CpQvQ']\n",
      "\n",
      "   Duplicate group 2 (2 instances):\n",
      "      Text: 'Eu tb n pegaria gorda Cara√≠, mas eu n ia ficar meia hora dando explica√ß√£o sobre isso n√© bixo kkkkkkk...'\n",
      "      Videos: ['-6Qxw7CpQvQ', '-6Qxw7CpQvQ']\n",
      "\n",
      "   Duplicate group 3 (2 instances):\n",
      "      Text: '\"tenho 18 anos e me cuido\" falou o cara que voc√™ olha de lado parece um quadrado do Minecraft, tem c...'\n",
      "      Videos: ['-6Qxw7CpQvQ', '-6Qxw7CpQvQ']\n",
      "\n",
      "üìä Deduplication Results:\n",
      "   üóëÔ∏è Removed: 5,413 duplicate comments (0.0104)\n",
      "   ‚úÖ Kept: 515,245 unique comments (0.9896)\n"
     ]
    }
   ],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate comments using content-based hashing.\n",
    "\n",
    "    Creates a unique identifier based on video_id, comment text, and author URL\n",
    "    to identify and remove duplicate comments.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with comment data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with duplicates removed\n",
    "    \"\"\"\n",
    "    logger.info(\"Detecting and removing duplicate comments\")\n",
    "\n",
    "    print(\"üîç DUPLICATE DETECTION AND REMOVAL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    original_count = len(df)\n",
    "    print(f\"üìä Dataset before deduplication: {original_count:,} comments\")\n",
    "\n",
    "    df_dedup = df.copy()\n",
    "\n",
    "    # Create unique identifier for each comment\n",
    "    # Combine video_id + text + author to detect duplicates\n",
    "    def create_comment_hash(row):\n",
    "        \"\"\"Create a unique hash for a comment.\"\"\"\n",
    "        try:\n",
    "            # Handle potential missing values\n",
    "            video_id = str(row.get(\"video_id\", \"\"))\n",
    "            text = str(row.get(\"textDisplay\", \"\"))\n",
    "            author_url = str(row.get(\"authorChannelUrl\", \"\"))\n",
    "\n",
    "            # Create composite string\n",
    "            composite = video_id + text + author_url\n",
    "\n",
    "            # Generate MD5 hash\n",
    "            return hashlib.md5(composite.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating hash: {str(e)}\")\n",
    "            return str(hash(str(row)))  # Fallback hash\n",
    "\n",
    "    # Apply hashing\n",
    "    print(\"üîí Creating content-based hashes...\")\n",
    "    df_dedup[\"comment_uuid\"] = df_dedup.apply(create_comment_hash, axis=1)\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicate_count = df_dedup[\"comment_uuid\"].duplicated().sum()\n",
    "    print(f\"üîç Found {duplicate_count:,} duplicate comments ({duplicate_count / original_count:.4f})\")\n",
    "\n",
    "    if duplicate_count > 0:\n",
    "        # Show examples of duplicates\n",
    "        print(f\"\\nüìã Sample duplicate comments:\")\n",
    "        duplicate_hashes = df_dedup[df_dedup[\"comment_uuid\"].duplicated(keep=False)][\"comment_uuid\"].unique()[:3]\n",
    "\n",
    "        for i, hash_val in enumerate(duplicate_hashes, 1):\n",
    "            duplicates = df_dedup[df_dedup[\"comment_uuid\"] == hash_val]\n",
    "            print(f\"\\n   Duplicate group {i} ({len(duplicates)} instances):\")\n",
    "            print(f\"      Text: '{duplicates.iloc[0]['textDisplay'][:100]}{'...' if len(duplicates.iloc[0]['textDisplay']) > 100 else ''}'\")\n",
    "            print(f\"      Videos: {duplicates['video_id'].tolist()}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    df_dedup = df_dedup.drop_duplicates(subset=[\"comment_uuid\"])\n",
    "    df_dedup = df_dedup.drop(columns=[\"comment_uuid\"])  # Remove helper column\n",
    "    df_dedup.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_count = len(df_dedup)\n",
    "    removed_count = original_count - final_count\n",
    "\n",
    "    print(f\"\\nüìä Deduplication Results:\")\n",
    "    print(f\"   üóëÔ∏è Removed: {removed_count:,} duplicate comments ({removed_count / original_count:.4f})\")\n",
    "    print(f\"   ‚úÖ Kept: {final_count:,} unique comments ({final_count / original_count:.4f})\")\n",
    "\n",
    "    return df_dedup\n",
    "\n",
    "\n",
    "# Remove duplicates\n",
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17ed9d",
   "metadata": {},
   "source": [
    "## 7. Language Detection and Filtering\n",
    "\n",
    "Identify Portuguese comments and filter out content in other languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3880ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:22,369 - INFO - Setting up language detection pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê LANGUAGE DETECTION SETUP\n",
      "==================================================\n",
      "üì¶ Loading language detection model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Language detection model loaded successfully\n",
      "üß™ Test detection:\n",
      "   Text: 'Ol√°, como voc√™ est√°?'\n",
      "   Detected: pt (confidence: 0.996)\n",
      "‚úÖ Portuguese detection working correctly\n"
     ]
    }
   ],
   "source": [
    "def setup_language_detection():\n",
    "    \"\"\"\n",
    "    Set up the language detection pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Language detection pipeline\n",
    "    \"\"\"\n",
    "    logger.info(\"Setting up language detection pipeline\")\n",
    "\n",
    "    try:\n",
    "        from transformers import pipeline\n",
    "\n",
    "        print(\"üåê LANGUAGE DETECTION SETUP\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üì¶ Loading language detection model...\")\n",
    "\n",
    "        # Use XLM-RoBERTa for multilingual language detection\n",
    "        pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=\"papluca/xlm-roberta-base-language-detection\",\n",
    "            device=1,  # Use CPU (set to 0 for GPU if available)\n",
    "        )\n",
    "\n",
    "        print(\"‚úÖ Language detection model loaded successfully\")\n",
    "\n",
    "        # Test the pipeline\n",
    "        test_result = pipe(\"Ol√°, como voc√™ est√°?\", top_k=1, truncation=True)\n",
    "        detected_lang = test_result[0][\"label\"]\n",
    "        confidence = test_result[0][\"score\"]\n",
    "\n",
    "        print(f\"üß™ Test detection:\")\n",
    "        print(f\"   Text: 'Ol√°, como voc√™ est√°?'\")\n",
    "        print(f\"   Detected: {detected_lang} (confidence: {confidence:.3f})\")\n",
    "\n",
    "        if detected_lang == \"pt\":\n",
    "            print(\"‚úÖ Portuguese detection working correctly\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Unexpected result - please verify model\")\n",
    "\n",
    "        return pipe\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Error: transformers library not installed\")\n",
    "        print(\"Please install with: pip install transformers torch\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up language detection: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Set up language detection\n",
    "language_detector = setup_language_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a366b315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:03:37,941 - INFO - Detecting languages for 515,245 comments\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê COMMENT LANGUAGE DETECTION\n",
      "==================================================\n",
      "üìä Processing 515,245 comments in batches of 784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b512aa0b5234fbfb2d7bef52520b05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detecting languages:   0%|          | 0/658 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Language Distribution:\n",
      "   pt: 240,028 (46.5852)\n",
      "   es: 154,493 (29.9844)\n",
      "   en: 47,658 (9.2496)\n",
      "   it: 22,683 (4.4024)\n",
      "   sw: 18,754 (3.6398)\n",
      "   hi: 11,844 (2.2987)\n",
      "   ur: 7,902 (1.5336)\n",
      "   tr: 3,022 (0.5865)\n",
      "   bg: 2,600 (0.5046)\n",
      "   ru: 1,428 (0.2771)\n",
      "\n",
      "üåç Sample non-Portuguese comments:\n",
      "\n",
      "   ES examples:\n",
      "      1. 'Ele esta correto'\n",
      "      2. 'Cara escroto sem comentarios'\n",
      "\n",
      "   EN examples:\n",
      "      1. 'Famoso sugar baby'\n",
      "      2. 'New money is so classless.'\n",
      "\n",
      "   IT examples:\n",
      "      1. 'Como se ele fosse magro KKKKKKKKKKKKKKKK'\n",
      "      2. 'Esse cara n tem 18'\n"
     ]
    }
   ],
   "source": [
    "def detect_comment_languages(df: pd.DataFrame, pipe, batch_size: int = 784) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect languages for all comments in the dataset.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with textDisplay column\n",
    "        pipe: Language detection pipeline\n",
    "        batch_size: Number of texts to process in each batch\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with language column added\n",
    "    \"\"\"\n",
    "    logger.info(f\"Detecting languages for {len(df):,} comments\")\n",
    "\n",
    "    print(\"üåê COMMENT LANGUAGE DETECTION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìä Processing {len(df):,} comments in batches of {batch_size}\")\n",
    "\n",
    "    df_lang = df.copy()\n",
    "    language_results = []\n",
    "\n",
    "    try:\n",
    "        # Process in batches with progress bar\n",
    "        for batch_start in tqdm(range(0, len(df_lang), batch_size), desc=\"Detecting languages\"):\n",
    "            batch_end = min(batch_start + batch_size, len(df_lang))\n",
    "            batch_texts = df_lang.iloc[batch_start:batch_end][\"textDisplay\"].tolist()\n",
    "\n",
    "            # Clean texts (handle potential None values)\n",
    "            batch_texts = [str(text) if text is not None else \"\" for text in batch_texts]\n",
    "\n",
    "            # Detect languages for batch\n",
    "            batch_results = pipe(batch_texts, top_k=1, truncation=True, batch_size=min(batch_size, len(batch_texts)))\n",
    "\n",
    "            # Extract language labels\n",
    "            batch_languages = [result[0][\"label\"] for result in batch_results]\n",
    "            language_results.extend(batch_languages)\n",
    "\n",
    "        # Add language column\n",
    "        df_lang[\"language\"] = language_results\n",
    "\n",
    "        # Language distribution\n",
    "        lang_counts = df_lang[\"language\"].value_counts()\n",
    "        print(f\"\\nüìä Language Distribution:\")\n",
    "        for lang, count in lang_counts.head(10).items():\n",
    "            percentage = (count / len(df_lang)) * 100\n",
    "            print(f\"   {lang}: {count:,} ({percentage:.4f})\")\n",
    "\n",
    "        # Show examples of non-Portuguese content\n",
    "        non_pt = df_lang[df_lang[\"language\"] != \"pt\"]\n",
    "        if len(non_pt) > 0:\n",
    "            print(f\"\\nüåç Sample non-Portuguese comments:\")\n",
    "            sample_languages = non_pt[\"language\"].value_counts().head(3)\n",
    "\n",
    "            for lang in sample_languages.index:\n",
    "                sample_comments = non_pt[non_pt[\"language\"] == lang][\"textDisplay\"].head(2)\n",
    "                print(f\"\\n   {lang.upper()} examples:\")\n",
    "                for i, comment in enumerate(sample_comments, 1):\n",
    "                    preview = comment[:100] + \"...\" if len(comment) > 100 else comment\n",
    "                    print(f\"      {i}. '{preview}'\")\n",
    "\n",
    "        return df_lang\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in language detection: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Detect languages for all comments\n",
    "df = detect_comment_languages(df, language_detector, CleaningConfig.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974d04f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:37:38,025 - INFO - Filtering comments for language: pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üáßüá∑ PORTUGUESE COMMENT FILTERING\n",
      "==================================================\n",
      "üìä Dataset before language filtering: 515,245 comments\n",
      "üìà Portuguese comments: 240,028 (0.4659)\n",
      "üìà Other languages: 275,217 (0.5341)\n",
      "\n",
      "üìä Filtering Results:\n",
      "   ‚úÖ Kept: 240,028 Portuguese comments (0.4659)\n",
      "   üóëÔ∏è Removed: 275,217 non-Portuguese comments (0.5341)\n"
     ]
    }
   ],
   "source": [
    "def filter_portuguese_comments(df: pd.DataFrame, target_language: str = \"pt\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter comments to keep only the target language.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with language column\n",
    "        target_language: Language code to keep (default: \"pt\" for Portuguese)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with only target language comments\n",
    "    \"\"\"\n",
    "    logger.info(f\"Filtering comments for language: {target_language}\")\n",
    "\n",
    "    print(f\"üáßüá∑ PORTUGUESE COMMENT FILTERING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    original_count = len(df)\n",
    "    print(f\"üìä Dataset before language filtering: {original_count:,} comments\")\n",
    "\n",
    "    # Show distribution before filtering\n",
    "    lang_dist = df[\"language\"].value_counts()\n",
    "    pt_count = lang_dist.get(target_language, 0)\n",
    "    print(f\"üìà Portuguese comments: {pt_count:,} ({pt_count / original_count:.4f})\")\n",
    "    print(f\"üìà Other languages: {original_count - pt_count:,} ({(original_count - pt_count) / original_count:.4f})\")\n",
    "\n",
    "    # Filter for Portuguese comments\n",
    "    df_pt = df[df[\"language\"] == target_language].copy()\n",
    "    df_pt.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_count = len(df_pt)\n",
    "    removed_count = original_count - final_count\n",
    "\n",
    "    print(f\"\\nüìä Filtering Results:\")\n",
    "    print(f\"   ‚úÖ Kept: {final_count:,} Portuguese comments ({final_count / original_count:.4f})\")\n",
    "    print(f\"   üóëÔ∏è Removed: {removed_count:,} non-Portuguese comments ({removed_count / original_count:.4f})\")\n",
    "\n",
    "    return df_pt\n",
    "\n",
    "\n",
    "# Filter for Portuguese comments only\n",
    "df = filter_portuguese_comments(df, CleaningConfig.TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731644be",
   "metadata": {},
   "source": [
    "## 8. Video Language Va|lidation\n",
    "\n",
    "Verify that the video titles are also in Portuguese to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8071296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:38:38,014 - INFO - Validating video title languages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• VIDEO LANGUAGE VALIDATION\n",
      "==================================================\n",
      "üé¨ Unique videos to check: 1,619\n",
      "üåê Detecting languages for video titles...\n",
      "\n",
      "üìä Video Title Language Distribution:\n",
      "   pt: 1,204 videos (74.3669)\n",
      "   es: 196 videos (12.1062)\n",
      "   it: 117 videos (7.2267)\n",
      "   hi: 28 videos (1.7295)\n",
      "   en: 27 videos (1.6677)\n",
      "   sw: 18 videos (1.1118)\n",
      "   ur: 8 videos (0.4941)\n",
      "   de: 6 videos (0.3706)\n",
      "   tr: 5 videos (0.3088)\n",
      "   bg: 4 videos (0.2471)\n",
      "   nl: 4 videos (0.2471)\n",
      "   ru: 2 videos (0.1235)\n",
      "\n",
      "üåç Sample non-PT video titles:\n",
      "\n",
      "   ES examples:\n",
      "      1. -7bTGqiS34w: 'Ra√∫l de Molina se entera que Lili Estefan perdi√≥ a una conocida en el Jet Set | ...'\n",
      "      2. -v6VfrcNzB0: 'üí•Era ACOSADO por GORDO pero un ISEKAI lo CONVIRTI√ì en TODO UN PAPUCHO | TEMPORAD...'\n",
      "\n",
      "   IT examples:\n",
      "      1. 1e9OzpsQg1k: 'Mi Gorda Bella capitulo 112'\n",
      "      2. 1u1gR2RJIzE: 'Mi pap√° es un fan!'\n",
      "\n",
      "   HI examples:\n",
      "      1. 2SSDY47Ecrw: 'TU MADRE EST√Å TAN GORDA'\n",
      "      2. 6jmg_XexH8U: 'SOU A BARBIE GORDA üí™ #roblox #barbie #fy #viraliza #robloxshorts #memeblox'\n",
      "\n",
      "üìä Video Language Filtering:\n",
      "   ‚úÖ PT videos: 1,204 (0.7437)\n",
      "   üóëÔ∏è Other languages: 415\n",
      "\n",
      "üìä Comment Filtering by Video Language:\n",
      "   ‚úÖ Comments kept: 191,946 (0.7997)\n",
      "   üóëÔ∏è Comments removed: 48,082 (0.2003)\n"
     ]
    }
   ],
   "source": [
    "def validate_video_languages(df: pd.DataFrame, pipe, target_language: str = \"pt\") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Validate that video titles are in the target language.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with video_id and video_title columns\n",
    "        pipe: Language detection pipeline\n",
    "        target_language: Expected language for videos\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (filtered_dataframe, list_of_target_language_video_ids)\n",
    "    \"\"\"\n",
    "    logger.info(\"Validating video title languages\")\n",
    "\n",
    "    print(\"üé• VIDEO LANGUAGE VALIDATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Get unique videos\n",
    "    videos = df[[\"video_id\", \"video_title\"]].drop_duplicates()\n",
    "    videos.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(f\"üé¨ Unique videos to check: {len(videos):,}\")\n",
    "\n",
    "    # Detect languages for video titles\n",
    "    print(f\"üåê Detecting languages for video titles...\")\n",
    "\n",
    "    video_titles = videos[\"video_title\"].tolist()\n",
    "    # Clean titles (handle potential None values)\n",
    "    video_titles = [str(title) if title is not None else \"\" for title in video_titles]\n",
    "\n",
    "    video_language_results = pipe(video_titles, top_k=1, truncation=True, batch_size=512)\n",
    "\n",
    "    videos[\"language\"] = [result[0][\"label\"] for result in video_language_results]\n",
    "\n",
    "    # Language distribution for videos\n",
    "    video_lang_dist = videos[\"language\"].value_counts()\n",
    "    print(f\"\\nüìä Video Title Language Distribution:\")\n",
    "    for lang, count in video_lang_dist.items():\n",
    "        percentage = (count / len(videos)) * 100\n",
    "        print(f\"   {lang}: {count:,} videos ({percentage:.4f})\")\n",
    "\n",
    "    # Show examples of non-target language videos\n",
    "    non_target_videos = videos[videos[\"language\"] != target_language]\n",
    "    if len(non_target_videos) > 0:\n",
    "        print(f\"\\nüåç Sample non-{target_language.upper()} video titles:\")\n",
    "        for lang in non_target_videos[\"language\"].value_counts().head(3).index:\n",
    "            sample_videos = non_target_videos[non_target_videos[\"language\"] == lang]\n",
    "            print(f\"\\n   {lang.upper()} examples:\")\n",
    "            for i, (_, row) in enumerate(sample_videos.head(2).iterrows(), 1):\n",
    "                title_preview = row[\"video_title\"][:80] + \"...\" if len(str(row[\"video_title\"])) > 80 else row[\"video_title\"]\n",
    "                print(f\"      {i}. {row['video_id']}: '{title_preview}'\")\n",
    "\n",
    "    # Filter for target language videos\n",
    "    target_videos = videos[videos[\"language\"] == target_language]\n",
    "    target_video_ids = target_videos[\"video_id\"].tolist()\n",
    "\n",
    "    print(f\"\\nüìä Video Language Filtering:\")\n",
    "    print(f\"   ‚úÖ {target_language.upper()} videos: {len(target_video_ids):,} ({len(target_video_ids) / len(videos):.4f})\")\n",
    "    print(f\"   üóëÔ∏è Other languages: {len(videos) - len(target_video_ids):,}\")\n",
    "\n",
    "    # Filter comments to only include target language videos\n",
    "    original_comment_count = len(df)\n",
    "    df_filtered = df[df[\"video_id\"].isin(target_video_ids)].copy()\n",
    "    df_filtered.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    final_comment_count = len(df_filtered)\n",
    "    removed_comments = original_comment_count - final_comment_count\n",
    "\n",
    "    print(f\"\\nüìä Comment Filtering by Video Language:\")\n",
    "    print(f\"   ‚úÖ Comments kept: {final_comment_count:,} ({final_comment_count / original_comment_count:.4f})\")\n",
    "    print(f\"   üóëÔ∏è Comments removed: {removed_comments:,} ({removed_comments / original_comment_count:.4f})\")\n",
    "\n",
    "    return df_filtered, target_video_ids\n",
    "\n",
    "\n",
    "# Validate video languages and filter accordingly\n",
    "df, portuguese_video_ids = validate_video_languages(df, language_detector, CleaningConfig.TARGET_LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d07ca",
   "metadata": {},
   "source": [
    "## 9. Final Data Summary and Export\n",
    "\n",
    "Review the cleaned dataset and export for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc7853ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:39:38,016 - INFO - Generating final dataset summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä FINAL CLEANED DATASET SUMMARY\n",
      "==================================================\n",
      "üìà Dataset Overview:\n",
      "   Total comments: 191,946\n",
      "   Unique videos: 1,204\n",
      "   Unique authors: 163,664\n",
      "   Dataset shape: (191946, 20)\n",
      "   Memory usage: 255.06 MB\n",
      "\n",
      "üìÖ Temporal Coverage:\n",
      "   Earliest comment: 2006-11-24 20:16:56+00:00\n",
      "   Latest comment: 2025-04-17 11:46:21+00:00\n",
      "   Time span: 6718 days\n",
      "\n",
      "üìù Comment Length Statistics:\n",
      "   Mean: 91.09 characters\n",
      "   Median: 55.00 characters\n",
      "   Std: 117.04 characters\n",
      "   Range: 10 - 1487 characters\n",
      "\n",
      "üé• Most Commented Videos (Top 5):\n",
      "   1. S4pDpA-g7hE: 9,418 comments\n",
      "      'Pirado - Jo√£o Gordo X Dado Dolabella'\n",
      "   2. 3JK3MbRhjUg: 5,927 comments\n",
      "      'ROMANTIZANDO A OBESIDADE.'\n",
      "   3. Fn54EKtAbTc: 5,873 comments\n",
      "      'Super Ora√ß√£o Contra Inveja, Olho Gordo e Mau Olhado'\n",
      "   4. YeGperfZ7QY: 5,742 comments\n",
      "      'COMECEI A TREINAR E‚Ä¶. #viral #emagrecimento #youtube #obesid...'\n",
      "   5. Y0NWZKORge0: 3,752 comments\n",
      "      'Mulher mais gorda do Brasil pesa 360 quilos'\n",
      "\n",
      "üîç Data Quality Assessment:\n",
      "   author: 191,946 missing (100.0000)\n",
      "   comment: 191,946 missing (100.0000)\n",
      "   date: 191,946 missing (100.0000)\n",
      "   likes: 191,946 missing (100.0000)\n",
      "\n",
      "üåê Language Distribution:\n",
      "   pt: 191,946 (100.0000)\n",
      "\n",
      "üìÑ Final Cleaned Dataset (sample):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c31b5f0a-b04e-440f-b403-bf25727df1e5",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chefe wigol deu um beijo grego no homer skksks",
         "Chefe wigol deu um beijo grego no homer skksks",
         "@MrLopess00",
         "https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO5lyhiApXfoR1zZJKsEBbZjpx-DQR3Rt_ajq96pk7PIl-26av=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@MrLopess00",
         "UCtrByOsq8kIDCQfSXfq3IKw",
         "True",
         "none",
         "447.0",
         "2024-12-29 02:00:55+00:00",
         "2024-12-29 02:00:55+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "2",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem era Batedor de Carteiras ?",
         "Quem era Batedor de Carteiras ?",
         "@mateuss.santossilva5059",
         "https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_2IMVBe6FpUQbhn3VYstkZM0AGckW3SDAPtmlRlxw4Y9GOUlO=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@mateuss.santossilva5059",
         "UCIY2M7NurJ728_H4Cs5zmQA",
         "True",
         "none",
         "5.0",
         "2024-12-29 12:53:19+00:00",
         "2024-12-29 12:53:19+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "3",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"Gra√ßas a deus que essa coisa est√° do nosso ladoüò®\" kskskskksjsks",
         "\"Gra√ßas a deus que essa coisa est√° do nosso ladoüò®\" kskskskksjsks",
         "@Ray._Ryan000",
         "https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX-ohqT8ffBd6vJTL5z8Rd2J8UK1CuBIftClOTcDqU_3wzrP2i7g=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Ray._Ryan000",
         "UCr5gdJ-I9wpcBXWdSR6T5iw",
         "True",
         "none",
         "1677.0",
         "2024-12-29 16:16:06+00:00",
         "2024-12-29 16:17:20+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "4",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "ü§® t√° estranho isso",
         "ü§® t√° estranho isso",
         "@darkgacha5649",
         "https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOHyOkDBA0ibu_cM1LtoM-Ar3C_3fH8CJtikMtCSMztWt7nhu-tvA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@darkgacha5649",
         "UCZnl2qgkPiF-SYyoPmTbzBw",
         "True",
         "none",
         "0.0",
         "2024-12-29 18:03:52+00:00",
         "2024-12-29 18:03:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chefe wigol deu um beijo grego no homer skksks</td>\n",
       "      <td>Chefe wigol deu um beijo grego no homer skksks</td>\n",
       "      <td>@MrLopess00</td>\n",
       "      <td>https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO...</td>\n",
       "      <td>http://www.youtube.com/@MrLopess00</td>\n",
       "      <td>UCtrByOsq8kIDCQfSXfq3IKw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>447.0</td>\n",
       "      <td>2024-12-29 02:00:55+00:00</td>\n",
       "      <td>2024-12-29 02:00:55+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Quem era Batedor de Carteiras ?</td>\n",
       "      <td>Quem era Batedor de Carteiras ?</td>\n",
       "      <td>@mateuss.santossilva5059</td>\n",
       "      <td>https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_...</td>\n",
       "      <td>http://www.youtube.com/@mateuss.santossilva5059</td>\n",
       "      <td>UCIY2M7NurJ728_H4Cs5zmQA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024-12-29 12:53:19+00:00</td>\n",
       "      <td>2024-12-29 12:53:19+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>\"Gra√ßas a deus que essa coisa est√° do nosso la...</td>\n",
       "      <td>\"Gra√ßas a deus que essa coisa est√° do nosso la...</td>\n",
       "      <td>@Ray._Ryan000</td>\n",
       "      <td>https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX...</td>\n",
       "      <td>http://www.youtube.com/@Ray._Ryan000</td>\n",
       "      <td>UCr5gdJ-I9wpcBXWdSR6T5iw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>2024-12-29 16:16:06+00:00</td>\n",
       "      <td>2024-12-29 16:17:20+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>ü§® t√° estranho isso</td>\n",
       "      <td>ü§® t√° estranho isso</td>\n",
       "      <td>@darkgacha5649</td>\n",
       "      <td>https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOH...</td>\n",
       "      <td>http://www.youtube.com/@darkgacha5649</td>\n",
       "      <td>UCZnl2qgkPiF-SYyoPmTbzBw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-29 18:03:52+00:00</td>\n",
       "      <td>2024-12-29 18:03:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "2  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "3  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "4  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1     Chefe wigol deu um beijo grego no homer skksks   \n",
       "2                    Quem era Batedor de Carteiras ?   \n",
       "3  \"Gra√ßas a deus que essa coisa est√° do nosso la...   \n",
       "4                                 ü§® t√° estranho isso   \n",
       "\n",
       "                                        textOriginal  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1     Chefe wigol deu um beijo grego no homer skksks   \n",
       "2                    Quem era Batedor de Carteiras ?   \n",
       "3  \"Gra√ßas a deus que essa coisa est√° do nosso la...   \n",
       "4                                 ü§® t√° estranho isso   \n",
       "\n",
       "          authorDisplayName  \\\n",
       "0         @evelynsoares4467   \n",
       "1               @MrLopess00   \n",
       "2  @mateuss.santossilva5059   \n",
       "3             @Ray._Ryan000   \n",
       "4            @darkgacha5649   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO...   \n",
       "2  https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_...   \n",
       "3  https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX...   \n",
       "4  https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOH...   \n",
       "\n",
       "                                  authorChannelUrl           authorChannelId  \\\n",
       "0         http://www.youtube.com/@evelynsoares4467  UCNhXx9ev5RtEiyGsVjMuTOA   \n",
       "1               http://www.youtube.com/@MrLopess00  UCtrByOsq8kIDCQfSXfq3IKw   \n",
       "2  http://www.youtube.com/@mateuss.santossilva5059  UCIY2M7NurJ728_H4Cs5zmQA   \n",
       "3             http://www.youtube.com/@Ray._Ryan000  UCr5gdJ-I9wpcBXWdSR6T5iw   \n",
       "4            http://www.youtube.com/@darkgacha5649  UCZnl2qgkPiF-SYyoPmTbzBw   \n",
       "\n",
       "   canRate viewerRating  likeCount               publishedAt  \\\n",
       "0     True         none        0.0 2024-12-28 21:38:37+00:00   \n",
       "1     True         none      447.0 2024-12-29 02:00:55+00:00   \n",
       "2     True         none        5.0 2024-12-29 12:53:19+00:00   \n",
       "3     True         none     1677.0 2024-12-29 16:16:06+00:00   \n",
       "4     True         none        0.0 2024-12-29 18:03:52+00:00   \n",
       "\n",
       "                  updatedAt author comment  date likes  \\\n",
       "0 2024-12-28 21:38:37+00:00   None    None  None  None   \n",
       "1 2024-12-29 02:00:55+00:00   None    None  None  None   \n",
       "2 2024-12-29 12:53:19+00:00   None    None  None  None   \n",
       "3 2024-12-29 16:17:20+00:00   None    None  None  None   \n",
       "4 2024-12-29 18:03:52+00:00   None    None  None  None   \n",
       "\n",
       "                          video_title language  \n",
       "0  Tony Gordo √© Incriminado #simpsons       pt  \n",
       "1  Tony Gordo √© Incriminado #simpsons       pt  \n",
       "2  Tony Gordo √© Incriminado #simpsons       pt  \n",
       "3  Tony Gordo √© Incriminado #simpsons       pt  \n",
       "4  Tony Gordo √© Incriminado #simpsons       pt  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_final_summary(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate comprehensive summary of the cleaned dataset.\n",
    "\n",
    "    Args:\n",
    "        df: Final cleaned DataFrame\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with summary statistics\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating final dataset summary\")\n",
    "\n",
    "    print(\"üìä FINAL CLEANED DATASET SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    summary = {\n",
    "        \"total_comments\": len(df),\n",
    "        \"unique_videos\": df[\"video_id\"].nunique(),\n",
    "        \"unique_authors\": df[\"authorDisplayName\"].nunique(),\n",
    "        \"date_range\": {\"earliest\": df[\"publishedAt\"].min() if \"publishedAt\" in df.columns else None, \"latest\": df[\"publishedAt\"].max() if \"publishedAt\" in df.columns else None},\n",
    "        \"dataset_shape\": df.shape,\n",
    "        \"memory_usage_mb\": df.memory_usage(deep=True).sum() / (1024 * 1024),\n",
    "    }\n",
    "\n",
    "    print(f\"üìà Dataset Overview:\")\n",
    "    print(f\"   Total comments: {summary['total_comments']:,}\")\n",
    "    print(f\"   Unique videos: {summary['unique_videos']:,}\")\n",
    "    print(f\"   Unique authors: {summary['unique_authors']:,}\")\n",
    "    print(f\"   Dataset shape: {summary['dataset_shape']}\")\n",
    "    print(f\"   Memory usage: {summary['memory_usage_mb']:.2f} MB\")\n",
    "\n",
    "    # Temporal coverage\n",
    "    if summary[\"date_range\"][\"earliest\"] and summary[\"date_range\"][\"latest\"]:\n",
    "        time_span = summary[\"date_range\"][\"latest\"] - summary[\"date_range\"][\"earliest\"]\n",
    "        print(f\"\\nüìÖ Temporal Coverage:\")\n",
    "        print(f\"   Earliest comment: {summary['date_range']['earliest']}\")\n",
    "        print(f\"   Latest comment: {summary['date_range']['latest']}\")\n",
    "        print(f\"   Time span: {time_span.days} days\")\n",
    "        summary[\"time_span_days\"] = time_span.days\n",
    "\n",
    "    # Comment length statistics\n",
    "    if \"textDisplay\" in df.columns:\n",
    "        df_temp = df.copy()\n",
    "        df_temp[\"comment_length\"] = df_temp[\"textDisplay\"].str.len()\n",
    "        length_stats = df_temp[\"comment_length\"].describe()\n",
    "\n",
    "        print(f\"\\nüìù Comment Length Statistics:\")\n",
    "        print(f\"   Mean: {length_stats['mean']:.2f} characters\")\n",
    "        print(f\"   Median: {length_stats['50%']:.2f} characters\")\n",
    "        print(f\"   Std: {length_stats['std']:.2f} characters\")\n",
    "        print(f\"   Range: {length_stats['min']:.0f} - {length_stats['max']:.0f} characters\")\n",
    "\n",
    "        summary[\"comment_length\"] = {\"mean\": length_stats[\"mean\"], \"median\": length_stats[\"50%\"], \"std\": length_stats[\"std\"], \"min\": length_stats[\"min\"], \"max\": length_stats[\"max\"]}\n",
    "\n",
    "    # Top videos by comment count\n",
    "    print(f\"\\nüé• Most Commented Videos (Top 5):\")\n",
    "    top_videos = df[\"video_id\"].value_counts().head()\n",
    "    for i, (video_id, count) in enumerate(top_videos.items(), 1):\n",
    "        title = df[df[\"video_id\"] == video_id][\"video_title\"].iloc[0] if \"video_title\" in df.columns else \"Unknown\"\n",
    "        title_preview = title[:60] + \"...\" if len(str(title)) > 60 else title\n",
    "        print(f\"   {i}. {video_id}: {count:,} comments\")\n",
    "        print(f\"      '{title_preview}'\")\n",
    "\n",
    "    summary[\"top_videos\"] = top_videos.to_dict()\n",
    "\n",
    "    # Data quality assessment\n",
    "    print(f\"\\nüîç Data Quality Assessment:\")\n",
    "    missing_stats = {}\n",
    "    for col in df.columns:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        missing_stats[col] = {\"count\": missing_count, \"percentage\": missing_pct}\n",
    "        if missing_count > 0:\n",
    "            print(f\"   {col}: {missing_count:,} missing ({missing_pct:.4f})\")\n",
    "\n",
    "    if not any(stats[\"count\"] > 0 for stats in missing_stats.values()):\n",
    "        print(\"   ‚úÖ No missing values detected\")\n",
    "\n",
    "    summary[\"missing_data\"] = missing_stats\n",
    "\n",
    "    # Language confirmation\n",
    "    if \"language\" in df.columns:\n",
    "        lang_dist = df[\"language\"].value_counts()\n",
    "        print(f\"\\nüåê Language Distribution:\")\n",
    "        for lang, count in lang_dist.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"   {lang}: {count:,} ({pct:.4f})\")\n",
    "        summary[\"language_distribution\"] = lang_dist.to_dict()\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# Generate final summary\n",
    "final_summary = generate_final_summary(df)\n",
    "\n",
    "# Display final cleaned dataset\n",
    "print(f\"\\nüìÑ Final Cleaned Dataset (sample):\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f35d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 18:40:38,017 - INFO - Exporting cleaned data to: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ EXPORTING CLEANED DATA\n",
      "==================================================\n",
      "‚úÖ Dataset exported successfully\n",
      "üìÅ File: 20250417_youtube_comments_pt_cleaned1.parquet\n",
      "üìä Size: 47.1 MB\n",
      "üìà Records: 191,946\n",
      "üìã Metadata exported: 20250417_youtube_comments_pt_cleaned1.json\n",
      "üíæ CSV backup created: 20250417_youtube_comments_pt_cleaned1.csv (104.9 MB)\n",
      "\n",
      "‚úÖ Export completed successfully!\n",
      "üìÅ Files created:\n",
      "   ‚Ä¢ 20250417_youtube_comments_pt_cleaned1.parquet (main dataset)\n",
      "   ‚Ä¢ 20250417_youtube_comments_pt_cleaned1.json (metadata)\n",
      "   ‚Ä¢ 20250417_youtube_comments_pt_cleaned1.csv (CSV backup)\n",
      "\n",
      "üéâ DATA CLEANING PIPELINE COMPLETED!\n",
      "üìä Final dataset: 191,946 Portuguese comments from 1,204 videos\n",
      "üìÅ Output file: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "‚ñ∂Ô∏è Ready for analysis in subsequent notebooks!\n"
     ]
    }
   ],
   "source": [
    "def export_cleaned_data(df: pd.DataFrame, output_path: Path, summary: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Export the cleaned dataset with comprehensive metadata.\n",
    "\n",
    "    Args:\n",
    "        df: Cleaned DataFrame to export\n",
    "        output_path: Path for the output file\n",
    "        summary: Summary statistics dictionary\n",
    "    \"\"\"\n",
    "    logger.info(f\"Exporting cleaned data to: {output_path}\")\n",
    "\n",
    "    print(\"üíæ EXPORTING CLEANED DATA\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Export main dataset\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "        print(f\"‚úÖ Dataset exported successfully\")\n",
    "        print(f\"üìÅ File: {output_path.name}\")\n",
    "        print(f\"üìä Size: {file_size_mb:.1f} MB\")\n",
    "        print(f\"üìà Records: {len(df):,}\")\n",
    "\n",
    "        # Export summary metadata\n",
    "        metadata_path = output_path.with_suffix(\".json\")\n",
    "\n",
    "        # Prepare metadata for JSON serialization\n",
    "        export_metadata = {\n",
    "            \"export_info\": {\"timestamp\": pd.Timestamp.now().isoformat(), \"file_name\": output_path.name, \"file_size_mb\": file_size_mb, \"format\": \"parquet\"},\n",
    "            \"dataset_summary\": summary,\n",
    "            \"cleaning_pipeline\": {\n",
    "                \"steps\": [\"Text length filtering (10th to 99.9th percentile)\", \"Emoji-only comment removal\", \"Data structure normalization\", \"Duplicate detection and removal (content-based hashing)\", \"Language detection and Portuguese filtering\", \"Video language validation\"],\n",
    "                \"target_language\": CleaningConfig.TARGET_LANGUAGE,\n",
    "                \"min_text_percentile\": CleaningConfig.MIN_TEXT_PERCENTILE,\n",
    "                \"max_text_percentile\": CleaningConfig.MAX_TEXT_PERCENTILE,\n",
    "            },\n",
    "            \"column_descriptions\": {\n",
    "                \"video_id\": \"YouTube video identifier\",\n",
    "                \"textDisplay\": \"Comment text content (Portuguese only)\",\n",
    "                \"authorDisplayName\": \"Comment author display name\",\n",
    "                \"authorChannelUrl\": \"Author channel URL\",\n",
    "                \"authorChannelId\": \"Author channel identifier\",\n",
    "                \"publishedAt\": \"Comment publication timestamp\",\n",
    "                \"updatedAt\": \"Comment last update timestamp\",\n",
    "                \"likeCount\": \"Number of likes on the comment\",\n",
    "                \"video_title\": \"Title of the YouTube video (Portuguese)\",\n",
    "                \"language\": \"Detected language code (pt for Portuguese)\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Handle datetime serialization\n",
    "        import json\n",
    "\n",
    "        def json_serializer(obj):\n",
    "            if isinstance(obj, pd.Timestamp):\n",
    "                return obj.isoformat()\n",
    "            elif hasattr(obj, \"isoformat\"):\n",
    "                return obj.isoformat()\n",
    "            return str(obj)\n",
    "\n",
    "        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(export_metadata, f, indent=2, ensure_ascii=False, default=json_serializer)\n",
    "\n",
    "        print(f\"üìã Metadata exported: {metadata_path.name}\")\n",
    "\n",
    "        # Export CSV backup\n",
    "        csv_path = output_path.with_suffix(\".csv\")\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "        csv_size_mb = csv_path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "        print(f\"üíæ CSV backup created: {csv_path.name} ({csv_size_mb:.1f} MB)\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Export completed successfully!\")\n",
    "        print(f\"üìÅ Files created:\")\n",
    "        print(f\"   ‚Ä¢ {output_path.name} (main dataset)\")\n",
    "        print(f\"   ‚Ä¢ {metadata_path.name} (metadata)\")\n",
    "        print(f\"   ‚Ä¢ {csv_path.name} (CSV backup)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error exporting data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Export the cleaned dataset\n",
    "export_cleaned_data(df, CleaningConfig.OUTPUT_FILE, final_summary)\n",
    "\n",
    "print(f\"\\nüéâ DATA CLEANING PIPELINE COMPLETED!\")\n",
    "print(f\"üìä Final dataset: {len(df):,} Portuguese comments from {df['video_id'].nunique():,} videos\")\n",
    "print(f\"üìÅ Output file: {CleaningConfig.OUTPUT_FILE}\")\n",
    "print(f\"‚ñ∂Ô∏è Ready for analysis in subsequent notebooks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaffc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e170f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_youtube_weight_stigma_1e733e1bf2b6c34f6bcb8483dce2a479",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
