{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube Data Collection for Weight Stigma Research\n",
    "\n",
    "This notebook contains the data collection pipeline for analyzing YouTube videos and comments related to weight stigma research. The code implements functions to search for videos using specific keywords and collect associated comments using the YouTube Data API v3.\n",
    "\n",
    "## Research Overview\n",
    "\n",
    "This study investigates weight stigma in Brazilian YouTube content by analyzing:\n",
    "- Video metadata for obesity-related keywords\n",
    "- User comments on these videos\n",
    "- Patterns in user engagement and sentiment\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- YouTube Data API v3 key\n",
    "- Python packages: pandas, google-api-python-client, tqdm, joblib\n",
    "- Sufficient API quota for large-scale data collection\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. Set your YouTube API key in the configuration section\n",
    "2. Define search keywords for your research\n",
    "3. Run the data collection pipeline\n",
    "4. Export collected data for further analysis\n",
    "\n",
    "**Note**: API keys should be stored securely and never committed to version control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded successfully\n",
      "📁 Data directory: data\n",
      "🔍 Search keywords: ['obesidade', 'gordo', 'gorda', 'obeso', 'obesa']\n",
      "🔑 API keys configured: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", handlers=[logging.FileHandler(\"youtube_data_collection.log\"), logging.StreamHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"Configuration class for YouTube data collection.\"\"\"\n",
    "\n",
    "    # API Configuration\n",
    "    API_KEYS = [os.getenv(\"YOUTUBE_API_KEY_1\", \"\"), os.getenv(\"YOUTUBE_API_KEY_2\", \"\"), os.getenv(\"YOUTUBE_API_KEY_3\", \"\")]\n",
    "\n",
    "    # Search Parameters\n",
    "    SEARCH_KEYWORDS = [\"obesidade\", \"gordo\", \"gorda\", \"obeso\", \"obesa\"]\n",
    "    LANGUAGE_CODE = \"pt\"\n",
    "    REGION_CODE = \"BR\"\n",
    "    PUBLISHED_AFTER = \"2000-01-01T00:00:00Z\"\n",
    "    PUBLISHED_BEFORE = \"2025-04-17T23:59:59Z\"\n",
    "\n",
    "    # File Paths\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "    TMP_DATA_DIR = DATA_DIR / \"tmp\"\n",
    "\n",
    "    # API Limits\n",
    "    MAX_RESULTS_PER_REQUEST = 50\n",
    "    MAX_COMMENTS_PER_REQUEST = 100\n",
    "    MAX_CONSECUTIVE_ERRORS = 10\n",
    "\n",
    "\n",
    "# Ensure data directories exist\n",
    "Config.RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "Config.TMP_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✅ Configuration loaded successfully\")\n",
    "print(f\"📁 Data directory: {Config.DATA_DIR}\")\n",
    "print(f\"🔍 Search keywords: {Config.SEARCH_KEYWORDS}\")\n",
    "print(f\"🔑 API keys configured: {len([key for key in Config.API_KEYS if key])}\")\n",
    "\n",
    "# Validate API keys\n",
    "if not any(Config.API_KEYS):\n",
    "    print(\"⚠️ WARNING: No API keys found in environment variables.\")\n",
    "    print(\"Please set YOUTUBE_API_KEY_1, YOUTUBE_API_KEY_2, and/or YOUTUBE_API_KEY_3\")\n",
    "    print(\"Example: export YOUTUBE_API_KEY_1='your_api_key_here'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Functions for YouTube Data Collection\n",
    "\n",
    "The following functions implement the data collection pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouTubeDataCollector:\n",
    "    \"\"\"\n",
    "    A comprehensive YouTube data collection class for research purposes.\n",
    "\n",
    "    This class provides methods to search for videos, collect comments,\n",
    "    and retrieve video metadata using the YouTube Data API v3.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize the YouTube data collector.\n",
    "\n",
    "        Args:\n",
    "            api_key: Valid YouTube Data API v3 key\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        logger.info(f\"YouTube client initialized with API key: ...{api_key[-4:]}\")\n",
    "\n",
    "    def extract_video_id(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract YouTube video ID from various URL formats.\n",
    "\n",
    "        Args:\n",
    "            url: YouTube video URL\n",
    "\n",
    "        Returns:\n",
    "            Video ID if found, None otherwise\n",
    "        \"\"\"\n",
    "        patterns = [r\"(?:v=|\\/)([0-9A-Za-z_-]{11}).*\", r\"(?:embed\\/)([0-9A-Za-z_-]{11})\", r\"(?:youtu\\.be\\/)([0-9A-Za-z_-]{11})\"]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "\n",
    "    def search_videos(self, keyword: str, language_code: str = Config.LANGUAGE_CODE, published_after: str = Config.PUBLISHED_AFTER, published_before: str = Config.PUBLISHED_BEFORE, region_code: str = Config.REGION_CODE, max_results: Optional[int] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Search YouTube for videos matching a keyword.\n",
    "\n",
    "        Args:\n",
    "            keyword: Search term\n",
    "            language_code: Language code for relevance\n",
    "            published_after: Earliest publication date (ISO 8601)\n",
    "            published_before: Latest publication date (ISO 8601)\n",
    "            region_code: Country/region code\n",
    "            max_results: Maximum number of results (None for all available)\n",
    "\n",
    "        Returns:\n",
    "            List of video IDs\n",
    "        \"\"\"\n",
    "        logger.info(f\"Searching videos for keyword: '{keyword}'\")\n",
    "\n",
    "        video_ids = []\n",
    "        next_page_token = None\n",
    "        total_requests = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                search_request = self.youtube.search().list(\n",
    "                    part=\"snippet\",\n",
    "                    q=keyword,\n",
    "                    type=\"video\",\n",
    "                    publishedAfter=published_after,\n",
    "                    publishedBefore=published_before,\n",
    "                    regionCode=region_code,\n",
    "                    maxResults=Config.MAX_RESULTS_PER_REQUEST,\n",
    "                    pageToken=next_page_token,\n",
    "                    relevanceLanguage=language_code,\n",
    "                )\n",
    "\n",
    "                response = search_request.execute()\n",
    "                total_requests += 1\n",
    "\n",
    "                # Extract video IDs\n",
    "                for item in response.get(\"items\", []):\n",
    "                    video_ids.append(item[\"id\"][\"videoId\"])\n",
    "\n",
    "                # Check for pagination\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or (max_results and len(video_ids) >= max_results):\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error searching videos for '{keyword}': {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        logger.info(f\"Found {len(video_ids)} videos for '{keyword}' in {total_requests} requests\")\n",
    "        return video_ids[:max_results] if max_results else video_ids\n",
    "\n",
    "    def get_video_comments(self, video_id: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Retrieve all comments from a YouTube video.\n",
    "\n",
    "        Args:\n",
    "            video_id: YouTube video ID\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with comment data or None if error/disabled comments\n",
    "        \"\"\"\n",
    "        logger.info(f\"Collecting comments for video: {video_id}\")\n",
    "\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        total_requests = 0\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                request = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    textFormat=\"plainText\",\n",
    "                    maxResults=Config.MAX_COMMENTS_PER_REQUEST,\n",
    "                    pageToken=next_page_token,\n",
    "                )\n",
    "\n",
    "                response = request.execute()\n",
    "                total_requests += 1\n",
    "\n",
    "                # Extract comment data\n",
    "                for thread in response.get(\"items\", []):\n",
    "                    comment_data = thread[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                    comments.append(comment_data)\n",
    "\n",
    "                # Check for pagination\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            if \"disabled\" in error_msg or \"forbidden\" in error_msg:\n",
    "                logger.warning(f\"Comments disabled for video {video_id}\")\n",
    "                # Return empty DataFrame with proper structure\n",
    "                return pd.DataFrame({\"textDisplay\": [None], \"authorDisplayName\": [None], \"publishedAt\": [None], \"updatedAt\": [None], \"likeCount\": [None], \"video_id\": [video_id]})\n",
    "            else:\n",
    "                logger.error(f\"Error getting comments for {video_id}: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        if comments:\n",
    "            df = pd.DataFrame(comments)\n",
    "            df[\"video_id\"] = video_id\n",
    "            logger.info(f\"Collected {len(comments)} comments for {video_id} in {total_requests} requests\")\n",
    "            return df\n",
    "        else:\n",
    "            logger.warning(f\"No comments found for video {video_id}\")\n",
    "            return pd.DataFrame({\"video_id\": [video_id]})\n",
    "\n",
    "    def get_video_metadata(self, video_ids: List[str]) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve metadata for multiple videos.\n",
    "\n",
    "        Args:\n",
    "            video_ids: List of YouTube video IDs\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping video IDs to metadata\n",
    "        \"\"\"\n",
    "        logger.info(f\"Collecting metadata for {len(video_ids)} videos\")\n",
    "\n",
    "        metadata = {}\n",
    "\n",
    "        # Process videos in batches of 50 (API limit)\n",
    "        batch_size = 50\n",
    "        for i in range(0, len(video_ids), batch_size):\n",
    "            batch = video_ids[i : i + batch_size]\n",
    "\n",
    "            try:\n",
    "                request = self.youtube.videos().list(part=\"snippet,statistics\", id=\",\".join(batch))\n",
    "                response = request.execute()\n",
    "\n",
    "                for item in response.get(\"items\", []):\n",
    "                    video_id = item[\"id\"]\n",
    "                    metadata[video_id] = {\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"][\"description\"],\n",
    "                        \"publishedAt\": item[\"snippet\"][\"publishedAt\"],\n",
    "                        \"channelTitle\": item[\"snippet\"][\"channelTitle\"],\n",
    "                        \"viewCount\": item.get(\"statistics\", {}).get(\"viewCount\", 0),\n",
    "                        \"likeCount\": item.get(\"statistics\", {}).get(\"likeCount\", 0),\n",
    "                        \"commentCount\": item.get(\"statistics\", {}).get(\"commentCount\", 0),\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error getting metadata for batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Collected metadata for {len(metadata)} videos\")\n",
    "        return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_youtube_data(keywords: List[str], api_keys: List[str], output_prefix: str = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Complete data collection pipeline for YouTube research.\n",
    "\n",
    "    Args:\n",
    "        keywords: List of search keywords\n",
    "        api_keys: List of YouTube API keys (for redundancy)\n",
    "        output_prefix: Prefix for output files (default: timestamp)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (comments_dataframe, collection_metadata)\n",
    "    \"\"\"\n",
    "    if output_prefix is None:\n",
    "        output_prefix = \"20250417\"\n",
    "\n",
    "    # Filter valid API keys\n",
    "    valid_api_keys = [key for key in api_keys if key.strip()]\n",
    "    if not valid_api_keys:\n",
    "        raise ValueError(\"No valid API keys provided\")\n",
    "\n",
    "    logger.info(f\"Starting data collection with {len(keywords)} keywords and {len(valid_api_keys)} API keys\")\n",
    "\n",
    "    # Step 1: Search for videos\n",
    "    all_video_ids = set()\n",
    "    current_api_index = 0\n",
    "\n",
    "    for keyword in tqdm(keywords, desc=\"Searching videos\"):\n",
    "        try:\n",
    "            collector = YouTubeDataCollector(valid_api_keys[current_api_index])\n",
    "            video_ids = collector.search_videos(keyword)\n",
    "            all_video_ids.update(video_ids)\n",
    "            logger.info(f\"Keyword '{keyword}': found {len(video_ids)} videos\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to search for '{keyword}': {str(e)}\")\n",
    "            # Rotate to next API key\n",
    "            current_api_index = (current_api_index + 1) % len(valid_api_keys)\n",
    "\n",
    "    all_video_ids = list(all_video_ids)\n",
    "    logger.info(f\"Total unique videos found: {len(all_video_ids)}\")\n",
    "\n",
    "    # Save video IDs\n",
    "    video_ids_file = Config.TMP_DATA_DIR / f\"{output_prefix}_video_ids.joblib\"\n",
    "    joblib.dump(all_video_ids, video_ids_file)\n",
    "    logger.info(f\"Video IDs saved to: {video_ids_file}\")\n",
    "\n",
    "    # Step 2: Collect comments\n",
    "    successful_videos = []\n",
    "    failed_videos = []\n",
    "    comment_dataframes = []\n",
    "    error_count = 0\n",
    "\n",
    "    for video_id in tqdm(all_video_ids, desc=\"Collecting comments\"):\n",
    "        try:\n",
    "            collector = YouTubeDataCollector(valid_api_keys[current_api_index])\n",
    "            comments_df = collector.get_video_comments(video_id)\n",
    "\n",
    "            if comments_df is not None:\n",
    "                successful_videos.append(video_id)\n",
    "                comment_dataframes.append(comments_df)\n",
    "                error_count = 0  # Reset error counter on success\n",
    "            else:\n",
    "                failed_videos.append(video_id)\n",
    "                error_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_videos.append(video_id)\n",
    "            logger.error(f\"Failed to get comments for {video_id}: {str(e)}\")\n",
    "            error_count += 1\n",
    "\n",
    "            # Rotate API key on consecutive errors\n",
    "            if error_count >= Config.MAX_CONSECUTIVE_ERRORS:\n",
    "                current_api_index = (current_api_index + 1) % len(valid_api_keys)\n",
    "                error_count = 0\n",
    "                logger.info(f\"Switched to API key index: {current_api_index}\")\n",
    "\n",
    "    # Step 3: Combine and process comments\n",
    "    if comment_dataframes:\n",
    "        all_comments = pd.concat(comment_dataframes, ignore_index=True)\n",
    "\n",
    "        # Clean and process data\n",
    "        all_comments = all_comments.dropna(subset=[\"textDisplay\"])\n",
    "        all_comments[\"publishedAt\"] = pd.to_datetime(all_comments[\"publishedAt\"])\n",
    "        all_comments[\"updatedAt\"] = pd.to_datetime(all_comments[\"updatedAt\"])\n",
    "        all_comments = all_comments.sort_values([\"video_id\", \"publishedAt\"])\n",
    "        all_comments = all_comments.reset_index(drop=True)\n",
    "\n",
    "        logger.info(f\"Total comments collected: {len(all_comments)}\")\n",
    "    else:\n",
    "        all_comments = pd.DataFrame()\n",
    "        logger.warning(\"No comments collected\")\n",
    "\n",
    "    # Step 4: Get video metadata\n",
    "    metadata = {}\n",
    "    if successful_videos:\n",
    "        try:\n",
    "            collector = YouTubeDataCollector(valid_api_keys[0])\n",
    "            metadata = collector.get_video_metadata(successful_videos)\n",
    "\n",
    "            # Add metadata to comments dataframe\n",
    "            if not all_comments.empty:\n",
    "                all_comments[\"video_title\"] = all_comments[\"video_id\"].map(lambda x: metadata.get(x, {}).get(\"title\", \"Unknown\"))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to collect video metadata: {str(e)}\")\n",
    "\n",
    "    # Step 5: Save results\n",
    "    if not all_comments.empty:\n",
    "        # Save comments\n",
    "        comments_file = Config.RAW_DATA_DIR / f\"{output_prefix}_youtube_comments.parquet\"\n",
    "        all_comments.to_parquet(comments_file, index=False)\n",
    "        logger.info(f\"Comments saved to: {comments_file}\")\n",
    "\n",
    "        # Save CSV backup\n",
    "        csv_file = Config.RAW_DATA_DIR / f\"{output_prefix}_youtube_comments.csv\"\n",
    "        all_comments.to_csv(csv_file, index=False)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_file = Config.TMP_DATA_DIR / f\"{output_prefix}_video_metadata.joblib\"\n",
    "    joblib.dump(metadata, metadata_file)\n",
    "\n",
    "    # Save processing results\n",
    "    results_file = Config.TMP_DATA_DIR / f\"{output_prefix}_collection_results.joblib\"\n",
    "    collection_metadata = {\n",
    "        \"total_videos_found\": len(all_video_ids),\n",
    "        \"successful_videos\": len(successful_videos),\n",
    "        \"failed_videos\": len(failed_videos),\n",
    "        \"total_comments\": len(all_comments) if not all_comments.empty else 0,\n",
    "        \"keywords_used\": keywords,\n",
    "        \"collection_timestamp\": datetime.now().isoformat(),\n",
    "        \"successful_video_ids\": successful_videos,\n",
    "        \"failed_video_ids\": failed_videos,\n",
    "    }\n",
    "    joblib.dump(collection_metadata, results_file)\n",
    "\n",
    "    logger.info(\"Data collection completed successfully!\")\n",
    "    logger.info(f\"Results summary:\")\n",
    "    logger.info(f\"  - Videos found: {collection_metadata['total_videos_found']}\")\n",
    "    logger.info(f\"  - Videos with comments: {collection_metadata['successful_videos']}\")\n",
    "    logger.info(f\"  - Failed videos: {collection_metadata['failed_videos']}\")\n",
    "    logger.info(f\"  - Total comments: {collection_metadata['total_comments']}\")\n",
    "\n",
    "    return all_comments, collection_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection Execution\n",
    "\n",
    "Run the data collection pipeline with the configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API keys configured. Ready to start data collection.\n",
      "🎯 Target keywords: ['obesidade', 'gordo', 'gorda', 'obeso', 'obesa']\n",
      "🌍 Region: BR, Language: pt\n",
      "📅 Date range: 2000-01-01T00:00:00Z to 2025-04-17T23:59:59Z\n",
      "\n",
      "⚠️ Note: Large-scale data collection may take several hours and consume significant API quota.\n",
      "Consider testing with a smaller keyword set first.\n"
     ]
    }
   ],
   "source": [
    "# Verify API keys before starting\n",
    "if not any(Config.API_KEYS):\n",
    "    print(\"❌ ERROR: No API keys configured!\")\n",
    "    print(\"Please set your YouTube API keys in environment variables:\")\n",
    "    print(\"export YOUTUBE_API_KEY_1='your_first_api_key'\")\n",
    "    print(\"export YOUTUBE_API_KEY_2='your_second_api_key'\")\n",
    "    print(\"export YOUTUBE_API_KEY_3='your_third_api_key'\")\n",
    "else:\n",
    "    print(\"✅ API keys configured. Ready to start data collection.\")\n",
    "    print(f\"🎯 Target keywords: {Config.SEARCH_KEYWORDS}\")\n",
    "    print(f\"🌍 Region: {Config.REGION_CODE}, Language: {Config.LANGUAGE_CODE}\")\n",
    "    print(f\"📅 Date range: {Config.PUBLISHED_AFTER} to {Config.PUBLISHED_BEFORE}\")\n",
    "    print(\"\\n⚠️ Note: Large-scale data collection may take several hours and consume significant API quota.\")\n",
    "    print(\"Consider testing with a smaller keyword set first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute data collection\n",
    "# Uncomment the following lines to run the data collection:\n",
    "\n",
    "# comments_df, metadata = collect_youtube_data(keywords=Config.SEARCH_KEYWORDS, api_keys=Config.API_KEYS, output_prefix=\"weight_stigma_study\")\n",
    "\n",
    "# # Display results summary\n",
    "# if not comments_df.empty:\n",
    "#     print(\"📊 Data Collection Results:\")\n",
    "#     print(f\"   Total comments collected: {len(comments_df):,}\")\n",
    "#     print(f\"   Unique videos: {comments_df['video_id'].nunique():,}\")\n",
    "#     print(f\"   Date range: {comments_df['publishedAt'].min()} to {comments_df['publishedAt'].max()}\")\n",
    "#     print(f\"   Most active video: {comments_df['video_id'].value_counts().index[0]} ({comments_df['video_id'].value_counts().iloc[0]} comments)\")\n",
    "\n",
    "#     # Display sample data\n",
    "#     print(\"\\n📝 Sample Comments:\")\n",
    "#     print(comments_df[[\"textDisplay\", \"authorDisplayName\", \"publishedAt\", \"video_title\"]].head())\n",
    "# else:\n",
    "#     print(\"❌ No data collected. Check API keys and network connection.\")\n",
    "\n",
    "print(\"💡 To run data collection, uncomment the code above and execute this cell.\")\n",
    "print(\"⏱️ Expected runtime: 1 hour depending on API quota and data volume.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Validation\n",
    "\n",
    "After data collection, explore the collected dataset to ensure quality and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading data from: 20250417_youtube_comments.parquet\n",
      "✅ Successfully loaded 593,509 records\n",
      "🔍 DATA EXPLORATION REPORT\n",
      "==================================================\n",
      "📊 Dataset Overview:\n",
      "   Total comments: 593,509\n",
      "   Unique videos: 1,850\n",
      "   Unique authors: 512,630\n",
      "   Dataset shape: (593509, 19)\n",
      "\n",
      "📅 Temporal Coverage:\n",
      "   Earliest comment: 2006-11-24 20:16:56+00:00\n",
      "   Latest comment: 2025-04-17 11:52:54+00:00\n",
      "   Time span: 6718 days\n",
      "\n",
      "📝 Comment Length Statistics:\n",
      "   Mean length: 68.3 characters\n",
      "   Median length: 38.0 characters\n",
      "   Max length: 62137 characters\n",
      "\n",
      "🎥 Most Commented Videos:\n",
      "   1. LVb5EcOp2vw: 22,409 comments\n",
      "      Title: La GORDA me llama FRIKI | #humor #risa\n",
      "   2. knmVYBNj_xY: 17,325 comments\n",
      "      Title: ES CRITICADA POR SER GORDA😱\n",
      "   3. qohZ83lZuzg: 15,057 comments\n",
      "      Title: Terrifying Night in Haunted Ghost Town | Cerro Gordo\n",
      "   4. 3leulf_BVgQ: 12,679 comments\n",
      "      Title: Un Día Comiendo Como La Mujer Más Gorda del Mundo (20,000 KCAL)\n",
      "   5. Qwr29yZ9S-8: 12,059 comments\n",
      "      Title: GORDA paródia de Tirullipa / LOKA Simone, Simaria e Anitta\n",
      "\n",
      "🔍 Data Quality Assessment:\n",
      "   Missing comment text: 0 (0.0%)\n",
      "   Missing author names: 0 (0.0%)\n",
      "\n",
      "👍 Engagement Metrics:\n",
      "   Total likes: 5,589,509.0\n",
      "   Average likes per comment: 9.42\n",
      "   Most liked comment: 263191.0 likes\n",
      "💡 Use load_collected_data() to load previously collected data\n",
      "💡 Use explore_collected_data(df) to analyze a dataset\n"
     ]
    }
   ],
   "source": [
    "def explore_collected_data(comments_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Provide comprehensive data exploration and quality assessment.\n",
    "\n",
    "    Args:\n",
    "        comments_df: DataFrame containing collected comments\n",
    "    \"\"\"\n",
    "    if comments_df.empty:\n",
    "        print(\"❌ No data to explore.\")\n",
    "        return\n",
    "\n",
    "    print(\"🔍 DATA EXPLORATION REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Basic statistics\n",
    "    print(f\"📊 Dataset Overview:\")\n",
    "    print(f\"   Total comments: {len(comments_df):,}\")\n",
    "    print(f\"   Unique videos: {comments_df['video_id'].nunique():,}\")\n",
    "    print(f\"   Unique authors: {comments_df['authorDisplayName'].nunique():,}\")\n",
    "    print(f\"   Dataset shape: {comments_df.shape}\")\n",
    "\n",
    "    # Date range\n",
    "    if \"publishedAt\" in comments_df.columns:\n",
    "        date_range = comments_df[\"publishedAt\"].agg([\"min\", \"max\"])\n",
    "        print(f\"\\n📅 Temporal Coverage:\")\n",
    "        print(f\"   Earliest comment: {date_range['min']}\")\n",
    "        print(f\"   Latest comment: {date_range['max']}\")\n",
    "        print(f\"   Time span: {(date_range['max'] - date_range['min']).days} days\")\n",
    "\n",
    "    # Comment length analysis\n",
    "    if \"textDisplay\" in comments_df.columns:\n",
    "        comments_df[\"comment_length\"] = comments_df[\"textDisplay\"].str.len()\n",
    "        print(f\"\\n📝 Comment Length Statistics:\")\n",
    "        print(f\"   Mean length: {comments_df['comment_length'].mean():.1f} characters\")\n",
    "        print(f\"   Median length: {comments_df['comment_length'].median():.1f} characters\")\n",
    "        print(f\"   Max length: {comments_df['comment_length'].max()} characters\")\n",
    "\n",
    "    # Top videos by comment count\n",
    "    print(f\"\\n🎥 Most Commented Videos:\")\n",
    "    top_videos = comments_df[\"video_id\"].value_counts().head()\n",
    "    for i, (video_id, count) in enumerate(top_videos.items(), 1):\n",
    "        title = comments_df[comments_df[\"video_id\"] == video_id][\"video_title\"].iloc[0] if \"video_title\" in comments_df.columns else \"Unknown\"\n",
    "        print(f\"   {i}. {video_id}: {count:,} comments\")\n",
    "        print(f\"      Title: {title[:80]}{'...' if len(str(title)) > 80 else ''}\")\n",
    "\n",
    "    # Data quality assessment\n",
    "    print(f\"\\n🔍 Data Quality Assessment:\")\n",
    "    missing_text = comments_df[\"textDisplay\"].isna().sum()\n",
    "    missing_author = comments_df[\"authorDisplayName\"].isna().sum()\n",
    "    print(f\"   Missing comment text: {missing_text:,} ({missing_text / len(comments_df) * 100:.1f}%)\")\n",
    "    print(f\"   Missing author names: {missing_author:,} ({missing_author / len(comments_df) * 100:.1f}%)\")\n",
    "\n",
    "    # Engagement metrics\n",
    "    if \"likeCount\" in comments_df.columns:\n",
    "        total_likes = comments_df[\"likeCount\"].fillna(0).sum()\n",
    "        avg_likes = comments_df[\"likeCount\"].fillna(0).mean()\n",
    "        print(f\"\\n👍 Engagement Metrics:\")\n",
    "        print(f\"   Total likes: {total_likes:,}\")\n",
    "        print(f\"   Average likes per comment: {avg_likes:.2f}\")\n",
    "        print(f\"   Most liked comment: {comments_df['likeCount'].max()} likes\")\n",
    "\n",
    "\n",
    "# Function to load previously collected data\n",
    "def load_collected_data(file_pattern: str = \"*youtube_comments.parquet\") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load previously collected YouTube data.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: Glob pattern to match data files\n",
    "\n",
    "    Returns:\n",
    "        DataFrame if data found, None otherwise\n",
    "    \"\"\"\n",
    "    from glob import glob\n",
    "\n",
    "    data_files = list(Config.RAW_DATA_DIR.glob(file_pattern))\n",
    "\n",
    "    if not data_files:\n",
    "        print(f\"❌ No data files found matching pattern: {file_pattern}\")\n",
    "        print(f\"📁 Search directory: {Config.RAW_DATA_DIR}\")\n",
    "        return None\n",
    "\n",
    "    # Load the most recent file\n",
    "    latest_file = max(data_files, key=lambda x: x.stat().st_mtime)\n",
    "    print(f\"📂 Loading data from: {latest_file.name}\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(latest_file)\n",
    "        print(f\"✅ Successfully loaded {len(df):,} records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "df = load_collected_data()\n",
    "if df is not None:\n",
    "    explore_collected_data(df)\n",
    "\n",
    "print(\"💡 Use load_collected_data() to load previously collected data\")\n",
    "print(\"💡 Use explore_collected_data(df) to analyze a dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Additional Utilities and Best Practices\n",
    "\n",
    "Helper functions for data management and research best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:22:53,316 - INFO - file_cache is only supported with oauth2client<4.0.0\n",
      "2025-07-21 17:22:53,779 - WARNING - Encountered 403 Forbidden with reason \"quotaExceeded\"\n",
      "2025-07-21 17:22:53,782 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key 1: quota_exceeded\n",
      "  Message: API quota exceeded. Try again later or use different key.\n",
      "  ❌ API key validation failed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 17:22:54,236 - WARNING - Encountered 403 Forbidden with reason \"quotaExceeded\"\n",
      "2025-07-21 17:22:54,238 - INFO - file_cache is only supported with oauth2client<4.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key 2: quota_exceeded\n",
      "  Message: API quota exceeded. Try again later or use different key.\n",
      "  ❌ API key validation failed.\n",
      "API Key 3: valid\n",
      "  Message: API key is working correctly\n",
      "  ✅ API key is valid and working.\n"
     ]
    }
   ],
   "source": [
    "def validate_api_quota(api_key: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Check API quota usage and limits.\n",
    "\n",
    "    Args:\n",
    "        api_key: YouTube API key to check\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with quota information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "        # Make a minimal request to check quota\n",
    "        response = youtube.search().list(part=\"snippet\", q=\"test\", maxResults=1).execute()\n",
    "\n",
    "        return {\"status\": \"valid\", \"test_successful\": True, \"message\": \"API key is working correctly\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "        if \"quota\" in error_msg:\n",
    "            return {\"status\": \"quota_exceeded\", \"test_successful\": False, \"message\": \"API quota exceeded. Try again later or use different key.\"}\n",
    "        elif \"invalid\" in error_msg or \"forbidden\" in error_msg:\n",
    "            return {\"status\": \"invalid\", \"test_successful\": False, \"message\": \"API key is invalid or has insufficient permissions.\"}\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"test_successful\": False, \"message\": f\"API test failed: {str(e)}\"}\n",
    "\n",
    "\n",
    "# Example usage of API quota validation\n",
    "if Config.API_KEYS:\n",
    "    for i, key in enumerate(Config.API_KEYS):\n",
    "        if key.strip():\n",
    "            result = validate_api_quota(key)\n",
    "            print(f\"API Key {i + 1}: {result['status']}\")\n",
    "            print(f\"  Message: {result['message']}\")\n",
    "            if result[\"test_successful\"]:\n",
    "                print(\"  ✅ API key is valid and working.\")\n",
    "            else:\n",
    "                print(\"  ❌ API key validation failed.\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No valid API keys found for quota validation.\")\n",
    "    print(\"Please set your YouTube API keys in environment variables:\")\n",
    "    print(\"export YOUTUBE_API_KEY_1='your_first_api_key'\")\n",
    "    print(\"export YOUTUBE_API_KEY_2='your_second_api_key'\")\n",
    "    print(\"export YOUTUBE_API_KEY_3='your_third_api_key'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-savio-youtube-KBt3GBFD-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
