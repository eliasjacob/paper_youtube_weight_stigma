{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Transcription Collection for YouTube Weight Stigma Research\n",
    "\n",
    "This notebook implements the video transcription collection pipeline for analyzing YouTube video content related to weight stigma research. The transcription data provides insights into the actual content being discussed in the videos, complementing the comment analysis.\n",
    "\n",
    "## Research Overview\n",
    "\n",
    "This study collects and processes video transcriptions to:\n",
    "- Analyze video content themes and language patterns\n",
    "- Compare video content with user comments\n",
    "- Identify weight stigma language in video discourse\n",
    "- Enable comprehensive content analysis\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "The transcription collection process includes:\n",
    "\n",
    "1. **Data Loading**: Load cleaned video metadata from previous steps\n",
    "2. **Transcript Collection**: Use YouTube Transcript API to collect Portuguese transcripts\n",
    "3. **Fallback Translation**: Translate non-Portuguese transcripts when needed\n",
    "4. **Data Processing**: Aggregate transcript segments into full video transcriptions\n",
    "5. **Quality Control**: Validate and clean transcription data\n",
    "6. **Export**: Save processed transcriptions for analysis\n",
    "\n",
    "## Input Data\n",
    "\n",
    "- **Source**: Cleaned video data from `02_basic_cleaning.ipynb`\n",
    "- **Expected location**: `../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet`\n",
    "- **Content**: Video IDs and metadata for transcription collection\n",
    "\n",
    "## Output Data\n",
    "\n",
    "- **Destination**: `../data/intermediate/20250417_youtube_transcriptions_no_labels.parquet`\n",
    "- **Content**: Processed video transcriptions with metadata\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- YouTube Transcript API (youtube-transcript-api)\n",
    "- Robust error handling for API limitations\n",
    "- Progress tracking with checkpointing for large datasets\n",
    "\n",
    "**Note**: Some videos may not have transcripts available. The pipeline handles these cases gracefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded and configuration set\n",
      "üìÅ Input file: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "üìÅ Output file: ../data/intermediate/20250417_youtube_transcriptions_no_labels.parquet\n",
      "üíæ Checkpoint file: ../data/tmp/transcription_video_processing_checkpoint.joblib\n",
      "‚è±Ô∏è Sleep interval: 2s\n",
      "üåê Translation fallback: True\n",
      "‚úÖ Input file found: 20250417_youtube_comments_pt_cleaned1.parquet\n",
      "üìä File size: 47.1 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path to import custom modules\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "# Import custom transcription utilities\n",
    "from transcription_utils import VideoTranscriptDownloader, process_transcripts_to_final_format\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Configuration class for transcription collection\n",
    "class TranscriptionConfig:\n",
    "    \"\"\"Configuration for video transcription collection pipeline.\"\"\"\n",
    "\n",
    "    # File paths\n",
    "    DATA_DIR = Path(\"../data\")\n",
    "    INTERMEDIATE_DATA_DIR = DATA_DIR / \"intermediate\"\n",
    "    TMP_DATA_DIR = DATA_DIR / \"tmp\"\n",
    "\n",
    "    # Input file (from cleaning notebook)\n",
    "    INPUT_FILE = INTERMEDIATE_DATA_DIR / \"20250417_youtube_comments_pt_cleaned1.parquet\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = INTERMEDIATE_DATA_DIR / \"20250417_youtube_transcriptions_no_labels.parquet\"\n",
    "\n",
    "    # Checkpoint file for progress tracking\n",
    "    CHECKPOINT_FILE = TMP_DATA_DIR / \"transcription_video_processing_checkpoint.joblib\"\n",
    "\n",
    "    # Processing parameters\n",
    "    SLEEP_INTERVAL = 2  # Seconds between API requests to avoid rate limiting\n",
    "    TRANSLATE_FALLBACK = True  # Whether to translate non-Portuguese transcripts\n",
    "    CHECKPOINT_FREQUENCY = 5  # Save progress every N videos\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        cls.INTERMEDIATE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        cls.TMP_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create directories\n",
    "TranscriptionConfig.create_directories()\n",
    "\n",
    "print(\"‚úÖ Libraries loaded and configuration set\")\n",
    "print(f\"üìÅ Input file: {TranscriptionConfig.INPUT_FILE}\")\n",
    "print(f\"üìÅ Output file: {TranscriptionConfig.OUTPUT_FILE}\")\n",
    "print(f\"üíæ Checkpoint file: {TranscriptionConfig.CHECKPOINT_FILE}\")\n",
    "print(f\"‚è±Ô∏è Sleep interval: {TranscriptionConfig.SLEEP_INTERVAL}s\")\n",
    "print(f\"üåê Translation fallback: {TranscriptionConfig.TRANSLATE_FALLBACK}\")\n",
    "\n",
    "# Verify input file exists\n",
    "if TranscriptionConfig.INPUT_FILE.exists():\n",
    "    print(f\"‚úÖ Input file found: {TranscriptionConfig.INPUT_FILE.name}\")\n",
    "    file_size = TranscriptionConfig.INPUT_FILE.stat().st_size / (1024 * 1024)  # MB\n",
    "    print(f\"üìä File size: {file_size:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Input file not found: {TranscriptionConfig.INPUT_FILE}\")\n",
    "    print(\"Please run the data cleaning notebook (02_basic_cleaning.ipynb) first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 07:07:24,696 - INFO - Loading video data from ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "2025-07-24 07:07:25,745 - INFO - ‚úÖ Loaded 191,946 comments\n",
      "2025-07-24 07:07:25,786 - INFO - üìπ Found 1,204 unique videos for transcription\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Video Data Sample:\n",
      "      video_id                                        video_title\n",
      "0  --tK3SaYWr4                 Tony Gordo √© Incriminado #simpsons\n",
      "1  -1DN4904BQw                 O pa√≠s mais obeso do mundo #shorts\n",
      "2  -4xj_teI1EQ  Preconceitos que eu j√° sofri por ser uma baila...\n",
      "3  -6Qxw7CpQvQ  esse milion√°rio de 18 anos n√£o quer pegar mulh...\n",
      "4  -7fJRjz1BCM  Lula volta a fazer piada com obesidade de Fl√°v...\n",
      "\n",
      "üìà Data Overview:\n",
      "- Total unique videos: 1,204\n",
      "- Total comments across videos: 191,946\n",
      "- Average comments per video: 159.4\n"
     ]
    }
   ],
   "source": [
    "def load_and_explore_video_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and explore video data for transcription collection.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the cleaned video comments data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with video metadata for transcription collection\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading video data from {file_path}\")\n",
    "\n",
    "    try:\n",
    "        # Load the cleaned comments data\n",
    "        df_comments = pd.read_parquet(file_path)\n",
    "        logger.info(f\"‚úÖ Loaded {len(df_comments):,} comments\")\n",
    "\n",
    "        # Extract unique video information\n",
    "        df_videos = df_comments[[\"video_id\", \"video_title\"]].drop_duplicates()\n",
    "        df_videos.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        logger.info(f\"üìπ Found {len(df_videos):,} unique videos for transcription\")\n",
    "\n",
    "        # Display sample data\n",
    "        print(\"\\nüìä Video Data Sample:\")\n",
    "        print(df_videos.head())\n",
    "\n",
    "        print(f\"\\nüìà Data Overview:\")\n",
    "        print(f\"- Total unique videos: {len(df_videos):,}\")\n",
    "        print(f\"- Total comments across videos: {len(df_comments):,}\")\n",
    "        print(f\"- Average comments per video: {len(df_comments) / len(df_videos):.1f}\")\n",
    "\n",
    "        return df_videos\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error loading video data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Load the video data\n",
    "df_videos = load_and_explore_video_data(TranscriptionConfig.INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Video Collection Summary:\n",
      "- Unique video IDs: 1,204\n",
      "- Videos with titles: 1,204\n",
      "- Videos without titles: 0\n",
      "‚úÖ No duplicate video IDs found\n",
      "\n",
      "üîç Sample Video IDs:\n",
      "- FWm_FWOyQL8: Perdendo peso e ganhando sa√∫de‚ù§Ô∏è‚Äçü©π #emagrecimento #vencendoa...\n",
      "- 2jYX2DHpQTA: Quais as doen√ßas associadas √† obesidade?\n",
      "- tp8aPwI3Mhg: DI√ÅRIO DA DIETA | ESTOU OBESA | PRECISO EMAGRECER 10KG\n",
      "- Ejq6pKngyXQ: O QUE UM OBESO MAIS PRECISA PARA EMAGRECER? ‚Äì IRONBERG PODCA...\n",
      "- Pv5iJPhhgp4: Con √Ånimo de Ofender : Cap #38 - Gordo, Gordo\n"
     ]
    }
   ],
   "source": [
    "# Analyze video distribution\n",
    "print(f\"üéØ Video Collection Summary:\")\n",
    "print(f\"- Unique video IDs: {df_videos['video_id'].nunique():,}\")\n",
    "print(f\"- Videos with titles: {df_videos['video_title'].notna().sum():,}\")\n",
    "print(f\"- Videos without titles: {df_videos['video_title'].isna().sum():,}\")\n",
    "\n",
    "# Check for any data quality issues\n",
    "if df_videos[\"video_id\"].duplicated().any():\n",
    "    logger.warning(\"‚ö†Ô∏è Duplicate video IDs found - this should not happen\")\n",
    "    duplicate_count = df_videos[\"video_id\"].duplicated().sum()\n",
    "    print(f\"‚ùå Found {duplicate_count} duplicate video IDs\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate video IDs found\")\n",
    "\n",
    "# Display video ID examples for validation\n",
    "print(f\"\\nüîç Sample Video IDs:\")\n",
    "sample_videos = df_videos.sample(min(5, len(df_videos)))\n",
    "for idx, row in sample_videos.iterrows():\n",
    "    print(f\"- {row['video_id']}: {row['video_title'][:60]}{'...' if len(str(row['video_title'])) > 60 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Video Transcription Collection\n",
    "\n",
    "This section uses the custom `VideoTranscriptDownloader` class to systematically collect video transcriptions with proper error handling and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing VideoTranscriptDownloader\n",
      "- Videos to process: 1,204\n",
      "- Checkpoint file: ../data/tmp/transcription_video_processing_checkpoint.joblib\n",
      "- Sleep interval: 2s\n",
      "‚úÖ VideoTranscriptDownloader initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the transcript downloader\n",
    "video_ids = df_videos[\"video_id\"].tolist()\n",
    "\n",
    "print(f\"üöÄ Initializing VideoTranscriptDownloader\")\n",
    "print(f\"- Videos to process: {len(video_ids):,}\")\n",
    "print(f\"- Checkpoint file: {TranscriptionConfig.CHECKPOINT_FILE}\")\n",
    "print(f\"- Sleep interval: {TranscriptionConfig.SLEEP_INTERVAL}s\")\n",
    "\n",
    "downloader = VideoTranscriptDownloader(video_ids=video_ids, checkpoint_file=TranscriptionConfig.CHECKPOINT_FILE, sleep_interval=TranscriptionConfig.SLEEP_INTERVAL)\n",
    "\n",
    "print(\"‚úÖ VideoTranscriptDownloader initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 07:07:29,549 - INFO - Resuming from checkpoint: ../data/tmp/transcription_video_processing_checkpoint.joblib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Starting transcript collection...\n",
      "This process may take some time depending on the number of videos.\n",
      "Progress is automatically saved - you can safely interrupt and resume.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 07:07:30,191 - INFO - All videos have already been processed.\n",
      "2025-07-24 07:07:30,192 - INFO - Processing complete. Total successes: 1011, Total failures: 193.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Transcript collection completed!\n",
      "- Raw transcript segments collected: 263,913\n",
      "- Videos with transcripts: 1,011\n",
      "- Average segments per video: 261.0\n",
      "\n",
      "üìã Sample Transcript Data:\n",
      "                                       text  start  duration     video_id\n",
      "0          em Springfield tem um batedor de   0.00      3.48  --tK3SaYWr4\n",
      "1  carteiras Ei cad√™ minha carteira √© minha   1.28      3.72  --tK3SaYWr4\n",
      "2     carteira subiu tamb√©m para pegar quem   3.48      2.96  --tK3SaYWr4\n",
      "3       est√° batendo carteiras os policiais   5.00      3.16  --tK3SaYWr4\n",
      "4        precisam de algu√©m como isca Vamos   6.44      3.04  --tK3SaYWr4\n"
     ]
    }
   ],
   "source": [
    "# Execute transcript collection with progress tracking\n",
    "print(\"üé¨ Starting transcript collection...\")\n",
    "print(\"This process may take some time depending on the number of videos.\")\n",
    "print(\"Progress is automatically saved - you can safely interrupt and resume.\")\n",
    "\n",
    "try:\n",
    "    # Download transcripts with translation fallback\n",
    "    df_raw_transcripts = downloader.download_transcripts(translate_to_pt=TranscriptionConfig.TRANSLATE_FALLBACK)\n",
    "\n",
    "    print(f\"\\n‚úÖ Transcript collection completed!\")\n",
    "    print(f\"- Raw transcript segments collected: {len(df_raw_transcripts):,}\")\n",
    "\n",
    "    if not df_raw_transcripts.empty:\n",
    "        print(f\"- Videos with transcripts: {df_raw_transcripts['video_id'].nunique():,}\")\n",
    "        print(f\"- Average segments per video: {len(df_raw_transcripts) / df_raw_transcripts['video_id'].nunique():.1f}\")\n",
    "\n",
    "        # Show sample transcript data\n",
    "        print(f\"\\nüìã Sample Transcript Data:\")\n",
    "        print(df_raw_transcripts.head())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No transcripts were collected. Check API availability and video accessibility.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Error during transcript collection: {e}\")\n",
    "    print(f\"‚ùå Transcript collection failed: {e}\")\n",
    "    print(\"Check the logs for detailed error information.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process and Aggregate Transcripts\n",
    "\n",
    "Convert raw transcript segments into consolidated video transcriptions with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing raw transcripts into final format...\n",
      "‚úÖ Transcript processing completed!\n",
      "- Final video transcripts: 1,011\n",
      "- Success rate: 84.0%\n",
      "\n",
      "üìä Transcript Statistics:\n",
      "- Average video duration: 1367.4 seconds (22.8 minutes)\n",
      "- Median video duration: 129.8 seconds (2.2 minutes)\n",
      "- Average transcript length: 9290 characters\n",
      "\n",
      "üìã Sample Processed Transcripts:\n",
      "\n",
      "üé¨ Video: O CAOS DO XIXI QUE ME TROUXE A RESPOSTA / MEU GATO OBESO   #...\n",
      "   Duration: 1025s\n",
      "   Transcript: come√ßando mais um vlog para voc√™s e esse gatinho a√≠ que voc√™s est√£o vendo √© o querido √© um gato que ...\n",
      "\n",
      "üé¨ Video: AUL√ÉO #001 - VOC√ä FOI PROGRAMADO PARA SER OBESO\n",
      "   Duration: 6307s\n",
      "   Transcript: G1 Oi tudo bem Boa noite E a√≠ G1 Oi eurice boa noite √© hoje s√≥ temos um teste n√© um per√≠odo de teste...\n",
      "\n",
      "üé¨ Video: Ex-obesa vira modelo #motiva√ß√£o #disciplina #inspira√ß√£o\n",
      "   Duration: 92s\n",
      "   Transcript: [M√∫sica] Mas essa √© uma daquelas baita hist√≥rias de supera√ß√£o o namorado que tanto ajudou Fl√°via mor...\n"
     ]
    }
   ],
   "source": [
    "# Process raw transcripts into final format\n",
    "if not df_raw_transcripts.empty:\n",
    "    print(\"üîÑ Processing raw transcripts into final format...\")\n",
    "\n",
    "    try:\n",
    "        # Use the utility function to process transcripts\n",
    "        df_final_transcripts = process_transcripts_to_final_format(df_raw_transcripts=df_raw_transcripts, df_video_info=df_videos)\n",
    "\n",
    "        print(f\"‚úÖ Transcript processing completed!\")\n",
    "        print(f\"- Final video transcripts: {len(df_final_transcripts):,}\")\n",
    "        print(f\"- Success rate: {len(df_final_transcripts) / len(df_videos) * 100:.1f}%\")\n",
    "\n",
    "        # Display processing statistics\n",
    "        if not df_final_transcripts.empty:\n",
    "            avg_duration = df_final_transcripts[\"duration\"].mean()\n",
    "            median_duration = df_final_transcripts[\"duration\"].median()\n",
    "            avg_transcript_length = df_final_transcripts[\"transcription\"].str.len().mean()\n",
    "\n",
    "            print(f\"\\nüìä Transcript Statistics:\")\n",
    "            print(f\"- Average video duration: {avg_duration:.1f} seconds ({avg_duration / 60:.1f} minutes)\")\n",
    "            print(f\"- Median video duration: {median_duration:.1f} seconds ({median_duration / 60:.1f} minutes)\")\n",
    "            print(f\"- Average transcript length: {avg_transcript_length:.0f} characters\")\n",
    "\n",
    "            # Show sample processed data\n",
    "            print(f\"\\nüìã Sample Processed Transcripts:\")\n",
    "            sample_df = df_final_transcripts.sample(min(3, len(df_final_transcripts)))\n",
    "            for idx, row in sample_df.iterrows():\n",
    "                print(f\"\\nüé¨ Video: {row['video_title'][:60]}{'...' if len(row['video_title']) > 60 else ''}\")\n",
    "                print(f\"   Duration: {row['duration']:.0f}s\")\n",
    "                print(f\"   Transcript: {row['transcription'][:100]}{'...' if len(row['transcription']) > 100 else ''}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error processing transcripts: {e}\")\n",
    "        print(f\"‚ùå Transcript processing failed: {e}\")\n",
    "        raise\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No raw transcripts available for processing.\")\n",
    "    df_final_transcripts = pd.DataFrame(columns=[\"video_id\", \"transcription\", \"duration\", \"video_title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating transcript data quality...\n",
      "\n",
      "üìã Data Quality Report:\n",
      "- Status: GOOD\n",
      "- Total transcripts: 1,011\n",
      "- Empty transcripts: 0\n",
      "- Empty titles: 0\n",
      "- Zero duration videos: 0\n",
      "- Duplicate videos: 0\n",
      "- Quality score: 100.00%\n",
      "‚úÖ Data quality is good - ready for analysis\n"
     ]
    }
   ],
   "source": [
    "# Perform data quality validation\n",
    "def validate_transcript_data(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate the quality of transcript data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with transcript data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "\n",
    "    if df.empty:\n",
    "        validation_results[\"status\"] = \"empty\"\n",
    "        validation_results[\"message\"] = \"No transcript data to validate\"\n",
    "        return validation_results\n",
    "\n",
    "    # Check for required columns\n",
    "    required_columns = [\"video_id\", \"transcription\", \"duration\", \"video_title\"]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        validation_results[\"status\"] = \"error\"\n",
    "        validation_results[\"message\"] = f\"Missing required columns: {missing_columns}\"\n",
    "        return validation_results\n",
    "\n",
    "    # Data quality checks\n",
    "    validation_results[\"total_transcripts\"] = len(df)\n",
    "    validation_results[\"empty_transcripts\"] = df[\"transcription\"].isna().sum()\n",
    "    validation_results[\"empty_titles\"] = df[\"video_title\"].isna().sum()\n",
    "    validation_results[\"zero_duration\"] = (df[\"duration\"] <= 0).sum()\n",
    "    validation_results[\"duplicate_videos\"] = df[\"video_id\"].duplicated().sum()\n",
    "\n",
    "    # Calculate quality metrics\n",
    "    valid_transcripts = len(df) - validation_results[\"empty_transcripts\"]\n",
    "    validation_results[\"quality_score\"] = valid_transcripts / len(df) if len(df) > 0 else 0\n",
    "\n",
    "    # Determine overall status\n",
    "    if validation_results[\"quality_score\"] >= 0.8:\n",
    "        validation_results[\"status\"] = \"good\"\n",
    "    elif validation_results[\"quality_score\"] >= 0.5:\n",
    "        validation_results[\"status\"] = \"acceptable\"\n",
    "    else:\n",
    "        validation_results[\"status\"] = \"poor\"\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "\n",
    "# Validate the transcript data\n",
    "print(\"üîç Validating transcript data quality...\")\n",
    "validation = validate_transcript_data(df_final_transcripts)\n",
    "\n",
    "print(f\"\\nüìã Data Quality Report:\")\n",
    "print(f\"- Status: {validation.get('status', 'unknown').upper()}\")\n",
    "\n",
    "if \"total_transcripts\" in validation:\n",
    "    print(f\"- Total transcripts: {validation['total_transcripts']:,}\")\n",
    "    print(f\"- Empty transcripts: {validation['empty_transcripts']:,}\")\n",
    "    print(f\"- Empty titles: {validation['empty_titles']:,}\")\n",
    "    print(f\"- Zero duration videos: {validation['zero_duration']:,}\")\n",
    "    print(f\"- Duplicate videos: {validation['duplicate_videos']:,}\")\n",
    "    print(f\"- Quality score: {validation['quality_score']:.2%}\")\n",
    "\n",
    "if validation[\"status\"] == \"good\":\n",
    "    print(\"‚úÖ Data quality is good - ready for analysis\")\n",
    "elif validation[\"status\"] == \"acceptable\":\n",
    "    print(\"‚ö†Ô∏è Data quality is acceptable - some issues detected\")\n",
    "elif validation[\"status\"] == \"poor\":\n",
    "    print(\"‚ùå Data quality is poor - significant issues detected\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è {validation.get('message', 'Validation completed')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Processed Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 07:07:38,902 - INFO - Existing file backed up to: ../data/intermediate/20250417_youtube_transcriptions_no_labels.backup_20250724_070738.parquet\n",
      "2025-07-24 07:07:39,007 - INFO - ‚úÖ Export successful: ../data/intermediate/20250417_youtube_transcriptions_no_labels.parquet\n",
      "2025-07-24 07:07:39,008 - INFO - üìä File size: 4.97 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting transcript data to: ../data/intermediate/20250417_youtube_transcriptions_no_labels.parquet\n",
      "‚úÖ Transcript data exported successfully!\n",
      "üìÅ Output file: ../data/intermediate/20250417_youtube_transcriptions_no_labels.parquet\n",
      "üìä Records exported: 1,011\n",
      "\n",
      "üèÅ Final Summary:\n",
      "- Videos processed: 1,204\n",
      "- Transcripts collected: 1,011\n",
      "- Success rate: 84.0%\n",
      "- Output file: 20250417_youtube_transcriptions_no_labels.parquet\n"
     ]
    }
   ],
   "source": [
    "def export_transcript_data(df: pd.DataFrame, output_path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Export transcript data with proper validation and backup.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to export\n",
    "        output_path: Path where to save the data\n",
    "\n",
    "    Returns:\n",
    "        True if export successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df.empty:\n",
    "            logger.warning(\"No transcript data to export\")\n",
    "            return False\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create backup if file already exists\n",
    "        if output_path.exists():\n",
    "            backup_path = output_path.with_suffix(f\".backup_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.parquet\")\n",
    "            output_path.rename(backup_path)\n",
    "            logger.info(f\"Existing file backed up to: {backup_path}\")\n",
    "\n",
    "        # Export to parquet\n",
    "        df.to_parquet(output_path, index=False)\n",
    "\n",
    "        # Verify export\n",
    "        file_size = output_path.stat().st_size / (1024 * 1024)  # MB\n",
    "        logger.info(f\"‚úÖ Export successful: {output_path}\")\n",
    "        logger.info(f\"üìä File size: {file_size:.2f} MB\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Export failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Export the processed transcript data\n",
    "if not df_final_transcripts.empty:\n",
    "    print(f\"üíæ Exporting transcript data to: {TranscriptionConfig.OUTPUT_FILE}\")\n",
    "\n",
    "    export_success = export_transcript_data(df_final_transcripts, TranscriptionConfig.OUTPUT_FILE)\n",
    "\n",
    "    if export_success:\n",
    "        print(\"‚úÖ Transcript data exported successfully!\")\n",
    "        print(f\"üìÅ Output file: {TranscriptionConfig.OUTPUT_FILE}\")\n",
    "        print(f\"üìä Records exported: {len(df_final_transcripts):,}\")\n",
    "\n",
    "        # Display final summary\n",
    "        print(f\"\\nüèÅ Final Summary:\")\n",
    "        print(f\"- Videos processed: {len(df_videos):,}\")\n",
    "        print(f\"- Transcripts collected: {len(df_final_transcripts):,}\")\n",
    "        print(f\"- Success rate: {len(df_final_transcripts) / len(df_videos) * 100:.1f}%\")\n",
    "        print(f\"- Output file: {TranscriptionConfig.OUTPUT_FILE.name}\")\n",
    "    else:\n",
    "        print(\"‚ùå Export failed - check logs for details\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No transcript data to export\")\n",
    "    print(\"This could be due to:\")\n",
    "    print(\"- API rate limiting\")\n",
    "    print(\"- Videos without available transcripts\")\n",
    "    print(\"- Network connectivity issues\")\n",
    "    print(\"- API key restrictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up temporary files...\n",
      "‚úÖ Checkpoint file archived: checkpoint_completed_20250724_070744.joblib\n",
      "\n",
      "üéØ Next Steps:\n",
      "1. ‚úÖ Video transcriptions collected and processed\n",
      "2. üìä Ready for content analysis and topic modeling\n",
      "3. üîç Can proceed with zero-shot classification\n",
      "4. üìà Combine with comment data for comprehensive analysis\n",
      "\n",
      "üìÅ Output Files Created:\n",
      "- Main output: ../data/intermediate/20250417_youtube_transcriptions_no_labels.parquet\n",
      "\n",
      "üí° Research Applications:\n",
      "- Content analysis of weight stigma themes\n",
      "- Comparison between video content and user comments\n",
      "- Language pattern analysis in Portuguese content\n",
      "- Sentiment analysis across video discourse\n",
      "\n",
      "üîß Pipeline Complete! ‚ú®\n"
     ]
    }
   ],
   "source": [
    "# Optional: Clean up checkpoint files after successful completion\n",
    "def cleanup_checkpoint_files():\n",
    "    \"\"\"Clean up temporary checkpoint files.\"\"\"\n",
    "    try:\n",
    "        if TranscriptionConfig.CHECKPOINT_FILE.exists():\n",
    "            # Archive instead of delete for debugging purposes\n",
    "            archive_name = f\"checkpoint_completed_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.joblib\"\n",
    "            archive_path = TranscriptionConfig.CHECKPOINT_FILE.parent / archive_name\n",
    "            TranscriptionConfig.CHECKPOINT_FILE.rename(archive_path)\n",
    "            print(f\"‚úÖ Checkpoint file archived: {archive_name}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No checkpoint file to clean up\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"‚ö†Ô∏è Could not clean up checkpoint file: {e}\")\n",
    "\n",
    "\n",
    "# Clean up if export was successful\n",
    "if not df_final_transcripts.empty and TranscriptionConfig.OUTPUT_FILE.exists():\n",
    "    print(\"üßπ Cleaning up temporary files...\")\n",
    "    cleanup_checkpoint_files()\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"1. ‚úÖ Video transcriptions collected and processed\")\n",
    "print(\"2. üìä Ready for content analysis and topic modeling\")\n",
    "print(\"3. üîç Can proceed with zero-shot classification\")\n",
    "print(\"4. üìà Combine with comment data for comprehensive analysis\")\n",
    "\n",
    "if not df_final_transcripts.empty:\n",
    "    print(f\"\\nüìÅ Output Files Created:\")\n",
    "    print(f\"- Main output: {TranscriptionConfig.OUTPUT_FILE}\")\n",
    "    if TranscriptionConfig.CHECKPOINT_FILE.exists():\n",
    "        print(f\"- Checkpoint: {TranscriptionConfig.CHECKPOINT_FILE}\")\n",
    "\n",
    "print(\"\\nüí° Research Applications:\")\n",
    "print(\"- Content analysis of weight stigma themes\")\n",
    "print(\"- Comparison between video content and user comments\")\n",
    "print(\"- Language pattern analysis in Portuguese content\")\n",
    "print(\"- Sentiment analysis across video discourse\")\n",
    "\n",
    "print(\"\\nüîß Pipeline Complete! ‚ú®\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_youtube_weight_stigma_1e733e1bf2b6c34f6bcb8483dce2a479",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
