{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification for YouTube Comments Analysis\n",
    "\n",
    "This notebook implements zero-shot classification of YouTube comments for weight stigma research using OpenAI's GPT models. The pipeline analyzes comment sentiment, detects weight-based discrimination (gordofobia), identifies language, and flags obesity-related content.\n",
    "\n",
    "## Research Overview\n",
    "\n",
    "This study applies advanced natural language processing techniques to:\n",
    "- Classify sentiment in Portuguese YouTube comments (positive, negative, neutral)\n",
    "- Detect explicit and implicit weight-based discrimination (gordofobia)\n",
    "- Identify language patterns in multilingual comment datasets\n",
    "- Flag obesity-related discussions for focused analysis\n",
    "- Enable large-scale content analysis using batch processing\n",
    "\n",
    "## Classification Framework\n",
    "\n",
    "The zero-shot classification system includes:\n",
    "\n",
    "1. **Sentiment Analysis**: Classify comments as positive, negative, or neutral\n",
    "2. **Weight Discrimination Detection**: Identify explicit and implicit gordofobia\n",
    "3. **Language Identification**: Detect comment language using ISO codes\n",
    "4. **Obesity Content Flagging**: Mark comments discussing obesity topics\n",
    "5. **Batch Processing**: Efficient processing using OpenAI's Batch API\n",
    "\n",
    "## Input Data\n",
    "\n",
    "- **Source**: Cleaned Portuguese comments from `02_basic_cleaning.ipynb`\n",
    "- **Expected location**: `../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet`\n",
    "- **Content**: Preprocessed YouTube comments ready for analysis\n",
    "\n",
    "## Output Data\n",
    "\n",
    "- **Destination**: `../data/intermediate/20250417_youtube_comments_yes_labels.parquet`\n",
    "- **Content**: Original comments with classification labels and metadata\n",
    "\n",
    "## Technical Requirements\n",
    "\n",
    "- OpenAI API access with sufficient batch processing quota\n",
    "- Pydantic for structured data validation\n",
    "- LangChain for prompt engineering and API integration\n",
    "- Robust error handling for large-scale batch processing\n",
    "\n",
    "## Classification Schema\n",
    "\n",
    "The system uses a structured Pydantic model to ensure consistent outputs:\n",
    "- **sentimento**: Sentiment classification (positivo/negativo/neutro)\n",
    "- **gordofobia_implicita**: Boolean flag for implicit weight discrimination\n",
    "- **gordofobia_explicita**: Boolean flag for explicit weight discrimination\n",
    "- **idioma**: Language code (ISO 639-1 format)\n",
    "- **obesidade**: Boolean flag for obesity-related content\n",
    "\n",
    "**Note**: This notebook processes large volumes of text data. Batch processing is used to optimize costs and API efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded successfully\n",
      "ü§ñ Using model: gpt-4.1-mini\n",
      "üìÅ Working directory: /media/nas-elias/pesquisas/papers/paper_savio_youtube/paper_youtube_weight_stigma\n",
      "‚úÖ Configuration initialized\n",
      "üìÇ Input file: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "üìÇ Output file: ../data/intermediate/20250417_youtube_comments_yes_labels.parquet\n",
      "üî¢ Batch size: 40,000\n",
      "üå°Ô∏è Temperature: 0.0\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Dict, Any\n",
    "from glob import glob\n",
    "\n",
    "# Data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import joblib\n",
    "\n",
    "# API and ML libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Custom modules\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "from openai_api import OpenAIBatchProcessor\n",
    "\n",
    "# Jupyter notebook utilities\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration for zero-shot classification\n",
    "MODEL_NAME = \"gpt-4.1-mini\"  # Updated to current model version\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")\n",
    "print(f\"ü§ñ Using model: {MODEL_NAME}\")\n",
    "print(f\"üìÅ Working directory: {Path.cwd()}\")\n",
    "\n",
    "\n",
    "# Configuration class for the classification pipeline\n",
    "class ClassificationConfig:\n",
    "    \"\"\"Configuration for zero-shot comment classification pipeline.\"\"\"\n",
    "\n",
    "    # File paths\n",
    "    DATA_DIR = Path(\"../data\")\n",
    "    INTERMEDIATE_DATA_DIR = DATA_DIR / \"intermediate\"\n",
    "    TMP_DATA_DIR = DATA_DIR / \"tmp\"\n",
    "    JSONL_DIR = INTERMEDIATE_DATA_DIR / \"jsonl\"\n",
    "\n",
    "    # Input file (from cleaning notebook)\n",
    "    INPUT_FILE = INTERMEDIATE_DATA_DIR / \"20250417_youtube_comments_pt_cleaned1.parquet\"\n",
    "\n",
    "    # Output file\n",
    "    OUTPUT_FILE = INTERMEDIATE_DATA_DIR / \"20250417_youtube_comments_yes_labels.parquet\"\n",
    "\n",
    "    # Temporary files for batch processing\n",
    "    PARSED_RESULTS_FILE = TMP_DATA_DIR / \"parsed_results_comments.joblib\"\n",
    "    RESULTS_FILE = TMP_DATA_DIR / \"results_comments.joblib\"\n",
    "\n",
    "    # Batch processing parameters\n",
    "    BATCH_SIZE = 40000  # Maximum requests per batch file\n",
    "    BATCH_NAME_PREFIX = \"20250417_youtube_comments_batch_api\"\n",
    "\n",
    "    # Model parameters\n",
    "    MODEL_NAME = MODEL_NAME\n",
    "    TEMPERATURE = 0.0  # Deterministic outputs for research\n",
    "\n",
    "    @classmethod\n",
    "    def create_directories(cls):\n",
    "        \"\"\"Create necessary directories for processing.\"\"\"\n",
    "        cls.INTERMEDIATE_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        cls.TMP_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        cls.JSONL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create directories\n",
    "ClassificationConfig.create_directories()\n",
    "\n",
    "print(\"‚úÖ Configuration initialized\")\n",
    "print(f\"üìÇ Input file: {ClassificationConfig.INPUT_FILE}\")\n",
    "print(f\"üìÇ Output file: {ClassificationConfig.OUTPUT_FILE}\")\n",
    "print(f\"üî¢ Batch size: {ClassificationConfig.BATCH_SIZE:,}\")\n",
    "print(f\"üå°Ô∏è Temperature: {ClassificationConfig.TEMPERATURE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cb287a66-598a-4134-a28e-ccd233a48af8",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "1",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Chefe wigol deu um beijo grego no homer skksks",
         "Chefe wigol deu um beijo grego no homer skksks",
         "@MrLopess00",
         "https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO5lyhiApXfoR1zZJKsEBbZjpx-DQR3Rt_ajq96pk7PIl-26av=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@MrLopess00",
         "UCtrByOsq8kIDCQfSXfq3IKw",
         "True",
         "none",
         "447.0",
         "2024-12-29 02:00:55+00:00",
         "2024-12-29 02:00:55+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "2",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem era Batedor de Carteiras ?",
         "Quem era Batedor de Carteiras ?",
         "@mateuss.santossilva5059",
         "https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_2IMVBe6FpUQbhn3VYstkZM0AGckW3SDAPtmlRlxw4Y9GOUlO=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@mateuss.santossilva5059",
         "UCIY2M7NurJ728_H4Cs5zmQA",
         "True",
         "none",
         "5.0",
         "2024-12-29 12:53:19+00:00",
         "2024-12-29 12:53:19+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "3",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"Gra√ßas a deus que essa coisa est√° do nosso ladoüò®\" kskskskksjsks",
         "\"Gra√ßas a deus que essa coisa est√° do nosso ladoüò®\" kskskskksjsks",
         "@Ray._Ryan000",
         "https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX-ohqT8ffBd6vJTL5z8Rd2J8UK1CuBIftClOTcDqU_3wzrP2i7g=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Ray._Ryan000",
         "UCr5gdJ-I9wpcBXWdSR6T5iw",
         "True",
         "none",
         "1677.0",
         "2024-12-29 16:16:06+00:00",
         "2024-12-29 16:17:20+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "4",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "ü§® t√° estranho isso",
         "ü§® t√° estranho isso",
         "@darkgacha5649",
         "https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOHyOkDBA0ibu_cM1LtoM-Ar3C_3fH8CJtikMtCSMztWt7nhu-tvA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@darkgacha5649",
         "UCZnl2qgkPiF-SYyoPmTbzBw",
         "True",
         "none",
         "0.0",
         "2024-12-29 18:03:52+00:00",
         "2024-12-29 18:03:52+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "5",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Esse Final nossaaa üôÅü§£",
         "Esse Final nossaaa üôÅü§£",
         "@DavideSilva-gs6qn",
         "https://yt3.ggpht.com/ytc/AIdro_nq1ujPdQHIVfcTYvQJ8AXejYHLogvID_ajD0NgZ11puyy0975KTEngMVn1OBn_mEd1AA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@DavideSilva-gs6qn",
         "UCy6mpGDowUhmqV-b75veuXQ",
         "True",
         "none",
         "29.0",
         "2024-12-29 19:07:47+00:00",
         "2024-12-29 19:07:47+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "6",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem batia carteira?",
         "Quem batia carteira?",
         "@tapiocaplays5611",
         "https://yt3.ggpht.com/BV9AJPZfmO8-Oto64nHQYU0Iq1OOej067ujmbpIXi9GLxDKaQn86vD88D1bf43qxQ8briI2L=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@tapiocaplays5611",
         "UCu5RDxw2xbmslyoYRfBu6sw",
         "True",
         "none",
         "1.0",
         "2024-12-29 20:36:38+00:00",
         "2024-12-29 20:36:38+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "7",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem era o ladr√£o no final?",
         "Quem era o ladr√£o no final?",
         "@IsraelDaSilva1",
         "https://yt3.ggpht.com/ytc/AIdro_nSNiYSvPvCCpOaA3XQDR5Z1JbETTCUrC0epju2BYRcUAhWBpYB6ETxcv4epfmtCNjofw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@IsraelDaSilva1",
         "UCcPBxrmm3F8lF29EEnkVOOg",
         "True",
         "none",
         "5.0",
         "2024-12-29 23:47:26+00:00",
         "2024-12-29 23:47:26+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "8",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Observa√ß√£o... wygol e tony gordo s√£o grandes amigos do homer",
         "Observa√ß√£o... wygol e tony gordo s√£o grandes amigos do homer",
         "@YagoHiegue",
         "https://yt3.ggpht.com/dkjqqpVqC_67PbbAjd5i4SaZla3jZqEwGW2jRDvDg4F2-Wfp0TXZp8QA8mW5qLTwFS9VvAbqSA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@YagoHiegue",
         "UCWM9ETJ_9mryz3LTjasbEOQ",
         "True",
         "none",
         "181.0",
         "2024-12-30 00:09:05+00:00",
         "2024-12-30 00:09:05+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "9",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Famoso beijo grego",
         "Famoso beijo grego",
         "@Maloco_ofc",
         "https://yt3.ggpht.com/ZoZ4PytHisfUfR9KQPomAB6u_6RFRfCwHob7QiFCqdCQvgsHpzvlCjkcKHeqjA75MUVJGfcL=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Maloco_ofc",
         "UCo-DAE1Z07n1LDlE-PPNVVA",
         "True",
         "none",
         "4.0",
         "2024-12-30 08:57:27+00:00",
         "2024-12-30 08:57:27+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "10",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Ele nunca deu sinais ele:",
         "Ele nunca deu sinais ele:",
         "@RenatoCampineirodeMoura-b2b",
         "https://yt3.ggpht.com/ytc/AIdro_ktgf3psRkAgKqgtBrHqhFCpTwn8CtlBGMPzkRqQ28gVAm_OArHXAgXv_ztPUnBWg1sIA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@RenatoCampineirodeMoura-b2b",
         "UC0RpM0SRIBVSEIWjU2v34Nw",
         "True",
         "none",
         "9.0",
         "2024-12-30 13:05:17+00:00",
         "2024-12-30 13:05:17+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "11",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "*O meu amigo numca deu sinais*\nEle:",
         "*O meu amigo numca deu sinais*\nEle:",
         "@gabrielantoniocatarina3966",
         "https://yt3.ggpht.com/c0ZaG7YN0WgMQW0Nlu87gB3sLQWGgYtAxlh7axl8EvTjn5DR5xzFGGxhAPBrbGntJTzzUGly=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@gabrielantoniocatarina3966",
         "UCtU8K4RoXMWo-YQZmI7hY7A",
         "True",
         "none",
         "0.0",
         "2024-12-30 14:14:04+00:00",
         "2024-12-30 14:14:04+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "12",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Foi para isso que voce nasceu nao tankei",
         "Foi para isso que voce nasceu nao tankei",
         "@tiagocarvalho5467",
         "https://yt3.ggpht.com/ytc/AIdro_ntgPrUcHyzZ9-vVtMugfOvEyfrYUXSF9CNTYHBl3ld87eoFostv-kIwHbSagae3U3GEw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@tiagocarvalho5467",
         "UCkSqTLE9WfLWQQcoWlhhMbg",
         "True",
         "none",
         "0.0",
         "2024-12-30 16:08:54+00:00",
         "2024-12-30 16:08:54+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "13",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"pq o traseiro do seu marido √© maior do q o de nois duas juntas\" cortou a melhor parte kkkk",
         "\"pq o traseiro do seu marido √© maior do q o de nois duas juntas\" cortou a melhor parte kkkk",
         "@Sardinha99",
         "https://yt3.ggpht.com/lTZtptVcmiiX39XnVtop12gCSYREcrsI7Hv2mr8qxTDpK0geu5kS3EcPzOLXqqKuTlWF5H8dLg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Sardinha99",
         "UCHfHImttvWfrD3EQn2qGEfA",
         "True",
         "none",
         "683.0",
         "2024-12-30 17:01:59+00:00",
         "2024-12-30 17:01:59+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "14",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Eles comem  ent√£o n√£o √© desperd√≠cio",
         "Eles comem  ent√£o n√£o √© desperd√≠cio",
         "@brunopvp7458",
         "https://yt3.ggpht.com/mHUYE9T97WhEvM064tDPbZpjfCgsYzTVYQd_qjLrh0GDA1YUr43cEgHA5Wiynsm_jA2H8eKZ=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@brunopvp7458",
         "UCTN6jjnGBWa-RXzhXXf81Yg",
         "True",
         "none",
         "0.0",
         "2024-12-30 23:01:44+00:00",
         "2024-12-30 23:01:44+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "15",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "N√£o esperava que o Luigi Risotto fosse membro da mafia, achava que ele era so cozinheiro üò¨",
         "N√£o esperava que o Luigi Risotto fosse membro da mafia, achava que ele era so cozinheiro üò¨",
         "@pericolloputo2069",
         "https://yt3.ggpht.com/6PWlA91INWThTEK7qzBRTX0O6zoZ7eLGx2lJLzl_RP68rqt_LH760SkCse5Z8gwba43CiXMjog=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@pericolloputo2069",
         "UCMcfx-vw14NgazALxlcdneQ",
         "True",
         "none",
         "112.0",
         "2024-12-31 09:35:27+00:00",
         "2024-12-31 09:35:27+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "16",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Eles acusando uma chefe da mafia profissional por crime de crian√ßa pro n√≠vel dele",
         "Eles acusando uma chefe da mafia profissional por crime de crian√ßa pro n√≠vel dele",
         "@gbanimation8486",
         "https://yt3.ggpht.com/ytc/AIdro_nO1MsW2FSKZ0tOtoZxUlBsP-XPX2oa14OJ4Tbsf01MS-U=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@gbanimation8486",
         "UCV9lEJJvgKKW5h5xPFZuyqw",
         "True",
         "none",
         "8.0",
         "2025-01-01 05:18:17+00:00",
         "2025-01-01 05:18:17+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "17",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Por que o cara n√£o coloca qual epis√≥dio de qual temporada? Pqp ein, quanto desservi√ßo.",
         "Por que o cara n√£o coloca qual epis√≥dio de qual temporada? Pqp ein, quanto desservi√ßo.",
         "@judeuaskenazita",
         "https://yt3.ggpht.com/ytc/AIdro_m9S2UCWKacoFGDqGasujqoPRc4Olfv461aEbyNYeQzZe2cZ6rr4QmeW_WykW2eQpc4KkY=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@judeuaskenazita",
         "UCHtuEAjVV11uTRObI7ut9yA",
         "True",
         "none",
         "0.0",
         "2025-01-01 06:28:00+00:00",
         "2025-01-01 06:28:00+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "18",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\" Gra√ßas a Deus que essa coisa est√° do nosso lado \" e o pior Chef sugou oque  ? üíÄ",
         "\" Gra√ßas a Deus que essa coisa est√° do nosso lado \" e o pior Chef sugou oque  ? üíÄ",
         "@eo-Dan",
         "https://yt3.ggpht.com/IRz2yPHEb4lJnmQKRsm3ik7zx6-YGp30W8iaZm_mOzhz4DPi4Xx9x20gxQfV11ro9nxKSKownQ=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@eo-Dan",
         "UC68loV447_u1dQW0JHMZJ1Q",
         "True",
         "none",
         "32.0",
         "2025-01-01 17:57:37+00:00",
         "2025-01-01 17:59:02+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "19",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Qual epis√≥dio/temporada?",
         "Qual epis√≥dio/temporada?",
         "@morning_shot",
         "https://yt3.ggpht.com/mAN5Iaoo0maOp8tSv_hOuQqznYqDD-fx1Shu1TQ5CciZD_EcA0HYkogzih01zYT8Rxp_-Vsmow=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@morning_shot",
         "UC1Ycb8kQtg-ojm23EFqUK7A",
         "True",
         "none",
         "3.0",
         "2025-01-02 16:52:24+00:00",
         "2025-01-02 16:52:24+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "20",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"gracas a deus essa coisa ta do nosso lado\"",
         "\"gracas a deus essa coisa ta do nosso lado\"",
         "@GSMlindao",
         "https://yt3.ggpht.com/5YmqgrpF01UHYXk2URWXuwmfecY6Gy76JMRUkuTN9DblLz7VhpYGxtefnXoGvVIytdIRWbpJTw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@GSMlindao",
         "UCM87KYQTwuk8P_r69lLH0ew",
         "True",
         "none",
         "25.0",
         "2025-01-03 02:00:35+00:00",
         "2025-01-03 02:00:35+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "21",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Qual √© o epis√≥dio e a temporada?",
         "Qual √© o epis√≥dio e a temporada?",
         "@loganx5567",
         "https://yt3.ggpht.com/ytc/AIdro_nfIK5rO00RGBZdyr73HNCsU6eVXAUX64WK4w0avaczJMY=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@loganx5567",
         "UCFLkUpeM3KtIofD8u_h0ePg",
         "True",
         "none",
         "4.0",
         "2025-01-03 16:27:07+00:00",
         "2025-01-03 16:27:07+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "22",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "E quem que tava roubando a final?",
         "E quem que tava roubando a final?",
         "@ContanovaComfi",
         "https://yt3.ggpht.com/ytc/AIdro_l54JuFkfz_MPuEzHjXdxfvxURY00Junn3arhXrUYET02NzrFzJeqJsa24OyzDDUvdakg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@ContanovaComfi",
         "UCtCLWn5m9sB3SSzNBabRljA",
         "True",
         "none",
         "0.0",
         "2025-01-03 17:47:03+00:00",
         "2025-01-03 17:47:03+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "23",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"Algu√©m tem que tirar a bala sugando.\" kkkkkk",
         "\"Algu√©m tem que tirar a bala sugando.\" kkkkkk",
         "@Lover117-v6l",
         "https://yt3.ggpht.com/uK7sqGUGG8IDMLrUNPADl69mgArJj-FsqF9lJxWbgqDyf6T-8YdvA9APiJPNoqWSQnqODl6S2vg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Lover117-v6l",
         "UCd1Rh0H0r1PgmzO6oC1aHwA",
         "True",
         "none",
         "1.0",
         "2025-01-03 17:56:09+00:00",
         "2025-01-03 17:56:09+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "24",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Ta porra como aquele fuzil cabeu no chap√©u do chef?",
         "Ta porra como aquele fuzil cabeu no chap√©u do chef?",
         "@Guilhermemorbach_07",
         "https://yt3.ggpht.com/vs2IWwq56bQUFsIzu7f-9qg4potQZUkT1m4hPufHH60LGfzSiCDfEOfrW6yZHLFQ4MLQoJxU=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Guilhermemorbach_07",
         "UC0N9scCFjC_X8e20nZN3mUg",
         "True",
         "none",
         "16.0",
         "2025-01-03 18:57:51+00:00",
         "2025-01-03 18:57:51+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "25",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Jorny no finalüé∂üé∂",
         "Jorny no finalüé∂üé∂",
         "@J√∫liocesarSantana-b2x",
         "https://yt3.ggpht.com/oe8VVYcVz6_YU87trXqal_PuBl28SBAYJkg2jPYu8q5bEXdlEcEWBbZtWXPw0TqbGW2oPC61Wg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@J%C3%BAliocesarSantana-b2x",
         "UCuUHylOBW-qykp7v8ysew_w",
         "True",
         "none",
         "0.0",
         "2025-01-08 22:14:53+00:00",
         "2025-01-08 22:14:53+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "26",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Mas quem roubava as carteiras???",
         "Mas quem roubava as carteiras???",
         "@guilhermeam761",
         "https://yt3.ggpht.com/n04E215M7pxvHHPQSvXYmrTpRBczCcp_dDoUODfTSDvKjChQJ7UQo8xnV-Rq97Ck-6-uCQmfYw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@guilhermeam761",
         "UC4Ze0OPPoxCiN4RhkB62I9A",
         "True",
         "none",
         "7.0",
         "2025-01-10 11:09:25+00:00",
         "2025-01-10 11:09:25+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "27",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Simpsons e legal",
         "Simpsons e legal",
         "@RafaelBarbosa-b6y",
         "https://yt3.ggpht.com/ytc/AIdro_ktgf3psRkAgKqgtBrHqhFCpTwn8CtlBGMPzkRqQ28gVAm_OArHXAgXv_ztPUnBWg1sIA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@RafaelBarbosa-b6y",
         "UCmvm-kok1Xagf6f6beSEx5g",
         "True",
         "none",
         "1.0",
         "2025-01-13 21:25:05+00:00",
         "2025-01-13 21:25:05+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "28",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem roubou as carteiras?",
         "Quem roubou as carteiras?",
         "@ricardoinf1012",
         "https://yt3.ggpht.com/ytc/AIdro_nW4hkSr5R_5Y20fJCxYCVQ6tPXJBZkq3mPOMHPRD6be90=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@ricardoinf1012",
         "UCqloDDbx5UzjYNuUCCzOGEw",
         "True",
         "none",
         "1.0",
         "2025-01-14 23:32:23+00:00",
         "2025-01-14 23:32:23+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "29",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"Gra√ßas a deus que essa coisa esta do nosso ladoüò∞üò∞\"",
         "\"Gra√ßas a deus que essa coisa esta do nosso ladoüò∞üò∞\"",
         "@beckaveloso1078",
         "https://yt3.ggpht.com/g-SBxsg6ERNslw4EYxTC4EOjQwj70KHR2a6659C-D1qpGYedfOWsvyhB4i3Zxt3L11f4WtnUHQ=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@beckaveloso1078",
         "UClOIaxv-ANDoQzIh0R-7c4g",
         "True",
         "none",
         "4.0",
         "2025-01-29 17:24:09+00:00",
         "2025-01-29 17:24:09+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "30",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Quem era o batedor de carteira?",
         "Quem era o batedor de carteira?",
         "@bulbaxChill",
         "https://yt3.ggpht.com/r1cInAhsr7xrydqoNyAJa9qlaOmRSxsO6LQUPb3PkwztvYeSLoPAnrlumaR_gCsodgHkobID=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@bulbaxChill",
         "UCXRqAHSgtuajlVPtDKzbDUw",
         "True",
         "none",
         "1.0",
         "2025-02-03 07:01:00+00:00",
         "2025-02-03 07:01:00+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "31",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Isso quer dizer que o chefe √© secretamente o \"Chupa-c# de Goianinha\"...",
         "Isso quer dizer que o chefe √© secretamente o \"Chupa-c# de Goianinha\"...",
         "@felipesimoes6890",
         "https://yt3.ggpht.com/ytc/AIdro_m72bpsOKZzS2PtQfjaj39tuV5qgI0uWir78uCbwKA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@felipesimoes6890",
         "UCRKjqCJrWzBLmZDTd-rL_FA",
         "True",
         "none",
         "0.0",
         "2025-02-08 16:05:47+00:00",
         "2025-02-08 16:05:47+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "32",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Ele ficou com a bala na boca at√© chegar no hospital kkkkkkkkkkk",
         "Ele ficou com a bala na boca at√© chegar no hospital kkkkkkkkkkk",
         "@senhorferrari2361",
         "https://yt3.ggpht.com/XJWDHJ8bvSIYsb6hIEUDK1xg6aAE7aN8FbJfZ0FSA2YGEbTv5oXtPPiN6OvputWwelBB5w1AQw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@senhorferrari2361",
         "UCakyzCSUjM_wzmLT_7F0aoA",
         "True",
         "none",
         "28.0",
         "2025-02-11 07:44:46+00:00",
         "2025-02-11 07:44:46+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "33",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "t√°, mas quem tava roubando as carteira",
         "t√°, mas quem tava roubando as carteira",
         "@OkamiBin",
         "https://yt3.ggpht.com/inij3Z-JTrVAS87cstAbN3h_1yaLGRI54_JYXiwyVJYZasK9je5oP9jkgHpN9WefeGgOuEK0=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@OkamiBin",
         "UCMzMqhZvofAokJ2TcD6QfHg",
         "True",
         "none",
         "5.0",
         "2025-02-12 14:13:18+00:00",
         "2025-02-12 14:13:18+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "34",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "√â a√≠ que o Wiggum erra mais uma vez, o Tony Gordo jamais bateria carteiras, j√° o Tony sarado gordo √© outra hist√≥ria",
         "√â a√≠ que o Wiggum erra mais uma vez, o Tony Gordo jamais bateria carteiras, j√° o Tony sarado gordo √© outra hist√≥ria",
         "@calangoraivoso9921",
         "https://yt3.ggpht.com/O1GR3SDRFTHz2lLmwJX2lbZmD2jc9d07X9RADfkAIOu_eibrZ1r9RVaXb4RVoeiktKmDZodkrvA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@calangoraivoso9921",
         "UC5ceQjUaved2-kNw60OWCjg",
         "True",
         "none",
         "1.0",
         "2025-02-26 03:48:23+00:00",
         "2025-02-26 03:48:23+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "35",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Tava geral armado",
         "Tava geral armado",
         "@Jo√£oPedroCamargoPinheiro",
         "https://yt3.ggpht.com/MyKchin0utg_8dbGIHq3J9d7zIwMXrw-AOtIlZw6jxb3T038gFL0iLEgnIlPJrI3bebg3xVI=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Jo%C3%A3oPedroCamargoPinheiro",
         "UC2ZALQI21uHBG2lDLeVLnCQ",
         "True",
         "none",
         "0.0",
         "2025-03-21 00:40:45+00:00",
         "2025-03-21 00:40:45+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "36",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Tony D'Amico √© o nome completo dele.",
         "Tony D'Amico √© o nome completo dele.",
         "@BLUSH3D",
         "https://yt3.ggpht.com/3bdo_yFl6F-eurIML1KrkkFkF5ThOvCKMffrGOUJzJiIFihs2v0dObpMjy92BCjCYpKSsfNM7B8=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@BLUSH3D",
         "UCJdXqMrn2XEWk6ZxaM_JYHA",
         "True",
         "none",
         "1.0",
         "2025-03-21 02:00:00+00:00",
         "2025-03-21 02:00:00+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "37",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Mano que final √© esse? üíÄ",
         "Mano que final √© esse? üíÄ",
         "@Mundonas4rodas",
         "https://yt3.ggpht.com/3iNR5M1ZWY-KBLfsYzrh6ecJm_PFu2t2npajhfS08bHJmcoaJlUSrEsOIcACTqqhnSK7IArE=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Mundonas4rodas",
         "UCR9H15f6RdgiW81fSF8i89A",
         "True",
         "none",
         "0.0",
         "2025-04-06 10:25:36+00:00",
         "2025-04-06 10:25:36+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "38",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Que nojo cara tirou do c#",
         "Que nojo cara tirou do c#",
         "@Daviivanilda2",
         "https://yt3.ggpht.com/cl-WTqx3EVjIxyv9ulpeUyw8Yi0kjJNCo88qhtAFUZbRmslBLppQsLfQPOunEF-cxu_Jssolhw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Daviivanilda2",
         "UCh2GkNrJwehmhK0rO-TY6fQ",
         "True",
         "none",
         "0.0",
         "2025-04-08 12:09:19+00:00",
         "2025-04-08 12:09:19+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "39",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Seu her√≥i foi pra isso que voc√™ nasceu üòÇ",
         "Seu her√≥i foi pra isso que voc√™ nasceu üòÇ",
         "@MustangDiamond",
         "https://yt3.ggpht.com/Y5MOHqgFHSYnkDEnTCvOcce8EdahyBxgMILDIKA248uEFUtvEJ2a8nkvoLcDemIF_iQ2DdXP=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@MustangDiamond",
         "UC-THrP36QJVLzjo05X1UjoA",
         "True",
         "none",
         "2.0",
         "2025-04-08 14:11:12+00:00",
         "2025-04-08 14:11:12+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "40",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "\"uhh, molho :D\"\n\" Algu√©m tem que tirar a bala sugando ü§®\"\n\" Foi pra isso mesmo que eu nasci üíÄüíÄ\"",
         "\"uhh, molho :D\"\n\" Algu√©m tem que tirar a bala sugando ü§®\"\n\" Foi pra isso mesmo que eu nasci üíÄüíÄ\"",
         "@samuelc.s.deoliveira163",
         "https://yt3.ggpht.com/yKhBjiXhxhJhvPEv8yL3b3mEe78HJ9TpztQRTHCGAJZSem30kYTWfa3rvN1ncmxLJ99wrhOtAY8=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@samuelc.s.deoliveira163",
         "UCM2EvUBs-bHJSo0UBOwt7iQ",
         "True",
         "none",
         "0.0",
         "2025-04-09 23:07:00+00:00",
         "2025-04-09 23:07:00+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "41",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "faz um do gallade",
         "faz um do gallade",
         "@Zackblue2011",
         "https://yt3.ggpht.com/BvV8KvG8vBeczzLh8Qjm-n4qP9RQeUCp7uYPhLMKPaMBW7Ei3V-vsTG-oy6UmkkmakojO6ScOA=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Zackblue2011",
         "UC5ta4KhbsTe73aCqJ6EfPJw",
         "True",
         "none",
         "0.0",
         "2025-04-14 16:50:43+00:00",
         "2025-04-14 16:50:43+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "42",
         "-1DN4904BQw",
         "UCbDy7ap3Ixk45DILe4O6Tbw",
         "-1DN4904BQw",
         "Aula gratuita: https://bit.ly/3RLbmWq",
         "Aula gratuita: https://bit.ly/3RLbmWq",
         "@sejasaudavel5167",
         "https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJnhxv_fnBhMBnW0RAs0EbYwGirxg_O5haaogRSzXIAEmE5bJkObg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@sejasaudavel5167",
         "UCbDy7ap3Ixk45DILe4O6Tbw",
         "True",
         "none",
         "1.0",
         "2022-10-06 20:41:16+00:00",
         "2022-10-06 20:41:16+00:00",
         null,
         null,
         null,
         null,
         "O pa√≠s mais obeso do mundo #shorts",
         "pt"
        ],
        [
         "43",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Vc √© linda e sua auto-estima √© contagiante. Seu testemunho vai dar for√ßa pra muitas pessoas! Quem v√™ cara ou corpo n√£o v√™ talento ou supera√ß√£o!",
         "Vc √© linda e sua auto-estima √© contagiante. Seu testemunho vai dar for√ßa pra muitas pessoas! Quem v√™ cara ou corpo n√£o v√™ talento ou supera√ß√£o!",
         "@isabelitacorrea2611",
         "https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkXrkG1-bxGyLdN5Kw-Vb4IQMCNkSM=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@isabelitacorrea2611",
         "UCnxzchRu-oFH4H4SKeF2C-Q",
         "True",
         "none",
         "176.0",
         "2024-05-02 01:58:41+00:00",
         "2024-05-02 01:58:41+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "44",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "N√£o ligue para essas pessoas idiotas!!! Voc√™ e incr√≠vel por n√£o levar essas coisas pro cora√ß√£o e estar relatando isso pra gente ‚ù§‚ù§‚ù§‚ù§",
         "N√£o ligue para essas pessoas idiotas!!! Voc√™ e incr√≠vel por n√£o levar essas coisas pro cora√ß√£o e estar relatando isso pra gente ‚ù§‚ù§‚ù§‚ù§",
         "@tauanblack-vg8ej",
         "https://yt3.ggpht.com/5vMUWt4cIF7mo6NLwr7CY2I-7ODei_xgFs08r3L7FqyJlL0jvFtNH5izYQiCKIPRgSYTvOod=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@tauanblack-vg8ej",
         "UChQkM2XtXDgaGsl6jMBnhVw",
         "True",
         "none",
         "16.0",
         "2024-05-02 03:16:20+00:00",
         "2024-05-02 03:16:20+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "45",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Ja lacro , entao blz\nAgora mostra o que vc sabe",
         "Ja lacro , entao blz\nAgora mostra o que vc sabe",
         "@hemersonsantos5713",
         "https://yt3.ggpht.com/jX8CHiXP7kJcHH7I4ar9dgAeZRC-pZyXKc1vmd6RSUVSFm0SWXABKQMs-STYyZoGjA7YRQZ7Mg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@hemersonsantos5713",
         "UCSISX3XUbho5T54DfwjvMMg",
         "True",
         "none",
         "3.0",
         "2024-05-02 04:02:57+00:00",
         "2024-05-02 04:02:57+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "46",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Nem combina com bal√© ! A pessoa imensa,pesada... A roupa pedindo socorro .",
         "Nem combina com bal√© ! A pessoa imensa,pesada... A roupa pedindo socorro .",
         "@macacoacido4904",
         "https://yt3.ggpht.com/pCPAnO2Z2i25z0Pky0HTUwjNuQD4tfAb12-PAmhF2T7haFGeDH7EBYQMJuSCwdLKmb9Kb-qCJw=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@macacoacido4904",
         "UCHYkk_GwHMrQhDXFwqRLCWg",
         "True",
         "none",
         "3.0",
         "2024-05-02 10:59:11+00:00",
         "2024-05-02 10:59:11+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "47",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Julia se eu tivesse uma filha gostaria muito que ela te conhecesse! Vc √©  um grande exemplo ... vc representa um sonho que pode ser apagado de muitas meninas por.puro preconceito! Muito mais sucesso para vc lindona ‚ù§",
         "Julia se eu tivesse uma filha gostaria muito que ela te conhecesse! Vc √©  um grande exemplo ... vc representa um sonho que pode ser apagado de muitas meninas por.puro preconceito! Muito mais sucesso para vc lindona ‚ù§",
         "@albanisa",
         "https://yt3.ggpht.com/ytc/AIdro_kFzI0ptGHeCwGwztHoLQPX6mtwcLGne7zxoSGGlYwFD5k=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@albanisa",
         "UCPcsmDfdiUDarn9105VGlkQ",
         "True",
         "none",
         "28.0",
         "2024-05-02 11:10:04+00:00",
         "2024-05-02 11:10:04+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "48",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Mo√ßa. Que tipo de dan√ßa a senhorita pratica? √â proficional ou amador? Se for bal√© cl√°ssico √© √≥bvio que n√£o √© boa. Os movimentos s√£o leves e n√£o fica bonito com sobrepeso. Ent√£o,vamos parar com esse mi-mi-mi. Se est√° fazendo dan√ßa e n√£o emagrece,ou n√£o dan√ßa ou n√£o se empenha. Ps:Nenhum bambino vai aguentar te levantar. Ou seja. Muitas coreografias cl√°ssicas do bal√© nem vai dar pra fazer. Aff",
         "Mo√ßa. Que tipo de dan√ßa a senhorita pratica? √â proficional ou amador? Se for bal√© cl√°ssico √© √≥bvio que n√£o √© boa. Os movimentos s√£o leves e n√£o fica bonito com sobrepeso. Ent√£o,vamos parar com esse mi-mi-mi. Se est√° fazendo dan√ßa e n√£o emagrece,ou n√£o dan√ßa ou n√£o se empenha. Ps:Nenhum bambino vai aguentar te levantar. Ou seja. Muitas coreografias cl√°ssicas do bal√© nem vai dar pra fazer. Aff",
         "@anaclarasantosduarte3335",
         "https://yt3.ggpht.com/ytc/AIdro_kjKiMvsrY9Fw6rTIjgsagiUWkgLc0vF0sz6wRym9yO3pI2=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@anaclarasantosduarte3335",
         "UC65Lg9H5_cdHtK5GmzTYhtw",
         "True",
         "none",
         "5.0",
         "2024-05-02 11:11:29+00:00",
         "2024-05-02 11:15:15+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "49",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Isso √© incr√≠vel, n√£o tinha visto ainda, c√™ incr√≠vel mo√ßa porque muitas na  mesma condi√ß√£o que voc√™ optam por n√£o continuar por conta do preconceito üòÆ‚ù§",
         "Isso √© incr√≠vel, n√£o tinha visto ainda, c√™ incr√≠vel mo√ßa porque muitas na  mesma condi√ß√£o que voc√™ optam por n√£o continuar por conta do preconceito üòÆ‚ù§",
         "@Raymara_oficial",
         "https://yt3.ggpht.com/P4qCq1oMRa7nlnP-7JGE6V27IupCUpCmcNbtDakVKWDjquROdJPM2nrRjPvNAWD5G8sY-CjLww=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@Raymara_oficial",
         "UCHGEI97zTm-3JXZOMJ8fA5Q",
         "True",
         "none",
         "39.0",
         "2024-05-02 11:15:13+00:00",
         "2024-05-02 11:15:13+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 191946
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Chefe wigol deu um beijo grego no homer skksks</td>\n",
       "      <td>Chefe wigol deu um beijo grego no homer skksks</td>\n",
       "      <td>@MrLopess00</td>\n",
       "      <td>https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO...</td>\n",
       "      <td>http://www.youtube.com/@MrLopess00</td>\n",
       "      <td>UCtrByOsq8kIDCQfSXfq3IKw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>447.0</td>\n",
       "      <td>2024-12-29 02:00:55+00:00</td>\n",
       "      <td>2024-12-29 02:00:55+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Quem era Batedor de Carteiras ?</td>\n",
       "      <td>Quem era Batedor de Carteiras ?</td>\n",
       "      <td>@mateuss.santossilva5059</td>\n",
       "      <td>https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_...</td>\n",
       "      <td>http://www.youtube.com/@mateuss.santossilva5059</td>\n",
       "      <td>UCIY2M7NurJ728_H4Cs5zmQA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2024-12-29 12:53:19+00:00</td>\n",
       "      <td>2024-12-29 12:53:19+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>\"Gra√ßas a deus que essa coisa est√° do nosso la...</td>\n",
       "      <td>\"Gra√ßas a deus que essa coisa est√° do nosso la...</td>\n",
       "      <td>@Ray._Ryan000</td>\n",
       "      <td>https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX...</td>\n",
       "      <td>http://www.youtube.com/@Ray._Ryan000</td>\n",
       "      <td>UCr5gdJ-I9wpcBXWdSR6T5iw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>2024-12-29 16:16:06+00:00</td>\n",
       "      <td>2024-12-29 16:17:20+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>ü§® t√° estranho isso</td>\n",
       "      <td>ü§® t√° estranho isso</td>\n",
       "      <td>@darkgacha5649</td>\n",
       "      <td>https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOH...</td>\n",
       "      <td>http://www.youtube.com/@darkgacha5649</td>\n",
       "      <td>UCZnl2qgkPiF-SYyoPmTbzBw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-29 18:03:52+00:00</td>\n",
       "      <td>2024-12-29 18:03:52+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191941</th>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>UCjOJvvYe6tyEHY21OD33h8A</td>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>Dizem a eles o que tem que fazer !!!??\\nIsto e...</td>\n",
       "      <td>Dizem a eles o que tem que fazer !!!??\\nIsto e...</td>\n",
       "      <td>@pauloferw</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kQShd7zzPCX_Ta...</td>\n",
       "      <td>http://www.youtube.com/@pauloferw</td>\n",
       "      <td>UCS6h73aFHTTrQBpUJuWwNfA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2024-05-01 08:02:32+00:00</td>\n",
       "      <td>2024-05-01 08:02:32+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mat√©ria de Capa | A epidemia global da obesida...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191942</th>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>UCjOJvvYe6tyEHY21OD33h8A</td>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>Desculpe mas sua obesidade n tem nada haver co...</td>\n",
       "      <td>Desculpe mas sua obesidade n tem nada haver co...</td>\n",
       "      <td>@stephaniemayer6082</td>\n",
       "      <td>https://yt3.ggpht.com/n__DHrkMmHm3ZWMyMlkl6Evy...</td>\n",
       "      <td>http://www.youtube.com/@stephaniemayer6082</td>\n",
       "      <td>UCIenqiHf4KoTSrK1cjEFwMA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-05-23 12:59:32+00:00</td>\n",
       "      <td>2024-05-23 12:59:32+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mat√©ria de Capa | A epidemia global da obesida...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191943</th>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>UCjOJvvYe6tyEHY21OD33h8A</td>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>Da onde foi tirado esse 2,89 pra fazer a conta...</td>\n",
       "      <td>Da onde foi tirado esse 2,89 pra fazer a conta...</td>\n",
       "      <td>@adrianagalvao9963</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_lZNyRnrOGQx6Ns...</td>\n",
       "      <td>http://www.youtube.com/@adrianagalvao9963</td>\n",
       "      <td>UCjN2L25tmQraNVLwsqOxnKQ</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-06-26 18:15:01+00:00</td>\n",
       "      <td>2024-06-26 18:15:01+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mat√©ria de Capa | A epidemia global da obesida...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191944</th>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>UCjOJvvYe6tyEHY21OD33h8A</td>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>Todos os profissionais falaram sobre exerc√≠cio...</td>\n",
       "      <td>Todos os profissionais falaram sobre exerc√≠cio...</td>\n",
       "      <td>@cinesiologiauniversal</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kmdHnxCpeng5JD...</td>\n",
       "      <td>http://www.youtube.com/@cinesiologiauniversal</td>\n",
       "      <td>UCG0vNc5oRvHT00FnSokCFQQ</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-07-20 20:34:55+00:00</td>\n",
       "      <td>2024-07-20 20:34:55+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mat√©ria de Capa | A epidemia global da obesida...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191945</th>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>UCjOJvvYe6tyEHY21OD33h8A</td>\n",
       "      <td>zvJJHnpUGMo</td>\n",
       "      <td>Ninguem fala que a culpa e da telepatia. Satan...</td>\n",
       "      <td>Ninguem fala que a culpa e da telepatia. Satan...</td>\n",
       "      <td>@dente-podre-virgem37</td>\n",
       "      <td>https://yt3.ggpht.com/y6FUNEmPplRye6g90vgW0ODo...</td>\n",
       "      <td>http://www.youtube.com/@dente-podre-virgem37</td>\n",
       "      <td>UCrrfvv9pB35aR255LgZxJeA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-14 21:42:59+00:00</td>\n",
       "      <td>2024-12-14 21:42:59+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mat√©ria de Capa | A epidemia global da obesida...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191946 rows √ó 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           video_id                 channelId      videoId  \\\n",
       "0       --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1       --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "2       --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "3       --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "4       --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "...             ...                       ...          ...   \n",
       "191941  zvJJHnpUGMo  UCjOJvvYe6tyEHY21OD33h8A  zvJJHnpUGMo   \n",
       "191942  zvJJHnpUGMo  UCjOJvvYe6tyEHY21OD33h8A  zvJJHnpUGMo   \n",
       "191943  zvJJHnpUGMo  UCjOJvvYe6tyEHY21OD33h8A  zvJJHnpUGMo   \n",
       "191944  zvJJHnpUGMo  UCjOJvvYe6tyEHY21OD33h8A  zvJJHnpUGMo   \n",
       "191945  zvJJHnpUGMo  UCjOJvvYe6tyEHY21OD33h8A  zvJJHnpUGMo   \n",
       "\n",
       "                                              textDisplay  \\\n",
       "0       Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1          Chefe wigol deu um beijo grego no homer skksks   \n",
       "2                         Quem era Batedor de Carteiras ?   \n",
       "3       \"Gra√ßas a deus que essa coisa est√° do nosso la...   \n",
       "4                                      ü§® t√° estranho isso   \n",
       "...                                                   ...   \n",
       "191941  Dizem a eles o que tem que fazer !!!??\\nIsto e...   \n",
       "191942  Desculpe mas sua obesidade n tem nada haver co...   \n",
       "191943  Da onde foi tirado esse 2,89 pra fazer a conta...   \n",
       "191944  Todos os profissionais falaram sobre exerc√≠cio...   \n",
       "191945  Ninguem fala que a culpa e da telepatia. Satan...   \n",
       "\n",
       "                                             textOriginal  \\\n",
       "0       Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1          Chefe wigol deu um beijo grego no homer skksks   \n",
       "2                         Quem era Batedor de Carteiras ?   \n",
       "3       \"Gra√ßas a deus que essa coisa est√° do nosso la...   \n",
       "4                                      ü§® t√° estranho isso   \n",
       "...                                                   ...   \n",
       "191941  Dizem a eles o que tem que fazer !!!??\\nIsto e...   \n",
       "191942  Desculpe mas sua obesidade n tem nada haver co...   \n",
       "191943  Da onde foi tirado esse 2,89 pra fazer a conta...   \n",
       "191944  Todos os profissionais falaram sobre exerc√≠cio...   \n",
       "191945  Ninguem fala que a culpa e da telepatia. Satan...   \n",
       "\n",
       "               authorDisplayName  \\\n",
       "0              @evelynsoares4467   \n",
       "1                    @MrLopess00   \n",
       "2       @mateuss.santossilva5059   \n",
       "3                  @Ray._Ryan000   \n",
       "4                 @darkgacha5649   \n",
       "...                          ...   \n",
       "191941                @pauloferw   \n",
       "191942       @stephaniemayer6082   \n",
       "191943        @adrianagalvao9963   \n",
       "191944    @cinesiologiauniversal   \n",
       "191945     @dente-podre-virgem37   \n",
       "\n",
       "                                    authorProfileImageUrl  \\\n",
       "0       https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1       https://yt3.ggpht.com/GbqCWSYWX0x7m12TrBOc7bBO...   \n",
       "2       https://yt3.ggpht.com/lIA6NvNbtRKR4LZyVTGVdNO_...   \n",
       "3       https://yt3.ggpht.com/WIrn4XlSuZAQuPHw6w53yiiX...   \n",
       "4       https://yt3.ggpht.com/R5NIvS_yYOP4_ngqdnlXIlOH...   \n",
       "...                                                   ...   \n",
       "191941  https://yt3.ggpht.com/ytc/AIdro_kQShd7zzPCX_Ta...   \n",
       "191942  https://yt3.ggpht.com/n__DHrkMmHm3ZWMyMlkl6Evy...   \n",
       "191943  https://yt3.ggpht.com/ytc/AIdro_lZNyRnrOGQx6Ns...   \n",
       "191944  https://yt3.ggpht.com/ytc/AIdro_kmdHnxCpeng5JD...   \n",
       "191945  https://yt3.ggpht.com/y6FUNEmPplRye6g90vgW0ODo...   \n",
       "\n",
       "                                       authorChannelUrl  \\\n",
       "0              http://www.youtube.com/@evelynsoares4467   \n",
       "1                    http://www.youtube.com/@MrLopess00   \n",
       "2       http://www.youtube.com/@mateuss.santossilva5059   \n",
       "3                  http://www.youtube.com/@Ray._Ryan000   \n",
       "4                 http://www.youtube.com/@darkgacha5649   \n",
       "...                                                 ...   \n",
       "191941                http://www.youtube.com/@pauloferw   \n",
       "191942       http://www.youtube.com/@stephaniemayer6082   \n",
       "191943        http://www.youtube.com/@adrianagalvao9963   \n",
       "191944    http://www.youtube.com/@cinesiologiauniversal   \n",
       "191945     http://www.youtube.com/@dente-podre-virgem37   \n",
       "\n",
       "                 authorChannelId  canRate viewerRating  likeCount  \\\n",
       "0       UCNhXx9ev5RtEiyGsVjMuTOA     True         none        0.0   \n",
       "1       UCtrByOsq8kIDCQfSXfq3IKw     True         none      447.0   \n",
       "2       UCIY2M7NurJ728_H4Cs5zmQA     True         none        5.0   \n",
       "3       UCr5gdJ-I9wpcBXWdSR6T5iw     True         none     1677.0   \n",
       "4       UCZnl2qgkPiF-SYyoPmTbzBw     True         none        0.0   \n",
       "...                          ...      ...          ...        ...   \n",
       "191941  UCS6h73aFHTTrQBpUJuWwNfA     True         none        1.0   \n",
       "191942  UCIenqiHf4KoTSrK1cjEFwMA     True         none        0.0   \n",
       "191943  UCjN2L25tmQraNVLwsqOxnKQ     True         none        0.0   \n",
       "191944  UCG0vNc5oRvHT00FnSokCFQQ     True         none        0.0   \n",
       "191945  UCrrfvv9pB35aR255LgZxJeA     True         none        0.0   \n",
       "\n",
       "                     publishedAt                 updatedAt author comment  \\\n",
       "0      2024-12-28 21:38:37+00:00 2024-12-28 21:38:37+00:00   None    None   \n",
       "1      2024-12-29 02:00:55+00:00 2024-12-29 02:00:55+00:00   None    None   \n",
       "2      2024-12-29 12:53:19+00:00 2024-12-29 12:53:19+00:00   None    None   \n",
       "3      2024-12-29 16:16:06+00:00 2024-12-29 16:17:20+00:00   None    None   \n",
       "4      2024-12-29 18:03:52+00:00 2024-12-29 18:03:52+00:00   None    None   \n",
       "...                          ...                       ...    ...     ...   \n",
       "191941 2024-05-01 08:02:32+00:00 2024-05-01 08:02:32+00:00   None    None   \n",
       "191942 2024-05-23 12:59:32+00:00 2024-05-23 12:59:32+00:00   None    None   \n",
       "191943 2024-06-26 18:15:01+00:00 2024-06-26 18:15:01+00:00   None    None   \n",
       "191944 2024-07-20 20:34:55+00:00 2024-07-20 20:34:55+00:00   None    None   \n",
       "191945 2024-12-14 21:42:59+00:00 2024-12-14 21:42:59+00:00   None    None   \n",
       "\n",
       "        date likes                                        video_title language  \n",
       "0       None  None                 Tony Gordo √© Incriminado #simpsons       pt  \n",
       "1       None  None                 Tony Gordo √© Incriminado #simpsons       pt  \n",
       "2       None  None                 Tony Gordo √© Incriminado #simpsons       pt  \n",
       "3       None  None                 Tony Gordo √© Incriminado #simpsons       pt  \n",
       "4       None  None                 Tony Gordo √© Incriminado #simpsons       pt  \n",
       "...      ...   ...                                                ...      ...  \n",
       "191941  None  None  Mat√©ria de Capa | A epidemia global da obesida...       pt  \n",
       "191942  None  None  Mat√©ria de Capa | A epidemia global da obesida...       pt  \n",
       "191943  None  None  Mat√©ria de Capa | A epidemia global da obesida...       pt  \n",
       "191944  None  None  Mat√©ria de Capa | A epidemia global da obesida...       pt  \n",
       "191945  None  None  Mat√©ria de Capa | A epidemia global da obesida...       pt  \n",
       "\n",
       "[191946 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading comment data from: ../data/intermediate/20250417_youtube_comments_pt_cleaned1.parquet\n",
      "‚úÖ Successfully loaded 191,946 comments\n",
      "üìä Data shape: (191946, 20)\n",
      "‚úÖ Successfully loaded 191,946 comments\n",
      "üìä Data shape: (191946, 20)\n",
      "üíæ Memory usage: 247.3 MB\n",
      "\n",
      "üìà Data Overview:\n",
      "- Total comments: 191,946\n",
      "- Unique videos: 1,204\n",
      "- Unique authors: 163,664\n",
      "- Date range: 2006-11-24 20:16:56+00:00 to 2025-04-17 11:46:21+00:00\n",
      "\n",
      "üìã Sample Comments:\n",
      "- √â isso que acontece quando da dinheiro para porcos\n",
      "- Mds, como a pessoa se permite chegar a esse ponto?\n",
      "- Mo√ßa üòä vc e muito bonitaüòä‚ù§\n",
      "üíæ Memory usage: 247.3 MB\n",
      "\n",
      "üìà Data Overview:\n",
      "- Total comments: 191,946\n",
      "- Unique videos: 1,204\n",
      "- Unique authors: 163,664\n",
      "- Date range: 2006-11-24 20:16:56+00:00 to 2025-04-17 11:46:21+00:00\n",
      "\n",
      "üìã Sample Comments:\n",
      "- √â isso que acontece quando da dinheiro para porcos\n",
      "- Mds, como a pessoa se permite chegar a esse ponto?\n",
      "- Mo√ßa üòä vc e muito bonitaüòä‚ù§\n"
     ]
    }
   ],
   "source": [
    "def load_and_explore_comment_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and explore YouTube comment data for classification.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the cleaned comments data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with comment data ready for classification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üìÇ Loading comment data from: {file_path}\")\n",
    "\n",
    "        # Verify file exists\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"Input file not found: {file_path}\")\n",
    "\n",
    "        # Load the data\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        print(f\"‚úÖ Successfully loaded {len(df):,} comments\")\n",
    "        print(f\"üìä Data shape: {df.shape}\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "        # Display basic statistics\n",
    "        print(f\"\\nüìà Data Overview:\")\n",
    "        print(f\"- Total comments: {len(df):,}\")\n",
    "        print(f\"- Unique videos: {df['video_id'].nunique():,}\")\n",
    "        print(f\"- Unique authors: {df.get('authorDisplayName', pd.Series()).nunique():,}\")\n",
    "        print(f\"- Date range: {df.get('publishedAt', pd.Series()).min()} to {df.get('publishedAt', pd.Series()).max()}\")\n",
    "\n",
    "        # Check for key columns\n",
    "        required_columns = [\"textDisplay\", \"video_id\"]\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "        # Display sample data\n",
    "        print(f\"\\nüìã Sample Comments:\")\n",
    "        sample_df = df.sample(min(3, len(df)))\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            text_preview = row[\"textDisplay\"][:100] + \"...\" if len(row[\"textDisplay\"]) > 100 else row[\"textDisplay\"]\n",
    "            print(f\"- {text_preview}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Load the comment data\n",
    "df = load_and_explore_comment_data(ClassificationConfig.INPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RespostaAnaliseSentimento(BaseModel):\n",
    "    \"\"\"A resposta de uma fun√ß√£o que realiza an√°lise de sentimento em texto e detec√ß√£o do idioma do texto.\"\"\"\n",
    "\n",
    "    # O r√≥tulo de sentimento atribu√≠do ao texto\n",
    "    sentimento: Literal[\"positivo\", \"negativo\", \"neutro\"] = Field(\n",
    "        default_factory=str,\n",
    "        description=\"O r√≥tulo de sentimento atribu√≠do ao texto. Voc√™ s√≥ pode ter 'positivo', 'negativo' ou 'neutro' como valores.\",\n",
    "    )\n",
    "\n",
    "    gordofobia_implicita: bool = Field(\n",
    "        default_factory=bool,\n",
    "        description=\"Se o texto cont√©m discrimina√ß√£o por peso (gordofobia) de forma impl√≠cita e/ou indireta. Se n√£o houver gordofobia, este campo deve ser False.\",\n",
    "    )\n",
    "\n",
    "    gordofobia_explicita: bool = Field(\n",
    "        default_factory=bool,\n",
    "        description=\"Se o texto cont√©m discrimina√ß√£o por peso (gordofobia) de forma expl√≠cita e/ou direta. Se n√£o houver gordofobia, este campo deve ser False.\",\n",
    "    )\n",
    "\n",
    "    # O idioma detectado no texto\n",
    "    idioma: str = Field(\n",
    "        default_factory=str,\n",
    "        description=\"O idioma detectado no texto, representado por um c√≥digo de idioma de duas letras.\",\n",
    "    )\n",
    "\n",
    "    obesidade: bool = Field(\n",
    "        default_factory=bool,\n",
    "        description=\"Se o texto toca no assunto de obesidade. Se n√£o houver men√ß√£o √† obesidade, este campo deve ser False.\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic configuration for the model.\"\"\"\n",
    "\n",
    "        json_encoders = {\n",
    "            # Custom encoders if needed\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classification schema defined successfully\n",
      "üìã Model fields: ['sentimento', 'gordofobia_implicita', 'gordofobia_explicita', 'idioma', 'obesidade']\n",
      "üîç Schema validation: OK\n",
      "üìä Required fields: []\n"
     ]
    }
   ],
   "source": [
    "# Validate the model structure\n",
    "print(\"‚úÖ Classification schema defined successfully\")\n",
    "print(f\"üìã Model fields: {list(RespostaAnaliseSentimento.model_fields.keys())}\")\n",
    "\n",
    "# Display model schema for validation\n",
    "try:\n",
    "    schema = RespostaAnaliseSentimento.schema()\n",
    "    print(f\"üîç Schema validation: OK\")\n",
    "    print(f\"üìä Required fields: {schema.get('required', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Schema validation error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System prompt configured\n",
      "üìù Prompt length: 1346 characters\n",
      "üéØ Classification targets: sentiment, gordofobia, language, obesity content\n"
     ]
    }
   ],
   "source": [
    "# System prompt for zero-shot classification\n",
    "SYSTEM_PROMPT = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"Voc√™ √© um especialista em an√°lise de sentimento com foco em coment√°rios relacionados a peso corporal e discrimina√ß√£o.\n",
    "\n",
    "Sua tarefa √© classificar coment√°rios do YouTube com precis√£o, identificando:\n",
    "1. Sentimento geral (positivo, negativo, neutro)\n",
    "2. Presen√ßa de gordofobia (discrimina√ß√£o por peso)\n",
    "3. Idioma do texto\n",
    "4. Men√ß√µes sobre obesidade\n",
    "\n",
    "DIRETRIZES DE CLASSIFICA√á√ÉO:\n",
    "\n",
    "SENTIMENTO:\n",
    "- 'positivo': Coment√°rios de apoio, encorajamento, aceita√ß√£o corporal, mensagens construtivas\n",
    "- 'negativo': Cr√≠ticas, julgamentos, discrimina√ß√£o, linguagem ofensiva, gordofobia\n",
    "- 'neutro': Coment√°rios informativos, quest√µes, observa√ß√µes sem julgamento de valor\n",
    "\n",
    "GORDOFOBIA:\n",
    "- Expl√≠cita: Insultos diretos, linguagem claramente discriminat√≥ria, termos pejorativos sobre peso\n",
    "- Impl√≠cita: Sugest√µes sutis, estere√≥tipos, press√µes indiretas relacionadas ao peso\n",
    "\n",
    "IDIOMA:\n",
    "- Use c√≥digos ISO 639-1 (pt, en, es, etc.)\n",
    "- Considere o idioma predominante se houver mistura\n",
    "\n",
    "OBESIDADE:\n",
    "- Marque como True se o coment√°rio menciona ou discute obesidade, mesmo que indiretamente\n",
    "\n",
    "CONTEXTO IMPORTANTE:\n",
    "- Considere ironia, sarcasmo e emojis no contexto\n",
    "- Analise o coment√°rio completo, n√£o apenas palavras isoladas\n",
    "- Coment√°rios de apoio √† diversidade corporal s√£o positivos\n",
    "- Seja preciso na detec√ß√£o de discrimina√ß√£o sutil\n",
    "\n",
    "Responda APENAS com o formato estruturado solicitado.\"\"\",\n",
    "}\n",
    "\n",
    "print(\"‚úÖ System prompt configured\")\n",
    "print(f\"üìù Prompt length: {len(SYSTEM_PROMPT['content'])} characters\")\n",
    "print(\"üéØ Classification targets: sentiment, gordofobia, language, obesity content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Function schema created successfully\n",
      "üìã Function name: RespostaAnaliseSentimento\n",
      "üîß Required parameters: ['sentimento', 'gordofobia_implicita', 'gordofobia_explicita', 'idioma', 'obesidade']\n",
      "\n",
      "üîç Function Schema Structure:\n",
      "- Name: RespostaAnaliseSentimento\n",
      "- Description: A resposta de uma fun√ß√£o que realiza an√°lise de sentimento em texto e detec√ß√£o do idioma do texto.\n",
      "- Parameters: 5 fields\n",
      "- Required fields: 5\n",
      "‚úÖ Function schema validation passed\n"
     ]
    }
   ],
   "source": [
    "# Generate OpenAI function schema from Pydantic model\n",
    "def create_function_schema() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create OpenAI function calling schema from the Pydantic model.\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the function schema for OpenAI API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert Pydantic model to OpenAI function format\n",
    "        function_schema = convert_pydantic_to_openai_function(RespostaAnaliseSentimento)\n",
    "\n",
    "        # Ensure all fields are required for consistent outputs\n",
    "        function_schema[\"parameters\"][\"required\"] = list(function_schema[\"parameters\"][\"properties\"].keys())\n",
    "        function_schema[\"parameters\"][\"type\"] = \"object\"\n",
    "\n",
    "        print(\"‚úÖ Function schema created successfully\")\n",
    "        print(f\"üìã Function name: {function_schema['name']}\")\n",
    "        print(f\"üîß Required parameters: {function_schema['parameters']['required']}\")\n",
    "\n",
    "        return function_schema\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating function schema: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Create the function schema\n",
    "function_schema = create_function_schema()\n",
    "\n",
    "# Display schema structure for validation\n",
    "print(f\"\\nüîç Function Schema Structure:\")\n",
    "print(f\"- Name: {function_schema['name']}\")\n",
    "print(f\"- Description: {function_schema['description']}\")\n",
    "print(f\"- Parameters: {len(function_schema['parameters']['properties'])} fields\")\n",
    "print(f\"- Required fields: {len(function_schema['parameters']['required'])}\")\n",
    "\n",
    "# Validate schema structure\n",
    "assert \"name\" in function_schema, \"Function schema missing name\"\n",
    "assert \"parameters\" in function_schema, \"Function schema missing parameters\"\n",
    "assert len(function_schema[\"parameters\"][\"required\"]) == 5, \"Expected 5 required parameters\"\n",
    "\n",
    "print(\"‚úÖ Function schema validation passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing text field differences...\n",
      "üìä Text differences analysis:\n",
      "- Identical texts: 191,946\n",
      "- Different texts: 0\n",
      "‚ÑπÔ∏è textDisplay and textOriginal are identical\n",
      "üìù Preparing 191,946 comments for classification...\n",
      "‚úÖ Prepared 191,946 texts for classification\n",
      "\n",
      "üìà Data Preparation Summary:\n",
      "- Total comments to classify: 191,946\n",
      "- Sample text length: 55 characters\n",
      "- Average text length: 91.1 characters\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_differences(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze differences between textDisplay and textOriginal columns.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing the comment data\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing text field differences...\")\n",
    "\n",
    "    if \"textOriginal\" in df.columns:\n",
    "        differences = (df.textDisplay != df.textOriginal).value_counts()\n",
    "        print(f\"üìä Text differences analysis:\")\n",
    "        print(f\"- Identical texts: {differences.get(False, 0):,}\")\n",
    "        print(f\"- Different texts: {differences.get(True, 0):,}\")\n",
    "\n",
    "        if differences.get(True, 0) > 0:\n",
    "            print(\"‚ÑπÔ∏è textDisplay will be used for classification (processed version)\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è textDisplay and textOriginal are identical\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Only textDisplay column available\")\n",
    "\n",
    "\n",
    "def prepare_classification_data(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Prepare comment texts for classification.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing the comment data\n",
    "\n",
    "    Returns:\n",
    "        List of comment texts ready for processing\n",
    "    \"\"\"\n",
    "    print(f\"üìù Preparing {len(df):,} comments for classification...\")\n",
    "\n",
    "    # Use textDisplay as it contains the processed version\n",
    "    input_texts = df.textDisplay.values.tolist()\n",
    "\n",
    "    # Basic validation\n",
    "    empty_texts = sum(1 for text in input_texts if not text or not text.strip())\n",
    "    if empty_texts > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {empty_texts} empty or whitespace-only comments\")\n",
    "\n",
    "    print(f\"‚úÖ Prepared {len(input_texts):,} texts for classification\")\n",
    "    return input_texts\n",
    "\n",
    "\n",
    "# Analyze the data structure\n",
    "analyze_text_differences(df)\n",
    "\n",
    "# Prepare the input texts\n",
    "input_texts = prepare_classification_data(df)\n",
    "\n",
    "print(f\"\\nüìà Data Preparation Summary:\")\n",
    "print(f\"- Total comments to classify: {len(input_texts):,}\")\n",
    "print(f\"- Sample text length: {len(input_texts[0]) if input_texts else 0} characters\")\n",
    "print(f\"- Average text length: {sum(len(text) for text in input_texts) / len(input_texts):.1f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating batch API requests...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9324266032e345c4906f7c3a6f79eaaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating requests:   0%|          | 0/191946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 191,946 API requests\n",
      "üì¶ Splitting 191,946 requests into batches of 40,000...\n",
      "‚úÖ Created 5 batch(es)\n",
      "  - Batch 0: 40,000 requests\n",
      "  - Batch 1: 40,000 requests\n",
      "  - Batch 2: 40,000 requests\n",
      "  - Batch 3: 40,000 requests\n",
      "  - Batch 4: 31,946 requests\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "- Total requests: 191,946\n",
      "- Number of batches: 5\n",
      "- Batch size limit: 40,000\n",
      "- Model: gpt-4.1-mini\n",
      "- Temperature: 0.0\n"
     ]
    }
   ],
   "source": [
    "def create_batch_requests(texts: List[str], df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create batch API requests for comment classification.\n",
    "\n",
    "    Args:\n",
    "        texts: List of comment texts to classify\n",
    "        df: Original DataFrame for generating unique IDs\n",
    "\n",
    "    Returns:\n",
    "        List of API request objects\n",
    "    \"\"\"\n",
    "    print(f\"üîß Creating batch API requests...\")\n",
    "\n",
    "    jsonl_data = []\n",
    "\n",
    "    for idx, text in enumerate(tqdm(texts, desc=\"Creating requests\")):\n",
    "        # Create unique identifier for the request\n",
    "        custom_uid = f\"{text}{idx}{df.video_id.iloc[idx]}\"\n",
    "        request_id = hashlib.md5(custom_uid.encode()).hexdigest()\n",
    "\n",
    "        # Create API request structure\n",
    "        request_data = {\n",
    "            \"custom_id\": request_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": ClassificationConfig.MODEL_NAME,\n",
    "                \"temperature\": ClassificationConfig.TEMPERATURE,\n",
    "                \"messages\": [SYSTEM_PROMPT, {\"role\": \"user\", \"content\": text.encode().decode(\"utf-8\")}],\n",
    "                \"parallel_tool_calls\": False,\n",
    "                \"tools\": [{\"type\": \"function\", \"function\": function_schema}],\n",
    "                \"tool_choice\": {\"type\": \"function\", \"function\": {\"name\": function_schema[\"name\"]}},\n",
    "            },\n",
    "        }\n",
    "        jsonl_data.append(request_data)\n",
    "\n",
    "    print(f\"‚úÖ Created {len(jsonl_data):,} API requests\")\n",
    "    return jsonl_data\n",
    "\n",
    "\n",
    "def split_into_batches(data: List[Dict[str, Any]], batch_size: int) -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Split requests into batches for API processing.\n",
    "\n",
    "    Args:\n",
    "        data: List of API requests\n",
    "        batch_size: Maximum requests per batch\n",
    "\n",
    "    Returns:\n",
    "        List of batches\n",
    "    \"\"\"\n",
    "    print(f\"üì¶ Splitting {len(data):,} requests into batches of {batch_size:,}...\")\n",
    "\n",
    "    chunks = [data[x : x + batch_size] for x in range(0, len(data), batch_size)]\n",
    "\n",
    "    print(f\"‚úÖ Created {len(chunks)} batch(es)\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"  - Batch {i}: {len(chunk):,} requests\")\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Create the batch requests\n",
    "jsonl_data = create_batch_requests(input_texts, df)\n",
    "\n",
    "# Split into manageable batches\n",
    "chunks = split_into_batches(jsonl_data, ClassificationConfig.BATCH_SIZE)\n",
    "\n",
    "print(f\"\\nüìä Batch Processing Summary:\")\n",
    "print(f\"- Total requests: {len(jsonl_data):,}\")\n",
    "print(f\"- Number of batches: {len(chunks)}\")\n",
    "print(f\"- Batch size limit: {ClassificationConfig.BATCH_SIZE:,}\")\n",
    "print(f\"- Model: {ClassificationConfig.MODEL_NAME}\")\n",
    "print(f\"- Temperature: {ClassificationConfig.TEMPERATURE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting batch files...\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_0.jsonl: 120.11 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_0.jsonl: 120.11 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_1.jsonl: 119.87 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_1.jsonl: 119.87 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_2.jsonl: 119.04 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_2.jsonl: 119.04 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_3.jsonl: 119.84 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_3.jsonl: 119.84 MB\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_4.jsonl: 95.90 MB\n",
      "\n",
      "üìÅ Batch Files Created:\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: 120.11 MB\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: 119.87 MB\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: 119.04 MB\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: 119.84 MB\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: 95.90 MB\n",
      "\n",
      "üéØ Ready for batch processing with 5 file(s)\n",
      "‚úÖ Created 20250417_youtube_comments_batch_api_4.jsonl: 95.90 MB\n",
      "\n",
      "üìÅ Batch Files Created:\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: 120.11 MB\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: 119.87 MB\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: 119.04 MB\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: 119.84 MB\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: 95.90 MB\n",
      "\n",
      "üéØ Ready for batch processing with 5 file(s)\n"
     ]
    }
   ],
   "source": [
    "def export_batch_files(chunks: List[List[Dict[str, Any]]], base_filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Export batch requests to JSONL files for API processing.\n",
    "\n",
    "    Args:\n",
    "        chunks: List of batch chunks\n",
    "        base_filename: Base filename for the batch files\n",
    "\n",
    "    Returns:\n",
    "        List of created file paths\n",
    "    \"\"\"\n",
    "    print(f\"üíæ Exporting batch files...\")\n",
    "\n",
    "    created_files = []\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        filename = f\"{base_filename}_{idx}.jsonl\"\n",
    "        filepath = ClassificationConfig.JSONL_DIR / filename\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in chunk:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            # Verify file creation and size\n",
    "            file_size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "            print(f\"‚úÖ Created {filename}: {file_size_mb:.2f} MB\")\n",
    "            created_files.append(str(filepath))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating {filename}: {e}\")\n",
    "            raise\n",
    "\n",
    "    return created_files\n",
    "\n",
    "\n",
    "# Export batch files\n",
    "base_filename = ClassificationConfig.BATCH_NAME_PREFIX\n",
    "created_files = export_batch_files(chunks, base_filename)\n",
    "\n",
    "print(f\"\\nüìÅ Batch Files Created:\")\n",
    "for file_path in created_files:\n",
    "    file_size = Path(file_path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"- {Path(file_path).name}: {file_size:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for batch processing with {len(created_files)} file(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df.textDisplay.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 5 batch files:\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: 120.11 MB\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: 119.87 MB\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: 119.04 MB\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: 119.84 MB\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: 95.90 MB\n",
      "üöÄ Initializing batch processors for 5 files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673d65a0b8b84b5eb3737ff9ad6d48df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitting batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted batch 59ace21c633a198ab4d504403f7add1b with id batch_68821110b710819087888f7e03858295\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_0_20250724_075513.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_0.jsonl\n",
      "Successfully submitted batch 1acbaa61699d58a8c14309a6cbfe7ccf with id batch_6882112382608190a895486bbe90dcca\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_1_20250724_075531.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_1.jsonl\n",
      "Successfully submitted batch 1acbaa61699d58a8c14309a6cbfe7ccf with id batch_6882112382608190a895486bbe90dcca\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_1_20250724_075531.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_1.jsonl\n",
      "Successfully submitted batch 2ae52d110f4f6a5ed352afa17260c60f with id batch_6882114605188190b231d141c91dbf24\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_2_20250724_075606.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_2.jsonl\n",
      "Successfully submitted batch 2ae52d110f4f6a5ed352afa17260c60f with id batch_6882114605188190b231d141c91dbf24\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_2_20250724_075606.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_2.jsonl\n",
      "Successfully submitted batch a1d4bd0f30a26f5ec467edc74b19fe8f with id batch_68821165e79c819088732fc040e5f5f4\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_3_20250724_075638.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_3.jsonl\n",
      "Successfully submitted batch a1d4bd0f30a26f5ec467edc74b19fe8f with id batch_68821165e79c819088732fc040e5f5f4\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_3_20250724_075638.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_3.jsonl\n",
      "Successfully submitted batch 3387027f321a3365db74d312c13cc019 with id batch_688211799874819084f3f672c7bdba49\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_4_20250724_075657.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_4.jsonl\n",
      "‚úÖ All 5 batches submitted successfully\n",
      "Successfully submitted batch 3387027f321a3365db74d312c13cc019 with id batch_688211799874819084f3f672c7bdba49\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_comments_batch_api_4_20250724_075657.txt\n",
      "‚úÖ Submitted batch for 20250417_youtube_comments_batch_api_4.jsonl\n",
      "‚úÖ All 5 batches submitted successfully\n"
     ]
    }
   ],
   "source": [
    "def get_file_hash(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate MD5 hash of a file for unique batch naming.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "\n",
    "    Returns:\n",
    "        MD5 hash string\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "\n",
    "def initialize_batch_processors(file_paths: List[str]) -> Dict[str, OpenAIBatchProcessor]:\n",
    "    \"\"\"\n",
    "    Initialize and submit batch jobs for processing.\n",
    "\n",
    "    Args:\n",
    "        file_paths: List of JSONL file paths to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping file paths to batch processors\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Initializing batch processors for {len(file_paths)} files...\")\n",
    "\n",
    "    batch_processors = {}\n",
    "\n",
    "    for file_path in tqdm(file_paths, desc=\"Submitting batches\"):\n",
    "        try:\n",
    "            # Create processor and submit job\n",
    "            processor = OpenAIBatchProcessor()\n",
    "            batch_name = get_file_hash(file_path)\n",
    "\n",
    "            processor.submit_batch_job(input_jsonl_path=file_path, batch_name=batch_name)\n",
    "\n",
    "            batch_processors[file_path] = processor\n",
    "            print(f\"‚úÖ Submitted batch for {Path(file_path).name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error submitting batch for {Path(file_path).name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    print(f\"‚úÖ All {len(batch_processors)} batches submitted successfully\")\n",
    "    return batch_processors\n",
    "\n",
    "\n",
    "# Find all batch files\n",
    "batch_files = glob(str(ClassificationConfig.JSONL_DIR / f\"{ClassificationConfig.BATCH_NAME_PREFIX}*.jsonl\"))\n",
    "\n",
    "print(f\"üìÅ Found {len(batch_files)} batch files:\")\n",
    "for file_path in batch_files:\n",
    "    file_size = Path(file_path).stat().st_size / (1024 * 1024)\n",
    "    print(f\"- {Path(file_path).name}: {file_size:.2f} MB\")\n",
    "\n",
    "# Initialize batch processors\n",
    "batch_processors = initialize_batch_processors(batch_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Batch Processing Status: 5/5 completed\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: completed\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: completed\n",
      "‚úÖ All batches completed successfully!\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: completed\n",
      "‚úÖ All batches completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def monitor_batch_status(processors: Dict[str, OpenAIBatchProcessor]) -> None:\n",
    "    \"\"\"\n",
    "    Monitor the status of all batch jobs.\n",
    "\n",
    "    Args:\n",
    "        processors: Dictionary of batch processors to monitor\n",
    "    \"\"\"\n",
    "    print(\"üìä Checking batch status...\")\n",
    "\n",
    "    for file_path, processor in processors.items():\n",
    "        try:\n",
    "            batch_info = processor.get_batch_info()\n",
    "            filename = Path(file_path).name\n",
    "            print(f\"- {filename}: {batch_info.status}\")\n",
    "\n",
    "            if hasattr(batch_info, \"request_counts\"):\n",
    "                counts = batch_info.request_counts\n",
    "                if counts:\n",
    "                    print(f\"  üìà Progress: {counts.get('completed', 0)}/{counts.get('total', 0)} requests\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error checking status for {Path(file_path).name}: {e}\")\n",
    "\n",
    "\n",
    "def wait_for_completion(processors: Dict[str, OpenAIBatchProcessor], check_interval: int = 60) -> None:\n",
    "    \"\"\"\n",
    "    Wait for all batch jobs to complete with periodic status updates.\n",
    "\n",
    "    Args:\n",
    "        processors: Dictionary of batch processors to monitor\n",
    "        check_interval: Seconds between status checks\n",
    "    \"\"\"\n",
    "    print(f\"‚è≥ Waiting for batch completion (checking every {check_interval}s)...\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Check if all batches are completed\n",
    "            statuses = []\n",
    "            for processor in processors.values():\n",
    "                status = processor.get_batch_info().status\n",
    "                statuses.append(status)\n",
    "\n",
    "            completed_count = sum(1 for status in statuses if status == \"completed\")\n",
    "            total_count = len(statuses)\n",
    "\n",
    "            # Clear output and show current status\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üîÑ Batch Processing Status: {completed_count}/{total_count} completed\")\n",
    "\n",
    "            # Show detailed status\n",
    "            for file_path, processor in processors.items():\n",
    "                batch_info = processor.get_batch_info()\n",
    "                filename = Path(file_path).name\n",
    "                print(f\"- {filename}: {batch_info.status}\")\n",
    "\n",
    "            # Check if all completed\n",
    "            if all(status == \"completed\" for status in statuses):\n",
    "                print(\"‚úÖ All batches completed successfully!\")\n",
    "                break\n",
    "\n",
    "            # Wait before next check\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚ö†Ô∏è Monitoring interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during monitoring: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "# Monitor initial status\n",
    "monitor_batch_status(batch_processors)\n",
    "\n",
    "# Start monitoring (this will run until completion)\n",
    "print(\"\\nüéØ Starting batch monitoring...\")\n",
    "print(\"Note: This cell will run until all batches are completed.\")\n",
    "print(\"You can interrupt with Ctrl+C if needed.\")\n",
    "\n",
    "wait_for_completion(batch_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Final batch status check:\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_0.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_1.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_2.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_3.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "- 20250417_youtube_comments_batch_api_4.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "\n",
      "üìä Completion Summary:\n",
      "- Completed batches: 5/5\n",
      "- Success rate: 100.0%\n",
      "‚úÖ All batches completed - ready for results processing\n",
      "\n",
      "üìä Completion Summary:\n",
      "- Completed batches: 5/5\n",
      "- Success rate: 100.0%\n",
      "‚úÖ All batches completed - ready for results processing\n"
     ]
    }
   ],
   "source": [
    "# Final status check for all batches\n",
    "print(\"üîç Final batch status check:\")\n",
    "for file_path, processor in batch_processors.items():\n",
    "    batch_info = processor.get_batch_info()\n",
    "    filename = Path(file_path).name\n",
    "    print(f\"- {filename}: {batch_info.status}\")\n",
    "\n",
    "    if batch_info.status == \"completed\":\n",
    "        print(f\"  ‚úÖ Ready for result processing\")\n",
    "    elif batch_info.status == \"failed\":\n",
    "        print(f\"  ‚ùå Batch failed - check error details\")\n",
    "    else:\n",
    "        print(f\"  ‚è≥ Still processing - current status: {batch_info.status}\")\n",
    "\n",
    "# Check if we can proceed to results processing\n",
    "completed_batches = sum(1 for processor in batch_processors.values() if processor.get_batch_info().status == \"completed\")\n",
    "total_batches = len(batch_processors)\n",
    "\n",
    "print(f\"\\nüìä Completion Summary:\")\n",
    "print(f\"- Completed batches: {completed_batches}/{total_batches}\")\n",
    "print(f\"- Success rate: {completed_batches / total_batches * 100:.1f}%\")\n",
    "\n",
    "if completed_batches == total_batches:\n",
    "    print(\"‚úÖ All batches completed - ready for results processing\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some batches are still pending - wait for completion before proceeding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "40000\n",
      "40000\n",
      "31946\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing batch results...\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_0.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_1.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_1.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_2.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_2.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_3.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_3.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_4.jsonl...\n",
      "  ‚úÖ Parsed: 40000, Errors: 0\n",
      "üìÇ Processing results from 20250417_youtube_comments_batch_api_4.jsonl...\n",
      "  ‚úÖ Parsed: 31946, Errors: 0\n",
      "\n",
      "üìä Results Processing Summary:\n",
      "- Total responses: 191,946\n",
      "- Successfully parsed: 191,946\n",
      "- Parse errors: 0\n",
      "- Success rate: 100.0%\n",
      "  ‚úÖ Parsed: 31946, Errors: 0\n",
      "\n",
      "üìä Results Processing Summary:\n",
      "- Total responses: 191,946\n",
      "- Successfully parsed: 191,946\n",
      "- Parse errors: 0\n",
      "- Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def process_batch_results(processors: Dict[str, OpenAIBatchProcessor]) -> Tuple[List[Any], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process and parse results from completed batch jobs.\n",
    "\n",
    "    Args:\n",
    "        processors: Dictionary of batch processors\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (parsed_results, raw_results)\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Processing batch results...\")\n",
    "\n",
    "    parsed_results = []\n",
    "    raw_results = []\n",
    "    error_count = 0\n",
    "\n",
    "    for file_path, processor in processors.items():\n",
    "        filename = Path(file_path).name\n",
    "        print(f\"üìÇ Processing results from {filename}...\")\n",
    "\n",
    "        try:\n",
    "            # Get batch output\n",
    "            file_response = processor.get_batch_output()\n",
    "            if not file_response:\n",
    "                print(f\"‚ö†Ô∏è No response data for {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Process each response\n",
    "            batch_parsed = 0\n",
    "            batch_errors = 0\n",
    "\n",
    "            for output in file_response:\n",
    "                try:\n",
    "                    # Parse the JSON response\n",
    "                    json_output = json.loads(output)\n",
    "                    function_args = json_output[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"]\n",
    "                    parsed_json = json.loads(function_args)\n",
    "\n",
    "                    # Validate with Pydantic model\n",
    "                    validated_obj = RespostaAnaliseSentimento.model_validate(parsed_json)\n",
    "                    parsed_results.append(validated_obj)\n",
    "                    raw_results.append(validated_obj.model_dump())\n",
    "                    batch_parsed += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Handle parsing errors\n",
    "                    parsed_results.append(None)\n",
    "                    raw_results.append(None)\n",
    "                    batch_errors += 1\n",
    "                    error_count += 1\n",
    "\n",
    "                    if batch_errors <= 3:  # Show first few errors\n",
    "                        print(f\"‚ö†Ô∏è Parsing error: {str(e)[:100]}\")\n",
    "\n",
    "            print(f\"  ‚úÖ Parsed: {batch_parsed}, Errors: {batch_errors}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    success_rate = (len(parsed_results) - error_count) / len(parsed_results) * 100 if parsed_results else 0\n",
    "\n",
    "    print(f\"\\nüìä Results Processing Summary:\")\n",
    "    print(f\"- Total responses: {len(parsed_results):,}\")\n",
    "    print(f\"- Successfully parsed: {len(parsed_results) - error_count:,}\")\n",
    "    print(f\"- Parse errors: {error_count:,}\")\n",
    "    print(f\"- Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "    return parsed_results, raw_results\n",
    "\n",
    "\n",
    "# Process all batch results\n",
    "parsed_results, raw_results = process_batch_results(batch_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving intermediate results...\n",
      "‚úÖ Parsed results saved: ../data/tmp/parsed_results_comments.joblib\n",
      "‚úÖ Parsed results saved: ../data/tmp/parsed_results_comments.joblib\n",
      "‚úÖ Raw results saved: ../data/tmp/results_comments.joblib\n",
      "üìä File sizes:\n",
      "- Parsed results: 10.62 MB\n",
      "- Raw results: 4.39 MB\n",
      "üîç Validating results consistency...\n",
      "- Original comments: 191,946\n",
      "- Classification results: 191,946\n",
      "‚úÖ Result count matches original data\n",
      "‚úÖ No null results found\n",
      "‚úÖ Raw results saved: ../data/tmp/results_comments.joblib\n",
      "üìä File sizes:\n",
      "- Parsed results: 10.62 MB\n",
      "- Raw results: 4.39 MB\n",
      "üîç Validating results consistency...\n",
      "- Original comments: 191,946\n",
      "- Classification results: 191,946\n",
      "‚úÖ Result count matches original data\n",
      "‚úÖ No null results found\n"
     ]
    }
   ],
   "source": [
    "def save_intermediate_results(parsed_results: List[Any], raw_results: List[Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Save intermediate results for backup and debugging.\n",
    "\n",
    "    Args:\n",
    "        parsed_results: List of parsed Pydantic objects\n",
    "        raw_results: List of raw result dictionaries\n",
    "    \"\"\"\n",
    "    print(\"üíæ Saving intermediate results...\")\n",
    "\n",
    "    try:\n",
    "        # Save parsed results\n",
    "        joblib.dump(parsed_results, ClassificationConfig.PARSED_RESULTS_FILE)\n",
    "        print(f\"‚úÖ Parsed results saved: {ClassificationConfig.PARSED_RESULTS_FILE}\")\n",
    "\n",
    "        # Save raw results\n",
    "        joblib.dump(raw_results, ClassificationConfig.RESULTS_FILE)\n",
    "        print(f\"‚úÖ Raw results saved: {ClassificationConfig.RESULTS_FILE}\")\n",
    "\n",
    "        # File size information\n",
    "        parsed_size = ClassificationConfig.PARSED_RESULTS_FILE.stat().st_size / (1024 * 1024)\n",
    "        raw_size = ClassificationConfig.RESULTS_FILE.stat().st_size / (1024 * 1024)\n",
    "\n",
    "        print(f\"üìä File sizes:\")\n",
    "        print(f\"- Parsed results: {parsed_size:.2f} MB\")\n",
    "        print(f\"- Raw results: {raw_size:.2f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving intermediate results: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def validate_results_consistency(parsed_results: List[Any], original_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Validate that results match the original data structure.\n",
    "\n",
    "    Args:\n",
    "        parsed_results: List of classification results\n",
    "        original_df: Original DataFrame with comments\n",
    "    \"\"\"\n",
    "    print(\"üîç Validating results consistency...\")\n",
    "\n",
    "    print(f\"- Original comments: {len(original_df):,}\")\n",
    "    print(f\"- Classification results: {len(parsed_results):,}\")\n",
    "\n",
    "    if len(parsed_results) == len(original_df):\n",
    "        print(\"‚úÖ Result count matches original data\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Result count mismatch - check for processing errors\")\n",
    "\n",
    "    # Check for null results\n",
    "    null_count = sum(1 for result in parsed_results if result is None)\n",
    "    if null_count > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {null_count} null results ({null_count / len(parsed_results) * 100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚úÖ No null results found\")\n",
    "\n",
    "\n",
    "# Save intermediate results for backup\n",
    "save_intermediate_results(parsed_results, raw_results)\n",
    "\n",
    "# Validate consistency\n",
    "validate_results_consistency(parsed_results, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Creating classification DataFrame...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3aa603436b41edb3fa2c0440fc4fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing results:   0%|          | 0/191946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created DataFrame with 191,946 classification results\n",
      "\n",
      "üìà Classification Statistics:\n",
      "- Sentiment distribution: {'positivo': 81684, 'neutro': 63692, 'negativo': 46569, '': 1}\n",
      "- Explicit gordofobia: 12,355 (6.4%)\n",
      "- Implicit gordofobia: 19,623 (10.2%)\n",
      "- Obesity-related: 20,512 (10.7%)\n",
      "- Top languages: {'pt': 189912, 'es': 1900, 'en': 108, 'id': 18, 'fr': 2}\n"
     ]
    }
   ],
   "source": [
    "def create_classification_dataframe(parsed_results: List[Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert parsed classification results into a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        parsed_results: List of parsed classification objects\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with classification results\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating classification DataFrame...\")\n",
    "\n",
    "    outputs = []\n",
    "\n",
    "    for i in tqdm(range(len(parsed_results)), desc=\"Processing results\"):\n",
    "        parsed_document = parsed_results[i]\n",
    "\n",
    "        if parsed_document is not None:\n",
    "            # Convert to dictionary\n",
    "            result_dict = parsed_document.model_dump()\n",
    "            outputs.append(result_dict)\n",
    "        else:\n",
    "            # Handle null results with default values\n",
    "            outputs.append(\n",
    "                {\n",
    "                    \"sentimento\": None,\n",
    "                    \"gordofobia_implicita\": None,\n",
    "                    \"gordofobia_explicita\": None,\n",
    "                    \"idioma\": None,\n",
    "                    \"obesidade\": None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_classifications = pd.DataFrame(outputs)\n",
    "    df_classifications.columns = [\"sentimento\", \"gordofobia_implicita\", \"gordofobia_explicita\", \"idioma\", \"obesidade\"]\n",
    "\n",
    "    print(f\"‚úÖ Created DataFrame with {len(df_classifications):,} classification results\")\n",
    "\n",
    "    # Display classification statistics\n",
    "    print(f\"\\nüìà Classification Statistics:\")\n",
    "\n",
    "    if \"sentimento\" in df_classifications.columns:\n",
    "        sentiment_counts = df_classifications[\"sentimento\"].value_counts()\n",
    "        print(f\"- Sentiment distribution: {dict(sentiment_counts)}\")\n",
    "\n",
    "    if \"gordofobia_explicita\" in df_classifications.columns:\n",
    "        explicit_count = df_classifications[\"gordofobia_explicita\"].sum()\n",
    "        print(f\"- Explicit gordofobia: {explicit_count:,} ({explicit_count / len(df_classifications) * 100:.1f}%)\")\n",
    "\n",
    "    if \"gordofobia_implicita\" in df_classifications.columns:\n",
    "        implicit_count = df_classifications[\"gordofobia_implicita\"].sum()\n",
    "        print(f\"- Implicit gordofobia: {implicit_count:,} ({implicit_count / len(df_classifications) * 100:.1f}%)\")\n",
    "\n",
    "    if \"obesidade\" in df_classifications.columns:\n",
    "        obesity_count = df_classifications[\"obesidade\"].sum()\n",
    "        print(f\"- Obesity-related: {obesity_count:,} ({obesity_count / len(df_classifications) * 100:.1f}%)\")\n",
    "\n",
    "    if \"idioma\" in df_classifications.columns:\n",
    "        language_counts = df_classifications[\"idioma\"].value_counts().head()\n",
    "        print(f\"- Top languages: {dict(language_counts)}\")\n",
    "\n",
    "    return df_classifications\n",
    "\n",
    "\n",
    "# Create the classification DataFrame\n",
    "df_classifications = create_classification_dataframe(parsed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Integrating classification results with original data...\n",
      "‚úÖ Successfully integrated data\n",
      "üìä Final dataset shape: (191946, 25)\n",
      "üìã Total columns: 25\n",
      "\n",
      "üîç Sample Integrated Data:\n",
      "\n",
      "Comment 1:\n",
      "  Text: Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer\n",
      "  sentimento: neutro\n",
      "  gordofobia_explicita: False\n",
      "  gordofobia_implicita: False\n",
      "  idioma: pt\n",
      "  obesidade: False\n",
      "\n",
      "Comment 2:\n",
      "  Text: Chefe wigol deu um beijo grego no homer skksks\n",
      "  sentimento: neutro\n",
      "  gordofobia_explicita: False\n",
      "  gordofobia_implicita: False\n",
      "  idioma: pt\n",
      "  obesidade: False\n",
      "\n",
      "Comment 3:\n",
      "  Text: Quem era Batedor de Carteiras ?\n",
      "  sentimento: neutro\n",
      "  gordofobia_explicita: False\n",
      "  gordofobia_implicita: False\n",
      "  idioma: pt\n",
      "  obesidade: False\n",
      "\n",
      "üìà Integration Summary:\n",
      "- Original comment columns: 20\n",
      "- Classification columns: 5\n",
      "- Final dataset columns: 25\n",
      "- Total records: 191,946\n",
      "\n",
      "‚úÖ No null classifications found\n"
     ]
    }
   ],
   "source": [
    "def integrate_classification_results(original_df: pd.DataFrame, classifications_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Integrate classification results with original comment data.\n",
    "\n",
    "    Args:\n",
    "        original_df: Original DataFrame with comments\n",
    "        classifications_df: DataFrame with classification results\n",
    "\n",
    "    Returns:\n",
    "        Combined DataFrame with original data and classifications\n",
    "    \"\"\"\n",
    "    print(\"üîó Integrating classification results with original data...\")\n",
    "\n",
    "    # Verify dimensions match\n",
    "    if len(original_df) != len(classifications_df):\n",
    "        raise ValueError(f\"Dimension mismatch: original ({len(original_df)}) vs classifications ({len(classifications_df)})\")\n",
    "\n",
    "    # Combine dataframes\n",
    "    df_integrated = pd.concat([original_df, classifications_df], axis=1)\n",
    "\n",
    "    print(f\"‚úÖ Successfully integrated data\")\n",
    "    print(f\"üìä Final dataset shape: {df_integrated.shape}\")\n",
    "    print(f\"üìã Total columns: {len(df_integrated.columns)}\")\n",
    "\n",
    "    # Display sample of integrated data\n",
    "    print(f\"\\nüîç Sample Integrated Data:\")\n",
    "    sample_cols = [\"textDisplay\", \"sentimento\", \"gordofobia_explicita\", \"gordofobia_implicita\", \"idioma\", \"obesidade\"]\n",
    "    available_cols = [col for col in sample_cols if col in df_integrated.columns]\n",
    "\n",
    "    if available_cols:\n",
    "        sample_data = df_integrated[available_cols].head(3)\n",
    "        for idx, row in sample_data.iterrows():\n",
    "            print(f\"\\nComment {idx + 1}:\")\n",
    "            text_preview = row[\"textDisplay\"][:80] + \"...\" if len(row[\"textDisplay\"]) > 80 else row[\"textDisplay\"]\n",
    "            print(f\"  Text: {text_preview}\")\n",
    "            for col in available_cols[1:]:  # Skip textDisplay\n",
    "                print(f\"  {col}: {row[col]}\")\n",
    "\n",
    "    return df_integrated\n",
    "\n",
    "\n",
    "# Integrate the results\n",
    "df_final = integrate_classification_results(df, df_classifications)\n",
    "\n",
    "# Display integration summary\n",
    "print(f\"\\nüìà Integration Summary:\")\n",
    "print(f\"- Original comment columns: {len(df.columns)}\")\n",
    "print(f\"- Classification columns: {len(df_classifications.columns)}\")\n",
    "print(f\"- Final dataset columns: {len(df_final.columns)}\")\n",
    "print(f\"- Total records: {len(df_final):,}\")\n",
    "\n",
    "# Check for any data quality issues\n",
    "null_classifications = df_final[[\"sentimento\", \"gordofobia_explicita\", \"gordofobia_implicita\", \"idioma\", \"obesidade\"]].isnull().sum()\n",
    "if null_classifications.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Null classifications found:\")\n",
    "    for col, count in null_classifications.items():\n",
    "        if count > 0:\n",
    "            print(f\"  - {col}: {count} null values\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No null classifications found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Performing final dataset validation...\n",
      "\n",
      "üìä Dataset Validation Results:\n",
      "- Status: EXCELLENT\n",
      "- Total records: 191,946\n",
      "- Total columns: 25\n",
      "- Overall completeness: 100.0%\n",
      "\n",
      "üìà Classification Completeness:\n",
      "- sentimento: 100.0% (0 null values)\n",
      "- gordofobia_explicita: 100.0% (0 null values)\n",
      "- gordofobia_implicita: 100.0% (0 null values)\n",
      "- idioma: 100.0% (0 null values)\n",
      "- obesidade: 100.0% (0 null values)\n",
      "\n",
      "üéØ Gordofobia Detection Results:\n",
      "- Explicit gordofobia: 12,355 comments\n",
      "- Implicit gordofobia: 19,623 comments\n",
      "- Any gordofobia: 31,538 comments (16.4%)\n",
      "\n",
      "üåê Top Languages Detected:\n",
      "- pt: 189,912 comments\n",
      "- es: 1,900 comments\n",
      "- en: 108 comments\n",
      "- id: 18 comments\n",
      "- fr: 2 comments\n"
     ]
    }
   ],
   "source": [
    "def validate_final_dataset(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform comprehensive validation of the final classified dataset.\n",
    "\n",
    "    Args:\n",
    "        df: Final dataset with classifications\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with validation results\n",
    "    \"\"\"\n",
    "    print(\"üîç Performing final dataset validation...\")\n",
    "\n",
    "    validation_results = {}\n",
    "\n",
    "    # Basic structure validation\n",
    "    validation_results[\"total_records\"] = len(df)\n",
    "    validation_results[\"total_columns\"] = len(df.columns)\n",
    "\n",
    "    # Check required columns\n",
    "    required_columns = [\"textDisplay\", \"video_id\", \"sentimento\", \"gordofobia_explicita\", \"gordofobia_implicita\", \"idioma\", \"obesidade\"]\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    validation_results[\"missing_columns\"] = missing_columns\n",
    "\n",
    "    # Classification completeness\n",
    "    classification_columns = [\"sentimento\", \"gordofobia_explicita\", \"gordofobia_implicita\", \"idioma\", \"obesidade\"]\n",
    "    for col in classification_columns:\n",
    "        if col in df.columns:\n",
    "            null_count = df[col].isnull().sum()\n",
    "            validation_results[f\"{col}_null_count\"] = null_count\n",
    "            validation_results[f\"{col}_completeness\"] = (len(df) - null_count) / len(df) * 100\n",
    "\n",
    "    # Data quality checks\n",
    "    if \"sentimento\" in df.columns:\n",
    "        valid_sentiments = [\"positivo\", \"negativo\", \"neutro\"]\n",
    "        invalid_sentiments = df[~df[\"sentimento\"].isin(valid_sentiments + [None])][\"sentimento\"].value_counts()\n",
    "        validation_results[\"invalid_sentiments\"] = dict(invalid_sentiments)\n",
    "\n",
    "    # Language distribution\n",
    "    if \"idioma\" in df.columns:\n",
    "        language_dist = df[\"idioma\"].value_counts().head(10)\n",
    "        validation_results[\"top_languages\"] = dict(language_dist)\n",
    "\n",
    "    # Gordofobia analysis\n",
    "    if \"gordofobia_explicita\" in df.columns and \"gordofobia_implicita\" in df.columns:\n",
    "        explicit_count = df[\"gordofobia_explicita\"].sum()\n",
    "        implicit_count = df[\"gordofobia_implicita\"].sum()\n",
    "        any_gordofobia = (df[\"gordofobia_explicita\"] | df[\"gordofobia_implicita\"]).sum()\n",
    "\n",
    "        validation_results[\"gordofobia_explicit\"] = explicit_count\n",
    "        validation_results[\"gordofobia_implicit\"] = implicit_count\n",
    "        validation_results[\"gordofobia_any\"] = any_gordofobia\n",
    "        validation_results[\"gordofobia_rate\"] = any_gordofobia / len(df) * 100\n",
    "\n",
    "    # Overall quality score\n",
    "    completeness_scores = [validation_results[f\"{col}_completeness\"] for col in classification_columns if f\"{col}_completeness\" in validation_results]\n",
    "    validation_results[\"overall_completeness\"] = sum(completeness_scores) / len(completeness_scores) if completeness_scores else 0\n",
    "\n",
    "    # Determine validation status\n",
    "    if validation_results[\"overall_completeness\"] >= 95:\n",
    "        validation_results[\"status\"] = \"excellent\"\n",
    "    elif validation_results[\"overall_completeness\"] >= 85:\n",
    "        validation_results[\"status\"] = \"good\"\n",
    "    elif validation_results[\"overall_completeness\"] >= 70:\n",
    "        validation_results[\"status\"] = \"acceptable\"\n",
    "    else:\n",
    "        validation_results[\"status\"] = \"poor\"\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "\n",
    "# Validate the final dataset\n",
    "validation_results = validate_final_dataset(df_final)\n",
    "\n",
    "# Display validation results\n",
    "print(f\"\\nüìä Dataset Validation Results:\")\n",
    "print(f\"- Status: {validation_results['status'].upper()}\")\n",
    "print(f\"- Total records: {validation_results['total_records']:,}\")\n",
    "print(f\"- Total columns: {validation_results['total_columns']}\")\n",
    "print(f\"- Overall completeness: {validation_results['overall_completeness']:.1f}%\")\n",
    "\n",
    "if validation_results[\"missing_columns\"]:\n",
    "    print(f\"‚ö†Ô∏è Missing columns: {validation_results['missing_columns']}\")\n",
    "\n",
    "print(f\"\\nüìà Classification Completeness:\")\n",
    "classification_columns = [\"sentimento\", \"gordofobia_explicita\", \"gordofobia_implicita\", \"idioma\", \"obesidade\"]\n",
    "for col in classification_columns:\n",
    "    if f\"{col}_completeness\" in validation_results:\n",
    "        completeness = validation_results[f\"{col}_completeness\"]\n",
    "        null_count = validation_results[f\"{col}_null_count\"]\n",
    "        print(f\"- {col}: {completeness:.1f}% ({null_count:,} null values)\")\n",
    "\n",
    "if \"gordofobia_rate\" in validation_results:\n",
    "    print(f\"\\nüéØ Gordofobia Detection Results:\")\n",
    "    print(f\"- Explicit gordofobia: {validation_results['gordofobia_explicit']:,} comments\")\n",
    "    print(f\"- Implicit gordofobia: {validation_results['gordofobia_implicit']:,} comments\")\n",
    "    print(f\"- Any gordofobia: {validation_results['gordofobia_any']:,} comments ({validation_results['gordofobia_rate']:.1f}%)\")\n",
    "\n",
    "if \"top_languages\" in validation_results:\n",
    "    print(f\"\\nüåê Top Languages Detected:\")\n",
    "    for lang, count in list(validation_results[\"top_languages\"].items())[:5]:\n",
    "        print(f\"- {lang}: {count:,} comments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting classified dataset to: ../data/intermediate/20250417_youtube_comments_yes_labels.parquet\n",
      "üìÅ Existing file backed up to: 20250417_youtube_comments_yes_labels.backup_20250724_094513.parquet\n",
      "‚úÖ Export successful!\n",
      "üìÅ File: 20250417_youtube_comments_yes_labels.parquet\n",
      "üìä Size: 47.22 MB\n",
      "üìà Records: 191,946\n",
      "üìã Columns: 25\n",
      "‚úÖ Export verification passed\n",
      "\n",
      "üéâ Classification pipeline completed successfully!\n",
      "üìÅ Output file: ../data/intermediate/20250417_youtube_comments_yes_labels.parquet\n",
      "üìä Final dataset: 191,946 comments with classifications\n",
      "\n",
      "üìà Final Summary:\n",
      "- Sentiment distribution: {'positivo': 81684, 'neutro': 63692, 'negativo': 46569, '': 1}\n",
      "- Obesity-related comments: 20,512\n",
      "- Comments with gordofobia: 31,538\n"
     ]
    }
   ],
   "source": [
    "def export_classified_dataset(df: pd.DataFrame, output_path: Path, validation_results: Dict[str, Any]) -> bool:\n",
    "    \"\"\"\n",
    "    Export the final classified dataset with validation checks.\n",
    "\n",
    "    Args:\n",
    "        df: Final dataset to export\n",
    "        output_path: Path where to save the dataset\n",
    "        validation_results: Results from validation\n",
    "\n",
    "    Returns:\n",
    "        True if export successful, False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"üíæ Exporting classified dataset to: {output_path}\")\n",
    "\n",
    "    try:\n",
    "        # Check if export should proceed based on validation\n",
    "        if validation_results[\"status\"] in [\"poor\"]:\n",
    "            print(\"‚ö†Ô∏è Dataset quality is poor - export may contain significant issues\")\n",
    "\n",
    "        # Create backup if file already exists\n",
    "        if output_path.exists():\n",
    "            backup_path = output_path.with_suffix(f\".backup_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.parquet\")\n",
    "            output_path.rename(backup_path)\n",
    "            print(f\"üìÅ Existing file backed up to: {backup_path.name}\")\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Export to parquet format\n",
    "        df.to_parquet(output_path, index=False)\n",
    "\n",
    "        # Verify export\n",
    "        exported_size = output_path.stat().st_size\n",
    "        print(f\"‚úÖ Export successful!\")\n",
    "        print(f\"üìÅ File: {output_path.name}\")\n",
    "        print(f\"üìä Size: {exported_size / (1024 * 1024):.2f} MB\")\n",
    "        print(f\"üìà Records: {len(df):,}\")\n",
    "        print(f\"üìã Columns: {len(df.columns)}\")\n",
    "\n",
    "        # Test read-back\n",
    "        test_df = pd.read_parquet(output_path)\n",
    "        if len(test_df) == len(df) and len(test_df.columns) == len(df.columns):\n",
    "            print(\"‚úÖ Export verification passed\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Export verification failed - file may be corrupted\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Export the final classified dataset\n",
    "export_success = export_classified_dataset(df_final, ClassificationConfig.OUTPUT_FILE, validation_results)\n",
    "\n",
    "if export_success:\n",
    "    print(f\"\\nüéâ Classification pipeline completed successfully!\")\n",
    "    print(f\"üìÅ Output file: {ClassificationConfig.OUTPUT_FILE}\")\n",
    "    print(f\"üìä Final dataset: {len(df_final):,} comments with classifications\")\n",
    "\n",
    "    # Display final summary statistics\n",
    "    print(f\"\\nüìà Final Summary:\")\n",
    "    if \"sentimento\" in df_final.columns:\n",
    "        sentiment_dist = df_final[\"sentimento\"].value_counts()\n",
    "        print(f\"- Sentiment distribution: {dict(sentiment_dist)}\")\n",
    "\n",
    "    if \"obesidade\" in df_final.columns:\n",
    "        obesity_count = df_final[\"obesidade\"].sum()\n",
    "        print(f\"- Obesity-related comments: {obesity_count:,}\")\n",
    "\n",
    "    gordofobia_any = (df_final.get(\"gordofobia_explicita\", False) | df_final.get(\"gordofobia_implicita\", False)).sum()\n",
    "    print(f\"- Comments with gordofobia: {gordofobia_any:,}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Export failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Zero-Shot Classification Pipeline Summary\n",
      "==================================================\n",
      "\n",
      "üìä Input Data:\n",
      "- Source file: 20250417_youtube_comments_pt_cleaned1.parquet\n",
      "- Comments processed: 191,946\n",
      "- Unique videos: 1,204\n",
      "\n",
      "ü§ñ Model Configuration:\n",
      "- Model: gpt-4.1-mini\n",
      "- Temperature: 0.0\n",
      "- Batch size: 40,000\n",
      "\n",
      "üè∑Ô∏è Classification Schema:\n",
      "- Sentiment analysis (positivo/negativo/neutro)\n",
      "- Gordofobia detection (explicit/implicit)\n",
      "- Language identification (ISO codes)\n",
      "- Obesity content flagging\n",
      "\n",
      "üìà Results Summary:\n",
      "- Final dataset: 191,946 records\n",
      "- Classification completeness: 100.0%\n",
      "- Data quality: EXCELLENT\n",
      "\n",
      "üí≠ Sentiment Analysis:\n",
      "  - positivo: 81,684 (42.6%)\n",
      "  - neutro: 63,692 (33.2%)\n",
      "  - negativo: 46,569 (24.3%)\n",
      "  - : 1 (0.0%)\n",
      "\n",
      "‚ö†Ô∏è Gordofobia Detection:\n",
      "  - Explicit: 12,355 (6.4%)\n",
      "  - Implicit: 19,623 (10.2%)\n",
      "  - Any form: 31,538 (16.4%)\n",
      "\n",
      "üåê Language Distribution:\n",
      "  - pt: 189,912 (98.9%)\n",
      "  - es: 1,900 (1.0%)\n",
      "  - en: 108 (0.1%)\n",
      "  - id: 18 (0.0%)\n",
      "  - fr: 2 (0.0%)\n",
      "\n",
      "üè• Obesity-Related Content:\n",
      "  - Comments mentioning obesity: 20,512 (10.7%)\n",
      "\n",
      "üìÅ Output Files:\n",
      "- Main dataset: ../data/intermediate/20250417_youtube_comments_yes_labels.parquet\n",
      "- Backup results: ../data/tmp/parsed_results_comments.joblib\n",
      "- Raw results: ../data/tmp/results_comments.joblib\n",
      "\n",
      "üèÅ Zero-Shot Classification Pipeline Complete! ‚ú®\n",
      "üìä Dataset ready for research analysis and publication\n"
     ]
    }
   ],
   "source": [
    "def generate_pipeline_summary() -> None:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of the classification pipeline.\n",
    "    \"\"\"\n",
    "    print(\"üìã Zero-Shot Classification Pipeline Summary\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Input data summary\n",
    "    print(f\"\\nüìä Input Data:\")\n",
    "    print(f\"- Source file: {ClassificationConfig.INPUT_FILE.name}\")\n",
    "    print(f\"- Comments processed: {len(df):,}\")\n",
    "    print(f\"- Unique videos: {df['video_id'].nunique():,}\")\n",
    "\n",
    "    # Model and configuration\n",
    "    print(f\"\\nü§ñ Model Configuration:\")\n",
    "    print(f\"- Model: {ClassificationConfig.MODEL_NAME}\")\n",
    "    print(f\"- Temperature: {ClassificationConfig.TEMPERATURE}\")\n",
    "    print(f\"- Batch size: {ClassificationConfig.BATCH_SIZE:,}\")\n",
    "\n",
    "    # Classification schema\n",
    "    print(f\"\\nüè∑Ô∏è Classification Schema:\")\n",
    "    print(f\"- Sentiment analysis (positivo/negativo/neutro)\")\n",
    "    print(f\"- Gordofobia detection (explicit/implicit)\")\n",
    "    print(f\"- Language identification (ISO codes)\")\n",
    "    print(f\"- Obesity content flagging\")\n",
    "\n",
    "    # Results summary\n",
    "    if \"df_final\" in globals() and not df_final.empty:\n",
    "        print(f\"\\nüìà Results Summary:\")\n",
    "        print(f\"- Final dataset: {len(df_final):,} records\")\n",
    "        print(f\"- Classification completeness: {validation_results.get('overall_completeness', 0):.1f}%\")\n",
    "        print(f\"- Data quality: {validation_results.get('status', 'unknown').upper()}\")\n",
    "\n",
    "        # Sentiment distribution\n",
    "        if \"sentimento\" in df_final.columns:\n",
    "            sentiment_stats = df_final[\"sentimento\"].value_counts()\n",
    "            print(f\"\\nüí≠ Sentiment Analysis:\")\n",
    "            for sentiment, count in sentiment_stats.items():\n",
    "                percentage = count / len(df_final) * 100\n",
    "                print(f\"  - {sentiment}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Gordofobia detection\n",
    "        if \"gordofobia_explicita\" in df_final.columns and \"gordofobia_implicita\" in df_final.columns:\n",
    "            explicit_count = df_final[\"gordofobia_explicita\"].sum()\n",
    "            implicit_count = df_final[\"gordofobia_implicita\"].sum()\n",
    "            any_gordofobia = (df_final[\"gordofobia_explicita\"] | df_final[\"gordofobia_implicita\"]).sum()\n",
    "\n",
    "            print(f\"\\n‚ö†Ô∏è Gordofobia Detection:\")\n",
    "            print(f\"  - Explicit: {explicit_count:,} ({explicit_count / len(df_final) * 100:.1f}%)\")\n",
    "            print(f\"  - Implicit: {implicit_count:,} ({implicit_count / len(df_final) * 100:.1f}%)\")\n",
    "            print(f\"  - Any form: {any_gordofobia:,} ({any_gordofobia / len(df_final) * 100:.1f}%)\")\n",
    "\n",
    "        # Language distribution\n",
    "        if \"idioma\" in df_final.columns:\n",
    "            lang_stats = df_final[\"idioma\"].value_counts().head(5)\n",
    "            print(f\"\\nüåê Language Distribution:\")\n",
    "            for lang, count in lang_stats.items():\n",
    "                percentage = count / len(df_final) * 100\n",
    "                print(f\"  - {lang}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Obesity content\n",
    "        if \"obesidade\" in df_final.columns:\n",
    "            obesity_count = df_final[\"obesidade\"].sum()\n",
    "            print(f\"\\nüè• Obesity-Related Content:\")\n",
    "            print(f\"  - Comments mentioning obesity: {obesity_count:,} ({obesity_count / len(df_final) * 100:.1f}%)\")\n",
    "\n",
    "    # Output files\n",
    "    print(f\"\\nüìÅ Output Files:\")\n",
    "    print(f\"- Main dataset: {ClassificationConfig.OUTPUT_FILE}\")\n",
    "    print(f\"- Backup results: {ClassificationConfig.PARSED_RESULTS_FILE}\")\n",
    "    print(f\"- Raw results: {ClassificationConfig.RESULTS_FILE}\")\n",
    "\n",
    "\n",
    "# Generate comprehensive summary\n",
    "generate_pipeline_summary()\n",
    "\n",
    "\n",
    "print(f\"\\nüèÅ Zero-Shot Classification Pipeline Complete! ‚ú®\")\n",
    "print(f\"üìä Dataset ready for research analysis and publication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_youtube_weight_stigma_1e733e1bf2b6c34f6bcb8483dce2a479",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
