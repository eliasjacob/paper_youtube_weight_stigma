{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7562dfc9",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification of YouTube Video Titles for Weight Stigma Analysis\n",
    "\n",
    "## Research Overview\n",
    "\n",
    "This notebook implements zero-shot classification of YouTube video titles to identify weight stigma content and sentiment patterns. As part of a comprehensive multi-modal analysis framework, this component complements comment classification (notebook 04) and transcription analysis (notebook 05) to provide complete coverage of YouTube content related to weight stigma research.\n",
    "\n",
    "## Scientific Objectives\n",
    "\n",
    "1. **Multi-Modal Content Analysis**: Classify video titles alongside comments and transcriptions for comprehensive YouTube content analysis\n",
    "2. **Weight Stigma Detection**: Identify explicit and implicit weight discrimination patterns in video titles using advanced NLP techniques\n",
    "3. **Sentiment Analysis**: Determine emotional sentiment patterns in obesity-related video titles for psychological research insights\n",
    "4. **Language Processing**: Detect and filter content by language for culturally-specific analysis of Brazilian Portuguese content\n",
    "5. **Research Integration**: Generate publication-ready datasets that integrate seamlessly with existing research workflows\n",
    "\n",
    "## Methodology and Technical Approach\n",
    "\n",
    "- **Zero-Shot Classification**: Leveraging OpenAI GPT-4o-mini-2024-07-18 for structured content analysis without training data requirements\n",
    "- **Batch Processing**: Efficient large-scale classification via OpenAI Batch API for cost-effective research operations\n",
    "- **Structured Outputs**: Pydantic models ensure consistent, validated data formats suitable for statistical analysis\n",
    "- **Quality Control**: Comprehensive validation, error handling, and data integrity checks throughout the pipeline\n",
    "- **Reproducible Research**: Standardized configuration management and comprehensive documentation for research reproducibility\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "The pipeline implements established research patterns optimized for video title analysis:\n",
    "\n",
    "1. **Data Loading and Preparation**: Integration with cleaned video datasets from previous processing stages\n",
    "2. **Schema Definition**: Pydantic-based structured output models for consistent data collection\n",
    "3. **System Prompt Engineering**: Specialized prompts optimized for video title content and weight stigma detection\n",
    "4. **Batch API Processing**: Scalable classification using OpenAI's batch processing infrastructure\n",
    "5. **Results Processing**: Comprehensive data validation, integration, and export for downstream research\n",
    "\n",
    "## Research Context and Contributions\n",
    "\n",
    "This notebook addresses the critical need for systematic analysis of weight stigma in digital media. Video titles represent a unique content modality that often serves as the primary gateway for audience engagement, making their analysis essential for understanding how weight-related content is framed and potentially stigmatizing messaging is propagated in digital spaces.\n",
    "\n",
    "The methodology contributes to computational social science research by providing a scalable framework for content analysis that can be adapted to other forms of digital discrimination research.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction and Setup\n",
    "\n",
    "This section establishes the computational environment and imports all necessary libraries for video title classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34ab2e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Video Title Classification Pipeline - Environment Setup Complete\n",
      "üìä Notebook 06: Zero-shot classification of YouTube video titles\n",
      "üéØ Focus: Weight stigma detection in video title content\n",
      "üîß Processing: OpenAI GPT-4o-mini with batch API for large-scale analysis\n",
      "\n",
      "üìã Technical Configuration:\n",
      "- Python version: 3.12.10\n",
      "- Working directory: /media/nas-elias/pesquisas/papers/paper_savio_youtube/paper_youtube_weight_stigma\n",
      "- Environment variables loaded: ‚úÖ\n",
      "‚úÖ All imports and environment setup completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Core imports for video title classification pipeline\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Literal, Optional, Dict, Any, Tuple\n",
    "\n",
    "# Add project root to path for local modules\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Environment and configuration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain utilities for OpenAI integration\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "# Pydantic for structured data validation\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# File system utilities\n",
    "from glob import glob\n",
    "import joblib\n",
    "\n",
    "# Custom modules for API processing\n",
    "from openai_api import OpenAIBatchProcessor\n",
    "\n",
    "# IPython utilities for notebook display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üöÄ Video Title Classification Pipeline - Environment Setup Complete\")\n",
    "print(\"üìä Notebook 06: Zero-shot classification of YouTube video titles\")\n",
    "print(\"üéØ Focus: Weight stigma detection in video title content\")\n",
    "print(\"üîß Processing: OpenAI GPT-4o-mini with batch API for large-scale analysis\")\n",
    "\n",
    "# Display technical specifications\n",
    "print(f\"\\nüìã Technical Configuration:\")\n",
    "print(f\"- Python version: {sys.version.split()[0]}\")\n",
    "print(f\"- Working directory: {Path.cwd()}\")\n",
    "print(f\"- Environment variables loaded: {'‚úÖ' if os.getenv('OPENAI_API_KEY') else '‚ùå'}\")\n",
    "\n",
    "print(\"‚úÖ All imports and environment setup completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da1e87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directory structure verified and created\n",
      "üîç Configuration Validation:\n",
      "  ‚úÖ Input file found: 47.12 MB\n",
      "  ‚úÖ Model: gpt-4.1-mini\n",
      "  ‚úÖ Temperature: 0.0 (deterministic)\n",
      "  ‚úÖ Batch size: 40,000 requests\n",
      "  ‚úÖ Language filter: pt\n",
      "‚öôÔ∏è Video Title Classification Configuration:\n",
      "- Model: gpt-4.1-mini\n",
      "- Temperature: 0.0\n",
      "- Batch size: 40,000\n",
      "- Language filter: pt\n",
      "- Input file: 20250417_youtube_comments_pt_cleaned1.parquet\n",
      "- Output file: 20250417_youtube_titles_yes_labels.parquet\n",
      "\n",
      "üìä Loading video title dataset...\n",
      "üìà Original dataset: 191,946 records\n",
      "üìà Unique videos: 1,204 records\n",
      "\n",
      "üìã Video Title Dataset Overview:\n",
      "- Total unique videos: 1,204\n",
      "- Dataset columns: ['video_id', 'channelId', 'videoId', 'textDisplay', 'textOriginal', 'authorDisplayName', 'authorProfileImageUrl', 'authorChannelUrl', 'authorChannelId', 'canRate', 'viewerRating', 'likeCount', 'publishedAt', 'updatedAt', 'author', 'comment', 'date', 'likes', 'video_title', 'language']\n",
      "- Video title statistics:\n",
      "  ‚Ä¢ Non-null titles: 1,204 (100.0%)\n",
      "  ‚Ä¢ Length range: 10-100 characters\n",
      "  ‚Ä¢ Mean length: 63.3 characters\n",
      "  ‚Ä¢ Median length: 64.0 characters\n",
      "\n",
      "üìù Sample Video Titles:\n",
      "  1. Tony Gordo √© Incriminado #simpsons\n",
      "  2. O pa√≠s mais obeso do mundo #shorts\n",
      "  3. Preconceitos que eu j√° sofri por ser uma bailarina gorda.\n",
      "‚úÖ Data loading and preparation completed\n",
      "üéØ Ready to process 1,204 video titles for weight stigma classification\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6a862a22-16f9-4d20-91c3-a00aa2972e16",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt"
        ],
        [
         "1",
         "-1DN4904BQw",
         "UCbDy7ap3Ixk45DILe4O6Tbw",
         "-1DN4904BQw",
         "Aula gratuita: https://bit.ly/3RLbmWq",
         "Aula gratuita: https://bit.ly/3RLbmWq",
         "@sejasaudavel5167",
         "https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJnhxv_fnBhMBnW0RAs0EbYwGirxg_O5haaogRSzXIAEmE5bJkObg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@sejasaudavel5167",
         "UCbDy7ap3Ixk45DILe4O6Tbw",
         "True",
         "none",
         "1.0",
         "2022-10-06 20:41:16+00:00",
         "2022-10-06 20:41:16+00:00",
         null,
         null,
         null,
         null,
         "O pa√≠s mais obeso do mundo #shorts",
         "pt"
        ],
        [
         "2",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Vc √© linda e sua auto-estima √© contagiante. Seu testemunho vai dar for√ßa pra muitas pessoas! Quem v√™ cara ou corpo n√£o v√™ talento ou supera√ß√£o!",
         "Vc √© linda e sua auto-estima √© contagiante. Seu testemunho vai dar for√ßa pra muitas pessoas! Quem v√™ cara ou corpo n√£o v√™ talento ou supera√ß√£o!",
         "@isabelitacorrea2611",
         "https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkXrkG1-bxGyLdN5Kw-Vb4IQMCNkSM=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@isabelitacorrea2611",
         "UCnxzchRu-oFH4H4SKeF2C-Q",
         "True",
         "none",
         "176.0",
         "2024-05-02 01:58:41+00:00",
         "2024-05-02 01:58:41+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt"
        ],
        [
         "3",
         "-6Qxw7CpQvQ",
         "UC6cALLZLWQGilBFBB0PWAog",
         "-6Qxw7CpQvQ",
         "V√≠deo completo: https://youtu.be/hnetjD-gje4",
         "V√≠deo completo: https://youtu.be/hnetjD-gje4",
         "",
         "https://yt3.ggpht.com/xXlZxbOOYCKigMGIaVKMpvi1kLiFsU7fSYMPrJeyS2qNyne1r4SA5iZxjS3mRB5ZWg_IDP8zvbg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/c/MaiconK%C3%BCster",
         "UC6cALLZLWQGilBFBB0PWAog",
         "True",
         "none",
         "436.0",
         "2023-01-26 00:25:05+00:00",
         "2023-01-26 00:25:05+00:00",
         null,
         null,
         null,
         null,
         "esse milion√°rio de 18 anos n√£o quer pegar mulher gorda",
         "pt"
        ],
        [
         "4",
         "-7fJRjz1BCM",
         "UC9mdw2mmn49ZuqGOpSri7Fw",
         "-7fJRjz1BCM",
         "Reuni√£o pra saber como roubar mais, desgoverno vergonhoso,cpilantra mentiroso! S√≥ engana trouxa",
         "Reuni√£o pra saber como roubar mais, desgoverno vergonhoso,cpilantra mentiroso! S√≥ engana trouxa",
         "@andersoncustodiooliveira1515",
         "https://yt3.ggpht.com/ytc/AIdro_m5pluPevjReHKOJz4FTIetVouLOTx7SHNXOqjqli0=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@andersoncustodiooliveira1515",
         "UCc7hcy4jmptDescrLGRHkeA",
         "True",
         "none",
         "0.0",
         "2023-06-15 21:35:26+00:00",
         "2023-06-15 21:35:26+00:00",
         null,
         null,
         null,
         null,
         "Lula volta a fazer piada com obesidade de Fl√°vio Dino: ‚ÄúTraz pouca comida para ele‚Äù",
         "pt"
        ]
       ],
       "shape": {
        "columns": 20,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>viewerRating</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>2024-12-28 21:38:37+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1DN4904BQw</td>\n",
       "      <td>UCbDy7ap3Ixk45DILe4O6Tbw</td>\n",
       "      <td>-1DN4904BQw</td>\n",
       "      <td>Aula gratuita: https://bit.ly/3RLbmWq</td>\n",
       "      <td>Aula gratuita: https://bit.ly/3RLbmWq</td>\n",
       "      <td>@sejasaudavel5167</td>\n",
       "      <td>https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJn...</td>\n",
       "      <td>http://www.youtube.com/@sejasaudavel5167</td>\n",
       "      <td>UCbDy7ap3Ixk45DILe4O6Tbw</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-10-06 20:41:16+00:00</td>\n",
       "      <td>2022-10-06 20:41:16+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O pa√≠s mais obeso do mundo #shorts</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4xj_teI1EQ</td>\n",
       "      <td>UCVIpR5_iHUkkpAPBkw24yDQ</td>\n",
       "      <td>-4xj_teI1EQ</td>\n",
       "      <td>Vc √© linda e sua auto-estima √© contagiante. Se...</td>\n",
       "      <td>Vc √© linda e sua auto-estima √© contagiante. Se...</td>\n",
       "      <td>@isabelitacorrea2611</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkX...</td>\n",
       "      <td>http://www.youtube.com/@isabelitacorrea2611</td>\n",
       "      <td>UCnxzchRu-oFH4H4SKeF2C-Q</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>176.0</td>\n",
       "      <td>2024-05-02 01:58:41+00:00</td>\n",
       "      <td>2024-05-02 01:58:41+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Preconceitos que eu j√° sofri por ser uma baila...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6Qxw7CpQvQ</td>\n",
       "      <td>UC6cALLZLWQGilBFBB0PWAog</td>\n",
       "      <td>-6Qxw7CpQvQ</td>\n",
       "      <td>V√≠deo completo: https://youtu.be/hnetjD-gje4</td>\n",
       "      <td>V√≠deo completo: https://youtu.be/hnetjD-gje4</td>\n",
       "      <td></td>\n",
       "      <td>https://yt3.ggpht.com/xXlZxbOOYCKigMGIaVKMpvi1...</td>\n",
       "      <td>http://www.youtube.com/c/MaiconK%C3%BCster</td>\n",
       "      <td>UC6cALLZLWQGilBFBB0PWAog</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>436.0</td>\n",
       "      <td>2023-01-26 00:25:05+00:00</td>\n",
       "      <td>2023-01-26 00:25:05+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>esse milion√°rio de 18 anos n√£o quer pegar mulh...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7fJRjz1BCM</td>\n",
       "      <td>UC9mdw2mmn49ZuqGOpSri7Fw</td>\n",
       "      <td>-7fJRjz1BCM</td>\n",
       "      <td>Reuni√£o pra saber como roubar mais, desgoverno...</td>\n",
       "      <td>Reuni√£o pra saber como roubar mais, desgoverno...</td>\n",
       "      <td>@andersoncustodiooliveira1515</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_m5pluPevjReHKO...</td>\n",
       "      <td>http://www.youtube.com/@andersoncustodioolivei...</td>\n",
       "      <td>UCc7hcy4jmptDescrLGRHkeA</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-06-15 21:35:26+00:00</td>\n",
       "      <td>2023-06-15 21:35:26+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Lula volta a fazer piada com obesidade de Fl√°v...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  -1DN4904BQw  UCbDy7ap3Ixk45DILe4O6Tbw  -1DN4904BQw   \n",
       "2  -4xj_teI1EQ  UCVIpR5_iHUkkpAPBkw24yDQ  -4xj_teI1EQ   \n",
       "3  -6Qxw7CpQvQ  UC6cALLZLWQGilBFBB0PWAog  -6Qxw7CpQvQ   \n",
       "4  -7fJRjz1BCM  UC9mdw2mmn49ZuqGOpSri7Fw  -7fJRjz1BCM   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1              Aula gratuita: https://bit.ly/3RLbmWq   \n",
       "2  Vc √© linda e sua auto-estima √© contagiante. Se...   \n",
       "3       V√≠deo completo: https://youtu.be/hnetjD-gje4   \n",
       "4  Reuni√£o pra saber como roubar mais, desgoverno...   \n",
       "\n",
       "                                        textOriginal  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1              Aula gratuita: https://bit.ly/3RLbmWq   \n",
       "2  Vc √© linda e sua auto-estima √© contagiante. Se...   \n",
       "3       V√≠deo completo: https://youtu.be/hnetjD-gje4   \n",
       "4  Reuni√£o pra saber como roubar mais, desgoverno...   \n",
       "\n",
       "               authorDisplayName  \\\n",
       "0              @evelynsoares4467   \n",
       "1              @sejasaudavel5167   \n",
       "2           @isabelitacorrea2611   \n",
       "3                                  \n",
       "4  @andersoncustodiooliveira1515   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJn...   \n",
       "2  https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkX...   \n",
       "3  https://yt3.ggpht.com/xXlZxbOOYCKigMGIaVKMpvi1...   \n",
       "4  https://yt3.ggpht.com/ytc/AIdro_m5pluPevjReHKO...   \n",
       "\n",
       "                                    authorChannelUrl  \\\n",
       "0           http://www.youtube.com/@evelynsoares4467   \n",
       "1           http://www.youtube.com/@sejasaudavel5167   \n",
       "2        http://www.youtube.com/@isabelitacorrea2611   \n",
       "3         http://www.youtube.com/c/MaiconK%C3%BCster   \n",
       "4  http://www.youtube.com/@andersoncustodioolivei...   \n",
       "\n",
       "            authorChannelId  canRate viewerRating  likeCount  \\\n",
       "0  UCNhXx9ev5RtEiyGsVjMuTOA     True         none        0.0   \n",
       "1  UCbDy7ap3Ixk45DILe4O6Tbw     True         none        1.0   \n",
       "2  UCnxzchRu-oFH4H4SKeF2C-Q     True         none      176.0   \n",
       "3  UC6cALLZLWQGilBFBB0PWAog     True         none      436.0   \n",
       "4  UCc7hcy4jmptDescrLGRHkeA     True         none        0.0   \n",
       "\n",
       "                publishedAt                 updatedAt author comment  date  \\\n",
       "0 2024-12-28 21:38:37+00:00 2024-12-28 21:38:37+00:00   None    None  None   \n",
       "1 2022-10-06 20:41:16+00:00 2022-10-06 20:41:16+00:00   None    None  None   \n",
       "2 2024-05-02 01:58:41+00:00 2024-05-02 01:58:41+00:00   None    None  None   \n",
       "3 2023-01-26 00:25:05+00:00 2023-01-26 00:25:05+00:00   None    None  None   \n",
       "4 2023-06-15 21:35:26+00:00 2023-06-15 21:35:26+00:00   None    None  None   \n",
       "\n",
       "  likes                                        video_title language  \n",
       "0  None                 Tony Gordo √© Incriminado #simpsons       pt  \n",
       "1  None                 O pa√≠s mais obeso do mundo #shorts       pt  \n",
       "2  None  Preconceitos que eu j√° sofri por ser uma baila...       pt  \n",
       "3  None  esse milion√°rio de 18 anos n√£o quer pegar mulh...       pt  \n",
       "4  None  Lula volta a fazer piada com obesidade de Fl√°v...       pt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. Configuration and Data Loading\n",
    "\n",
    "\"\"\"\n",
    "This section defines the configuration parameters and loads the video title dataset for classification.\n",
    "The configuration class centralizes all parameters for reproducible research.\n",
    "\"\"\"\n",
    "\n",
    "class TitleClassificationConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for YouTube video title classification pipeline.\n",
    "    \n",
    "    Centralizes all configuration parameters for reproducible research and\n",
    "    provides comprehensive validation of the research environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model configuration for OpenAI API\n",
    "    MODEL_NAME = \"gpt-4.1-mini\"\n",
    "    TEMPERATURE = 0.0  # Deterministic outputs for research reproducibility\n",
    "    \n",
    "    # Data paths and file management\n",
    "    DATA_DIR = Path(\"../data\")\n",
    "    INPUT_FILE = DATA_DIR / \"intermediate\" / \"20250417_youtube_comments_pt_cleaned1.parquet\"\n",
    "    \n",
    "    # Batch processing configuration\n",
    "    BATCH_SIZE = 40000  # Optimized for API limits and processing efficiency\n",
    "    JSONL_DIR = DATA_DIR / \"intermediate\" / \"jsonl\"\n",
    "    BATCH_NAME_PREFIX = \"20250417_youtube_titles_batch_api\"\n",
    "    \n",
    "    # Output files for research data\n",
    "    RESULTS_FILE = DATA_DIR / \"tmp\" / \"results_titles.joblib\"\n",
    "    PARSED_RESULTS_FILE = DATA_DIR / \"tmp\" / \"parsed_results_titles.joblib\"\n",
    "    FINAL_OUTPUT_FILE = DATA_DIR / \"intermediate\" / \"20250417_youtube_titles_yes_labels.parquet\"\n",
    "    \n",
    "    # Research parameters\n",
    "    LANGUAGE_FILTER = \"pt\"  # Focus on Portuguese content for Brazilian research context\n",
    "    \n",
    "    @classmethod\n",
    "    def ensure_directories(cls) -> None:\n",
    "        \"\"\"Create necessary directories if they don't exist.\"\"\"\n",
    "        directories = [cls.JSONL_DIR, cls.DATA_DIR / \"tmp\", cls.DATA_DIR / \"intermediate\"]\n",
    "        \n",
    "        for directory in directories:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "        print(\"üìÅ Directory structure verified and created\")\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_configuration(cls) -> None:\n",
    "        \"\"\"Validate configuration parameters and environment.\"\"\"\n",
    "        print(\"üîç Configuration Validation:\")\n",
    "        \n",
    "        # Check input file existence\n",
    "        if cls.INPUT_FILE.exists():\n",
    "            file_size = cls.INPUT_FILE.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ‚úÖ Input file found: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Input file missing: {cls.INPUT_FILE}\")\n",
    "        \n",
    "        # Validate model configuration\n",
    "        print(f\"  ‚úÖ Model: {cls.MODEL_NAME}\")\n",
    "        print(f\"  ‚úÖ Temperature: {cls.TEMPERATURE} (deterministic)\")\n",
    "        print(f\"  ‚úÖ Batch size: {cls.BATCH_SIZE:,} requests\")\n",
    "        print(f\"  ‚úÖ Language filter: {cls.LANGUAGE_FILTER}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def display_config(cls) -> None:\n",
    "        \"\"\"Display current configuration for research documentation.\"\"\"\n",
    "        print(\"‚öôÔ∏è Video Title Classification Configuration:\")\n",
    "        print(f\"- Model: {cls.MODEL_NAME}\")\n",
    "        print(f\"- Temperature: {cls.TEMPERATURE}\")\n",
    "        print(f\"- Batch size: {cls.BATCH_SIZE:,}\")\n",
    "        print(f\"- Language filter: {cls.LANGUAGE_FILTER}\")\n",
    "        print(f\"- Input file: {cls.INPUT_FILE.name}\")\n",
    "        print(f\"- Output file: {cls.FINAL_OUTPUT_FILE.name}\")\n",
    "\n",
    "# Initialize configuration and validate environment\n",
    "TitleClassificationConfig.ensure_directories()\n",
    "TitleClassificationConfig.validate_configuration()\n",
    "TitleClassificationConfig.display_config()\n",
    "\n",
    "# Load and prepare the dataset\n",
    "print(\"\\nüìä Loading video title dataset...\")\n",
    "df = pd.read_parquet(TitleClassificationConfig.INPUT_FILE)\n",
    "\n",
    "# Remove duplicates to get unique videos for title analysis\n",
    "print(f\"üìà Original dataset: {len(df):,} records\")\n",
    "df.drop_duplicates(subset=[\"video_id\"], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(f\"üìà Unique videos: {len(df):,} records\")\n",
    "\n",
    "# Comprehensive dataset analysis\n",
    "print(f\"\\nüìã Video Title Dataset Overview:\")\n",
    "print(f\"- Total unique videos: {len(df):,}\")\n",
    "print(f\"- Dataset columns: {list(df.columns)}\")\n",
    "\n",
    "if 'video_title' in df.columns:\n",
    "    # Title content analysis\n",
    "    title_lengths = df['video_title'].str.len()\n",
    "    non_null_titles = df['video_title'].notna().sum()\n",
    "    \n",
    "    print(f\"- Video title statistics:\")\n",
    "    print(f\"  ‚Ä¢ Non-null titles: {non_null_titles:,} ({non_null_titles/len(df)*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Length range: {title_lengths.min()}-{title_lengths.max()} characters\")\n",
    "    print(f\"  ‚Ä¢ Mean length: {title_lengths.mean():.1f} characters\")\n",
    "    print(f\"  ‚Ä¢ Median length: {title_lengths.median():.1f} characters\")\n",
    "    \n",
    "    # Sample titles for verification\n",
    "    print(f\"\\nüìù Sample Video Titles:\")\n",
    "    sample_titles = df['video_title'].dropna().head(3)\n",
    "    for i, title in enumerate(sample_titles, 1):\n",
    "        preview = title[:80] + \"...\" if len(title) > 80 else title\n",
    "        print(f\"  {i}. {preview}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è video_title column not found in dataset\")\n",
    "\n",
    "print(\"‚úÖ Data loading and preparation completed\")\n",
    "print(f\"üéØ Ready to process {len(df):,} video titles for weight stigma classification\")\n",
    "\n",
    "# Display final dataset structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd20c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Schema Validation for Video Title Classification:\n",
      "- Model name: RespostaAnaliseSentimento\n",
      "- Total fields: 5\n",
      "  ‚úÖ sentimento: string\n",
      "  ‚úÖ gordofobia_implicita: boolean\n",
      "  ‚úÖ gordofobia_explicita: boolean\n",
      "  ‚úÖ idioma: string\n",
      "  ‚úÖ obesidade: boolean\n",
      "\n",
      "üìã Research Classification Dimensions:\n",
      "- Sentiment Analysis: Emotional tone for psychological research\n",
      "- Implicit Weight Stigma: Subtle discrimination patterns\n",
      "- Explicit Weight Stigma: Direct discrimination language\n",
      "- Language Detection: Cultural and linguistic analysis\n",
      "- Obesity Content: Health communication research\n",
      "\n",
      "üéØ Research Applications:\n",
      "- Digital discrimination analysis in Brazilian Portuguese content\n",
      "- Cross-platform weight stigma comparison (titles vs. comments vs. transcriptions)\n",
      "- Content framing analysis in health communication\n",
      "- Sentiment trends in obesity-related video content\n",
      "\n",
      "üìù Sample Classification Structure:\n",
      "Sample output: {'sentimento': 'neutro', 'gordofobia_implicita': False, 'gordofobia_explicita': False, 'idioma': 'pt', 'obesidade': True}\n",
      "\n",
      "üìä Schema Metadata:\n",
      "- Schema format: JSON Schema compatible\n",
      "- API integration: OpenAI function calling\n",
      "- Validation: Pydantic v2 with type checking\n",
      "‚úÖ Schema validation and testing completed successfully\n"
     ]
    }
   ],
   "source": [
    "## 3. Data Schema and Classification Model\n",
    "\n",
    "\"\"\"\n",
    "This section defines the structured output schema for video title classification using Pydantic models.\n",
    "The schema ensures consistent, validated outputs suitable for research analysis and statistical processing.\n",
    "\"\"\"\n",
    "class RespostaAnaliseSentimento(BaseModel):\n",
    "    \"\"\"A resposta de uma fun√ß√£o que realiza an√°lise de sentimento em texto e detec√ß√£o do idioma do texto.\"\"\"\n",
    "\n",
    "    # O r√≥tulo de sentimento atribu√≠do ao texto\n",
    "    sentimento: Literal[\"positivo\", \"negativo\", \"neutro\"] = Field(\n",
    "        default_factory=str,\n",
    "        description=\"O r√≥tulo de sentimento atribu√≠do ao texto. Voc√™ s√≥ pode ter 'positivo', 'negativo' ou 'neutro' como valores.\",\n",
    "    )\n",
    "\n",
    "    gordofobia_implicita: bool = Field(\n",
    "        default_factory=bool,\n",
    "        description=\"Se o texto cont√©m discrimina√ß√£o por peso (gordofobia) de forma impl√≠cita e/ou indireta. Se n√£o houver gordofobia, este campo deve ser False.\",\n",
    "    )\n",
    "\n",
    "    gordofobia_explicita: bool = Field(\n",
    "        default_factory=bool,\n",
    "        description=\"Se o texto cont√©m discrimina√ß√£o por peso (gordofobia) de forma expl√≠cita e/ou direta. Se n√£o houver gordofobia, este campo deve ser False.\",\n",
    "    )\n",
    "\n",
    "    # O idioma detectado no texto\n",
    "    idioma: str = Field(\n",
    "        default_factory=str,\n",
    "        description=\"O idioma detectado no texto, representado por um c√≥digo de idioma de duas letras.\",\n",
    "    )\n",
    "\n",
    "    obesidade: bool = Field(\n",
    "        default_factory=bool,\n",
    "        description=\"Se o texto toca no assunto de obesidade. Se n√£o houver men√ß√£o √† obesidade, este campo deve ser False.\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic configuration for the model.\"\"\"\n",
    "\n",
    "        json_encoders = {\n",
    "            # Custom encoders if needed\n",
    "        }\n",
    "\n",
    "def validate_schema_structure() -> None:\n",
    "    \"\"\"Validate the Pydantic schema structure for research requirements.\"\"\"\n",
    "    schema = RespostaAnaliseSentimento.model_json_schema()\n",
    "    \n",
    "    print(\"üîç Schema Validation for Video Title Classification:\")\n",
    "    print(f\"- Model name: {RespostaAnaliseSentimento.__name__}\")\n",
    "    print(f\"- Total fields: {len(schema['properties'])}\")\n",
    "    \n",
    "    # Validate required fields for research completeness\n",
    "    required_fields = [\"sentimento\", \"gordofobia_implicita\", \"gordofobia_explicita\", \"idioma\", \"obesidade\"]\n",
    "    for field in required_fields:\n",
    "        if field in schema['properties']:\n",
    "            field_type = schema['properties'][field].get('type', 'unknown')\n",
    "            print(f\"  ‚úÖ {field}: {field_type}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {field}: MISSING\")\n",
    "    \n",
    "    print(\"\\nüìã Research Classification Dimensions:\")\n",
    "    print(\"- Sentiment Analysis: Emotional tone for psychological research\")\n",
    "    print(\"- Implicit Weight Stigma: Subtle discrimination patterns\")\n",
    "    print(\"- Explicit Weight Stigma: Direct discrimination language\")\n",
    "    print(\"- Language Detection: Cultural and linguistic analysis\")\n",
    "    print(\"- Obesity Content: Health communication research\")\n",
    "    \n",
    "    print(\"\\nüéØ Research Applications:\")\n",
    "    print(\"- Digital discrimination analysis in Brazilian Portuguese content\")\n",
    "    print(\"- Cross-platform weight stigma comparison (titles vs. comments vs. transcriptions)\")\n",
    "    print(\"- Content framing analysis in health communication\")\n",
    "    print(\"- Sentiment trends in obesity-related video content\")\n",
    "\n",
    "def create_sample_classification() -> RespostaAnaliseSentimento:\n",
    "    \"\"\"Create a sample classification for testing and documentation.\"\"\"\n",
    "    sample_response = RespostaAnaliseSentimento(\n",
    "        sentimento=\"neutro\",\n",
    "        gordofobia_implicita=False,\n",
    "        gordofobia_explicita=False,\n",
    "        idioma=\"pt\",\n",
    "        obesidade=True\n",
    "    )\n",
    "    return sample_response\n",
    "\n",
    "# Validate the schema structure\n",
    "validate_schema_structure()\n",
    "\n",
    "# Create and display sample classification\n",
    "sample_response = create_sample_classification()\n",
    "print(f\"\\nüìù Sample Classification Structure:\")\n",
    "print(f\"Sample output: {sample_response.model_dump()}\")\n",
    "\n",
    "# Verify schema JSON format for API integration\n",
    "schema_json = RespostaAnaliseSentimento.model_json_schema()\n",
    "print(f\"\\nüìä Schema Metadata:\")\n",
    "print(f\"- Schema format: JSON Schema compatible\")\n",
    "print(f\"- API integration: OpenAI function calling\")\n",
    "print(f\"- Validation: Pydantic v2 with type checking\")\n",
    "\n",
    "print(\"‚úÖ Schema validation and testing completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec1ad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System prompt configured\n",
      "üìù Prompt length: 1442 characters\n",
      "üéØ Classification targets: sentiment, gordofobia, language, obesity content\n",
      "üîç System Prompt Validation for Video Title Classification:\n",
      "- Prompt type: system\n",
      "- Content length: 1442 characters\n",
      "- Word count: 179 words\n",
      "\n",
      "üìã Essential Components Check:\n",
      "  ‚úÖ an√°lise de sentimento\n",
      "  ‚úÖ gordofobia\n",
      "  ‚úÖ obesidade\n",
      "  ‚úÖ idioma\n",
      "  ‚úÖ YouTube\n",
      "\n",
      "üéØ Prompt Specialization for Video Titles:\n",
      "- Title-specific analysis (concise, clickbait-aware)\n",
      "- Brazilian/Portuguese cultural context\n",
      "- Weight stigma detection (implicit/explicit patterns)\n",
      "- YouTube platform-specific considerations\n",
      "- Research-grade consistency requirements\n",
      "\n",
      "üìä Research Quality Features:\n",
      "- Comprehensive weight stigma detection guidelines\n",
      "- Cultural sensitivity for Brazilian Portuguese content\n",
      "- Clear examples and classification criteria\n",
      "- Consistency guidelines for reproducible research\n",
      "\n",
      "üìù System Prompt Research Summary:\n",
      "- Purpose: Video title classification for weight stigma research\n",
      "- Target content: Brazilian Portuguese YouTube video titles\n",
      "- Classification dimensions: 5 (sentiment, implicit/explicit stigma, language, obesity content)\n",
      "- Research focus: Digital discrimination in health communication\n",
      "‚úÖ System prompt configuration and validation completed\n"
     ]
    }
   ],
   "source": [
    "## 4. System Prompt and Classification Configuration\n",
    "\n",
    "\"\"\"\n",
    "This section defines the AI system prompt optimized for video title analysis and classification.\n",
    "The prompt is specifically engineered for Brazilian Portuguese content and weight stigma research.\n",
    "\"\"\"\n",
    "\n",
    "def create_video_title_system_prompt() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create specialized system prompt for YouTube video title classification.\n",
    "    \n",
    "    The prompt is designed for:\n",
    "    - Video title-specific content analysis (short, clickbait-style text)\n",
    "    - Brazilian Portuguese cultural context and language patterns\n",
    "    - Weight stigma detection in digital media\n",
    "    - Research-grade classification consistency\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the system prompt configuration\n",
    "    \"\"\"\n",
    "    \n",
    "    # System prompt for zero-shot classification\n",
    "    prompt_text = \"\"\"Voc√™ √© um especialista em an√°lise de sentimento com foco em coment√°rios relacionados a peso corporal e discrimina√ß√£o.\n",
    "\n",
    "    Sua tarefa √© classificar coment√°rios do YouTube com precis√£o, identificando:\n",
    "    1. Sentimento geral (positivo, negativo, neutro)\n",
    "    2. Presen√ßa de gordofobia (discrimina√ß√£o por peso)\n",
    "    3. Idioma do texto\n",
    "    4. Men√ß√µes sobre obesidade\n",
    "\n",
    "    DIRETRIZES DE CLASSIFICA√á√ÉO:\n",
    "\n",
    "    SENTIMENTO:\n",
    "    - 'positivo': Coment√°rios de apoio, encorajamento, aceita√ß√£o corporal, mensagens construtivas\n",
    "    - 'negativo': Cr√≠ticas, julgamentos, discrimina√ß√£o, linguagem ofensiva, gordofobia\n",
    "    - 'neutro': Coment√°rios informativos, quest√µes, observa√ß√µes sem julgamento de valor\n",
    "\n",
    "    GORDOFOBIA:\n",
    "    - Expl√≠cita: Insultos diretos, linguagem claramente discriminat√≥ria, termos pejorativos sobre peso\n",
    "    - Impl√≠cita: Sugest√µes sutis, estere√≥tipos, press√µes indiretas relacionadas ao peso\n",
    "\n",
    "    IDIOMA:\n",
    "    - Use c√≥digos ISO 639-1 (pt, en, es, etc.)\n",
    "    - Considere o idioma predominante se houver mistura\n",
    "\n",
    "    OBESIDADE:\n",
    "    - Marque como True se o coment√°rio menciona ou discute obesidade, mesmo que indiretamente\n",
    "\n",
    "    CONTEXTO IMPORTANTE:\n",
    "    - Considere ironia, sarcasmo e emojis no contexto\n",
    "    - Analise o coment√°rio completo, n√£o apenas palavras isoladas\n",
    "    - Coment√°rios de apoio √† diversidade corporal s√£o positivos\n",
    "    - Seja preciso na detec√ß√£o de discrimina√ß√£o sutil\n",
    "\n",
    "    Responda APENAS com o formato estruturado solicitado.\"\"\"\n",
    "    \n",
    "\n",
    "    print(\"‚úÖ System prompt configured\")\n",
    "    print(f\"üìù Prompt length: {len(prompt_text)} characters\")\n",
    "    print(\"üéØ Classification targets: sentiment, gordofobia, language, obesity content\")\n",
    "    \n",
    "    return {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": prompt_text\n",
    "    }\n",
    "\n",
    "def validate_system_prompt(prompt: Dict[str, str]) -> None:\n",
    "    \"\"\"\n",
    "    Validate the system prompt for research requirements and completeness.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The system prompt dictionary\n",
    "    \"\"\"\n",
    "    print(\"üîç System Prompt Validation for Video Title Classification:\")\n",
    "    print(f\"- Prompt type: {prompt['role']}\")\n",
    "    print(f\"- Content length: {len(prompt['content'])} characters\")\n",
    "    print(f\"- Word count: {len(prompt['content'].split())} words\")\n",
    "    \n",
    "    # Check for key components specific to title analysis\n",
    "    key_components = [\n",
    "        \"an√°lise de sentimento\", \n",
    "        \"gordofobia\",\n",
    "        \"obesidade\",\n",
    "        \"idioma\",\n",
    "        \"YouTube\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüìã Essential Components Check:\")\n",
    "    for component in key_components:\n",
    "        if component.lower() in prompt['content'].lower():\n",
    "            print(f\"  ‚úÖ {component}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {component} - MISSING\")\n",
    "    \n",
    "    print(\"\\nüéØ Prompt Specialization for Video Titles:\")\n",
    "    print(\"- Title-specific analysis (concise, clickbait-aware)\")\n",
    "    print(\"- Brazilian/Portuguese cultural context\")\n",
    "    print(\"- Weight stigma detection (implicit/explicit patterns)\")\n",
    "    print(\"- YouTube platform-specific considerations\")\n",
    "    print(\"- Research-grade consistency requirements\")\n",
    "    \n",
    "    print(\"\\nüìä Research Quality Features:\")\n",
    "    print(\"- Comprehensive weight stigma detection guidelines\")\n",
    "    print(\"- Cultural sensitivity for Brazilian Portuguese content\")\n",
    "    print(\"- Clear examples and classification criteria\")\n",
    "    print(\"- Consistency guidelines for reproducible research\")\n",
    "\n",
    "# Create and validate the system prompt\n",
    "SYSTEM_PROMPT = create_video_title_system_prompt()\n",
    "validate_system_prompt(SYSTEM_PROMPT)\n",
    "\n",
    "# Display prompt summary for research documentation\n",
    "print(f\"\\nüìù System Prompt Research Summary:\")\n",
    "print(f\"- Purpose: Video title classification for weight stigma research\")\n",
    "print(f\"- Target content: Brazilian Portuguese YouTube video titles\")\n",
    "print(f\"- Classification dimensions: 5 (sentiment, implicit/explicit stigma, language, obesity content)\")\n",
    "print(f\"- Research focus: Digital discrimination in health communication\")\n",
    "\n",
    "print(\"‚úÖ System prompt configuration and validation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b912680",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Function Schema Creation and Validation\n",
    "\n",
    "\"\"\"\n",
    "This section creates the OpenAI function schema for structured video title classification.\n",
    "The schema ensures consistent, validated outputs from the AI model suitable for research analysis.\n",
    "\"\"\"\n",
    "\n",
    "def create_function_schema() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create OpenAI function schema from Pydantic model for video title classification.\n",
    "    \n",
    "    This function converts the RespostaAnaliseSentimento Pydantic model into the format\n",
    "    required by OpenAI's function calling API, ensuring structured and validated outputs.\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the function schema for OpenAI API integration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert Pydantic model to OpenAI function format\n",
    "        function_schema = convert_pydantic_to_openai_function(RespostaAnaliseSentimento)\n",
    "        \n",
    "        # Ensure all fields are required for consistent research outputs\n",
    "        function_schema[\"parameters\"][\"required\"] = list(function_schema[\"parameters\"][\"properties\"].keys())\n",
    "        function_schema[\"parameters\"][\"type\"] = \"object\"\n",
    "        \n",
    "        print(\"Function schema created successfully for video title analysis\")\n",
    "        print(f\"Function name: {function_schema['name']}\")\n",
    "        print(f\"Required parameters: {function_schema['parameters']['required']}\")\n",
    "        \n",
    "        return function_schema\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating function schema: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125306d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_function_schema(schema: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Comprehensive validation of the function schema structure and research requirements.\n",
    "    \n",
    "    Args:\n",
    "        schema: The function schema to validate\n",
    "    \"\"\"\n",
    "    print(\"üîç Comprehensive Function Schema Validation:\")\n",
    "    \n",
    "    # Check basic structure\n",
    "    required_keys = [\"name\", \"description\", \"parameters\"]\n",
    "    for key in required_keys:\n",
    "        if key in schema:\n",
    "            print(f\"{key}: Present\")\n",
    "        else:\n",
    "            print(f\"{key}: MISSING - Critical error\")\n",
    "    \n",
    "    # Detailed parameters validation\n",
    "    if \"parameters\" in schema:\n",
    "        params = schema[\"parameters\"]\n",
    "        print(f\"\\nParameters Structure Analysis:\")\n",
    "        print(f\"- Parameter type: {params.get('type', 'Not specified')}\")\n",
    "        print(f\"- Properties count: {len(params.get('properties', {}))}\")\n",
    "        print(f\"- Required fields count: {len(params.get('required', []))}\")\n",
    "        \n",
    "        # Validate research requirements\n",
    "        expected_fields = [\"sentimento\", \"gordofobia_implicita\", \"gordofobia_explicita\", \"idioma\", \"obesidade\"]\n",
    "        properties = params.get('properties', {})\n",
    "        \n",
    "        print(f\"\\nüî¨ Research Field Validation:\")\n",
    "        for field in expected_fields:\n",
    "            if field in properties:\n",
    "                field_type = properties[field].get('type', 'unknown')\n",
    "                print(f\"{field}: {field_type}\")\n",
    "            else:\n",
    "                print(f\"{field}: MISSING - Critical for research\")\n",
    "        \n",
    "        # Validate all properties are required (ensures data completeness)\n",
    "        props_count = len(properties)\n",
    "        required_count = len(params.get('required', []))\n",
    "        \n",
    "        if props_count == required_count:\n",
    "            print(f\"\\n Data Completeness: All {props_count} properties are required\")\n",
    "            print(\"   This ensures consistent data for statistical analysis\")\n",
    "        else:\n",
    "            print(f\"\\n Data Completeness Warning: {props_count} properties, {required_count} required\")\n",
    "            print(\"   This may result in incomplete data for research\")\n",
    "    \n",
    "    print(f\"\\n Schema Integration Analysis:\")\n",
    "    print(\"- API Integration: OpenAI function calling\")\n",
    "    print(\"- Data Validation: Pydantic model validation\")\n",
    "    print(\"- Research Application: Video title classification\")\n",
    "    print(\"- Output Format: Structured JSON for statistical analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26042855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_schema_functionality(schema: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Test the schema functionality with sample data.\n",
    "    \n",
    "    Args:\n",
    "        schema: The function schema to test\n",
    "    \"\"\"\n",
    "    print(\"\\n Schema Functionality Testing:\")\n",
    "    \n",
    "    # Test schema structure\n",
    "    assert \"name\" in schema, \"Schema missing function name\"\n",
    "    assert \"parameters\" in schema, \"Schema missing parameters\"\n",
    "    assert \"properties\" in schema[\"parameters\"], \"Schema missing properties\"\n",
    "    \n",
    "    # Test required fields match expected research requirements\n",
    "    required_fields = schema[\"parameters\"].get(\"required\", [])\n",
    "    expected_count = 5\n",
    "    \n",
    "    if len(required_fields) == expected_count:\n",
    "        print(f\" Required fields count: {len(required_fields)}/{expected_count}\")\n",
    "    else:\n",
    "        print(f\" Required fields mismatch: {len(required_fields)}/{expected_count}\")\n",
    "    \n",
    "    print(\" Schema functionality tests passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baa18926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function schema created successfully for video title analysis\n",
      "Function name: RespostaAnaliseSentimento\n",
      "Required parameters: ['sentimento', 'gordofobia_implicita', 'gordofobia_explicita', 'idioma', 'obesidade']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3533933/3121604808.py:20: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  function_schema = convert_pydantic_to_openai_function(RespostaAnaliseSentimento)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the function schema\n",
    "function_schema = create_function_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec331745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Comprehensive Function Schema Validation:\n",
      "name: Present\n",
      "description: Present\n",
      "parameters: Present\n",
      "\n",
      "Parameters Structure Analysis:\n",
      "- Parameter type: object\n",
      "- Properties count: 5\n",
      "- Required fields count: 5\n",
      "\n",
      "üî¨ Research Field Validation:\n",
      "sentimento: string\n",
      "gordofobia_implicita: boolean\n",
      "gordofobia_explicita: boolean\n",
      "idioma: string\n",
      "obesidade: boolean\n",
      "\n",
      " Data Completeness: All 5 properties are required\n",
      "   This ensures consistent data for statistical analysis\n",
      "\n",
      " Schema Integration Analysis:\n",
      "- API Integration: OpenAI function calling\n",
      "- Data Validation: Pydantic model validation\n",
      "- Research Application: Video title classification\n",
      "- Output Format: Structured JSON for statistical analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comprehensive validation\n",
    "validate_function_schema(function_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d743576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Schema Functionality Testing:\n",
      " Required fields count: 5/5\n",
      " Schema functionality tests passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test functionality\n",
    "test_schema_functionality(function_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b2a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Function Schema Research Summary:\n",
      "- Function name: RespostaAnaliseSentimento\n",
      "- Description: A resposta de uma fun√ß√£o que realiza an√°lise de sentimento em texto e detec√ß√£o do idioma do texto.\n",
      "- Classification fields: 5\n",
      "- Required fields: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display schema summary for research documentation\n",
    "print(f\"\\nFunction Schema Research Summary:\")\n",
    "print(f\"- Function name: {function_schema['name']}\")\n",
    "print(f\"- Description: {function_schema['description']}\")\n",
    "print(f\"- Classification fields: {len(function_schema['parameters']['properties'])}\")\n",
    "print(f\"- Required fields: {len(function_schema['parameters']['required'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee513575",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Data Preparation and Input Processing\n",
    "\n",
    "\"\"\"\n",
    "This section prepares video titles for batch classification processing with comprehensive\n",
    "data validation and quality assessment for research applications.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_title_characteristics(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Analyze characteristics of video title data for research documentation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing video title data\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing video title characteristics for research...\")\n",
    "    \n",
    "    if 'video_title' in df.columns:\n",
    "        titles = df['video_title'].dropna()\n",
    "        \n",
    "        # Length analysis for research insights\n",
    "        lengths = titles.str.len()\n",
    "        print(f\"üìä Title length analysis:\")\n",
    "        print(f\"- Total titles: {len(titles):,}\")\n",
    "        print(f\"- Min length: {lengths.min():,} characters\")\n",
    "        print(f\"- Max length: {lengths.max():,} characters\")\n",
    "        print(f\"- Mean length: {lengths.mean():.1f} characters\")\n",
    "        print(f\"- Median length: {lengths.median():.1f} characters\")\n",
    "        print(f\"- Standard deviation: {lengths.std():.1f} characters\")\n",
    "        \n",
    "        # Title content patterns analysis\n",
    "        print(f\"\\nüìà Content pattern analysis:\")\n",
    "        # Check for common YouTube title patterns\n",
    "        exclamation_count = titles.str.contains('!', na=False).sum()\n",
    "        question_count = titles.str.contains('\\\\?', na=False).sum()\n",
    "        caps_count = titles.str.contains('[A-Z]{3,}', na=False).sum()\n",
    "        number_count = titles.str.contains('\\\\d+', na=False).sum()\n",
    "        \n",
    "        print(f\"- Titles with exclamation marks: {exclamation_count:,} ({exclamation_count/len(titles)*100:.1f}%)\")\n",
    "        print(f\"- Titles with questions: {question_count:,} ({question_count/len(titles)*100:.1f}%)\")\n",
    "        print(f\"- Titles with caps (3+ chars): {caps_count:,} ({caps_count/len(titles)*100:.1f}%)\")\n",
    "        print(f\"- Titles with numbers: {number_count:,} ({number_count/len(titles)*100:.1f}%)\")\n",
    "        \n",
    "        # Data quality assessment\n",
    "        empty_titles = df['video_title'].isna().sum()\n",
    "        very_short_titles = (lengths < 10).sum()\n",
    "        very_long_titles = (lengths > 100).sum()\n",
    "        \n",
    "        print(f\"\\n‚ö†Ô∏è Data quality assessment:\")\n",
    "        print(f\"- Empty titles: {empty_titles:,}\")\n",
    "        print(f\"- Very short titles (<10 chars): {very_short_titles:,}\")\n",
    "        print(f\"- Very long titles (>100 chars): {very_long_titles:,}\")\n",
    "        \n",
    "        if very_short_titles > 0:\n",
    "            print(\"‚ÑπÔ∏è Very short titles may indicate data quality issues\")\n",
    "        if very_long_titles > 0:\n",
    "            print(\"‚ÑπÔ∏è Very long titles may require special handling for API limits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3af7ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_video_titles(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract and prepare video titles for classification with comprehensive validation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing video data\n",
    "        \n",
    "    Returns:\n",
    "        List of video titles ready for processing\n",
    "    \"\"\"\n",
    "    print(f\"üìù Preparing {len(df):,} video titles for classification...\")\n",
    "    \n",
    "    # Extract video titles with validation\n",
    "    if 'video_title' not in df.columns:\n",
    "        raise ValueError(\"video_title column not found in dataset\")\n",
    "    \n",
    "    # Convert to list and handle missing values\n",
    "    input_texts = df.video_title.fillna(\"\").values.tolist()\n",
    "    \n",
    "    # Quality assessment and statistics\n",
    "    non_empty_titles = sum(1 for title in input_texts if title and title.strip())\n",
    "    empty_titles = len(input_texts) - non_empty_titles\n",
    "    \n",
    "    print(f\"üìä Title Preparation Statistics:\")\n",
    "    print(f\"- Total titles processed: {len(input_texts):,}\")\n",
    "    print(f\"- Non-empty titles: {non_empty_titles:,} ({non_empty_titles/len(input_texts)*100:.1f}%)\")\n",
    "    print(f\"- Empty/missing titles: {empty_titles:,} ({empty_titles/len(input_texts)*100:.1f}%)\")\n",
    "    \n",
    "    if empty_titles > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {empty_titles} empty titles - these will be processed as empty strings\")\n",
    "    \n",
    "    # Title length analysis for API planning\n",
    "    valid_titles = [title for title in input_texts if title and title.strip()]\n",
    "    if valid_titles:\n",
    "        title_lengths = [len(title) for title in valid_titles]\n",
    "        avg_length = sum(title_lengths) / len(title_lengths)\n",
    "        max_length = max(title_lengths)\n",
    "        min_length = min(title_lengths)\n",
    "        \n",
    "        print(f\"\\nÔøΩ Title Length Statistics:\")\n",
    "        print(f\"- Average length: {avg_length:.1f} characters\")\n",
    "        print(f\"- Length range: {min_length}-{max_length} characters\")\n",
    "        \n",
    "        # API considerations\n",
    "        long_titles = sum(1 for length in title_lengths if length > 200)\n",
    "        if long_titles > 0:\n",
    "            print(f\"‚ÑπÔ∏è Found {long_titles} titles >200 chars - may need truncation for API efficiency\")\n",
    "    \n",
    "    # Sample titles for research documentation\n",
    "    sample_titles = [title for title in input_texts[:5] if title and title.strip()]\n",
    "    print(f\"\\nüìã Sample Video Titles for Review:\")\n",
    "    for i, title in enumerate(sample_titles, 1):\n",
    "        preview = title[:100] + \"...\" if len(title) > 100 else title\n",
    "        print(f\"  {i}. {preview}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully prepared {len(input_texts):,} video titles for classification\")\n",
    "    print(\"üéØ Ready for batch request creation and API processing\")\n",
    "    \n",
    "    return input_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043a0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_prepared_data(input_texts: List[str], original_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Validate prepared data for research integrity.\n",
    "    \n",
    "    Args:\n",
    "        input_texts: Prepared list of titles\n",
    "        original_df: Original DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üîç Validating prepared data for research integrity...\")\n",
    "    \n",
    "    # Check data consistency\n",
    "    if len(input_texts) != len(original_df):\n",
    "        print(f\"‚ùå Data length mismatch: {len(input_texts)} titles vs {len(original_df)} records\")\n",
    "        raise ValueError(\"Data preparation failed: length mismatch\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Data consistency: {len(input_texts):,} titles match dataset records\")\n",
    "    \n",
    "    # Check for data type consistency\n",
    "    text_types = [type(text).__name__ for text in input_texts[:100]]  # Sample check\n",
    "    if all(t == 'str' for t in text_types):\n",
    "        print(\"‚úÖ Data types: All titles are strings\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Data types: Mixed types detected - may cause API issues\")\n",
    "    \n",
    "    print(\"‚úÖ Data validation completed - ready for API processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d02feaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Analyzing video title characteristics for research...\n",
      "üìä Title length analysis:\n",
      "- Total titles: 1,204\n",
      "- Min length: 10 characters\n",
      "- Max length: 100 characters\n",
      "- Mean length: 63.3 characters\n",
      "- Median length: 64.0 characters\n",
      "- Standard deviation: 23.3 characters\n",
      "\n",
      "üìà Content pattern analysis:\n",
      "- Titles with exclamation marks: 199 (16.5%)\n",
      "- Titles with questions: 163 (13.5%)\n",
      "- Titles with caps (3+ chars): 433 (36.0%)\n",
      "- Titles with numbers: 318 (26.4%)\n",
      "\n",
      "‚ö†Ô∏è Data quality assessment:\n",
      "- Empty titles: 0\n",
      "- Very short titles (<10 chars): 0\n",
      "- Very long titles (>100 chars): 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analyze title characteristics for research documentation\n",
    "analyze_title_characteristics(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7dc3741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Preparing 1,204 video titles for classification...\n",
      "üìä Title Preparation Statistics:\n",
      "- Total titles processed: 1,204\n",
      "- Non-empty titles: 1,204 (100.0%)\n",
      "- Empty/missing titles: 0 (0.0%)\n",
      "\n",
      "ÔøΩ Title Length Statistics:\n",
      "- Average length: 63.3 characters\n",
      "- Length range: 10-100 characters\n",
      "\n",
      "üìã Sample Video Titles for Review:\n",
      "  1. Tony Gordo √© Incriminado #simpsons\n",
      "  2. O pa√≠s mais obeso do mundo #shorts\n",
      "  3. Preconceitos que eu j√° sofri por ser uma bailarina gorda.\n",
      "  4. esse milion√°rio de 18 anos n√£o quer pegar mulher gorda\n",
      "  5. Lula volta a fazer piada com obesidade de Fl√°vio Dino: ‚ÄúTraz pouca comida para ele‚Äù\n",
      "\n",
      "‚úÖ Successfully prepared 1,204 video titles for classification\n",
      "üéØ Ready for batch request creation and API processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare the input data with comprehensive validation\n",
    "input_texts = prepare_video_titles(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "009f4761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating prepared data for research integrity...\n",
      "‚úÖ Data consistency: 1,204 titles match dataset records\n",
      "‚úÖ Data types: All titles are strings\n",
      "‚úÖ Data validation completed - ready for API processing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validate prepared data\n",
    "validate_prepared_data(input_texts, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af8e5bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Batch Request Creation and Processing\n",
    "\n",
    "\"\"\"\n",
    "This section creates batch API requests for efficient large-scale video title classification.\n",
    "The implementation optimizes for research reproducibility and cost-effective processing.\n",
    "\"\"\"\n",
    "\n",
    "def create_batch_requests_for_titles(texts: List[str], df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create batch API requests for video title classification with research-grade validation.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of video titles to classify\n",
    "        df: Original DataFrame for generating unique IDs and validation\n",
    "        \n",
    "    Returns:\n",
    "        List of API request objects ready for batch processing\n",
    "    \"\"\"\n",
    "    print(f\"üîß Creating batch API requests for {len(texts):,} video titles...\")\n",
    "    \n",
    "    if len(texts) != len(df):\n",
    "        raise ValueError(f\"Data mismatch: {len(texts)} texts vs {len(df)} DataFrame records\")\n",
    "    \n",
    "    jsonl_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for idx, text in enumerate(tqdm(texts, desc=\"Creating title classification requests\")):\n",
    "        try:\n",
    "            # Handle empty or invalid titles\n",
    "            if not text or not text.strip():\n",
    "                text = \"[EMPTY_TITLE]\"  # Placeholder for empty titles\n",
    "                skipped_count += 1\n",
    "            \n",
    "            # Create unique identifier using title content and video metadata\n",
    "            custom_uid = f\"title_{df.video_id.iloc[idx]}_{idx}_{text[:50]}\"\n",
    "            request_id = hashlib.md5(custom_uid.encode()).hexdigest()\n",
    "            \n",
    "            # Create API request structure optimized for title classification\n",
    "            request_data = {\n",
    "                \"custom_id\": request_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": TitleClassificationConfig.MODEL_NAME,\n",
    "                    \"temperature\": TitleClassificationConfig.TEMPERATURE,\n",
    "                    \"messages\": [\n",
    "                        SYSTEM_PROMPT,\n",
    "                        {\"role\": \"user\", \"content\": text.encode().decode(\"utf-8\")}\n",
    "                    ],\n",
    "                    \"parallel_tool_calls\": False,\n",
    "                    \"tools\": [{\"type\": \"function\", \"function\": function_schema}],\n",
    "                    \"tool_choice\": {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\"name\": function_schema[\"name\"]}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            jsonl_data.append(request_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error creating request for index {idx}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(jsonl_data):,} API requests for video titles\")\n",
    "    if skipped_count > 0:\n",
    "        print(f\"‚ÑπÔ∏è Processed {skipped_count} empty titles with placeholder text\")\n",
    "    \n",
    "    return jsonl_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "677c390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_into_batches(data: List[Dict[str, Any]], batch_size: int) -> List[List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Split requests into optimally-sized batches for API processing.\n",
    "    \n",
    "    Args:\n",
    "        data: List of API requests\n",
    "        batch_size: Maximum requests per batch (API and cost optimization)\n",
    "        \n",
    "    Returns:\n",
    "        List of batches ready for API submission\n",
    "    \"\"\"\n",
    "    print(f\"üì¶ Splitting {len(data):,} title requests into batches...\")\n",
    "    print(f\"üéØ Target batch size: {batch_size:,} requests per batch\")\n",
    "    \n",
    "    # Create batches\n",
    "    chunks = [data[x:x + batch_size] for x in range(0, len(data), batch_size)]\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunks)} batch(es) for video title processing\")\n",
    "    \n",
    "    # Display batch statistics for research documentation\n",
    "    total_requests = 0\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_size = len(chunk)\n",
    "        total_requests += chunk_size\n",
    "        print(f\"  - Batch {i+1}: {chunk_size:,} requests\")\n",
    "    \n",
    "    print(f\"\\nüìä Batch Processing Summary:\")\n",
    "    print(f\"- Total requests: {total_requests:,}\")\n",
    "    print(f\"- Number of batches: {len(chunks)}\")\n",
    "    print(f\"- Average batch size: {total_requests/len(chunks):.0f} requests\")\n",
    "    print(f\"- Estimated processing time: {len(chunks) * 2:.0f}-{len(chunks) * 10:.0f} minutes\")\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f51c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_batch_structure(chunks: List[List[Dict[str, Any]]]) -> None:\n",
    "    \"\"\"\n",
    "    Validate batch structure for research quality assurance.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of batch chunks to validate\n",
    "    \"\"\"\n",
    "    print(\"üîç Validating batch structure for research quality...\")\n",
    "    \n",
    "    total_requests = sum(len(chunk) for chunk in chunks)\n",
    "    print(f\"üìä Batch Validation Results:\")\n",
    "    print(f\"- Total batches: {len(chunks)}\")\n",
    "    print(f\"- Total requests: {total_requests:,}\")\n",
    "    \n",
    "    # Validate each batch structure\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        sample_request = chunk[0] if chunk else None\n",
    "        if sample_request:\n",
    "            required_keys = [\"custom_id\", \"method\", \"url\", \"body\"]\n",
    "            missing_keys = [key for key in required_keys if key not in sample_request]\n",
    "            \n",
    "            if missing_keys:\n",
    "                print(f\"‚ùå Batch {i+1}: Missing keys {missing_keys}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Batch {i+1}: Structure valid ({len(chunk):,} requests)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Batch {i+1}: Empty batch\")\n",
    "    \n",
    "    print(\"‚úÖ Batch validation completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c0605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating batch API requests for 1,204 video titles...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41f63c5960c4be58c85fb135c5d8902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating title classification requests:   0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 1,204 API requests for video titles\n",
      "üì¶ Splitting 1,204 title requests into batches...\n",
      "üéØ Target batch size: 40,000 requests per batch\n",
      "‚úÖ Created 1 batch(es) for video title processing\n",
      "  - Batch 1: 1,204 requests\n",
      "\n",
      "üìä Batch Processing Summary:\n",
      "- Total requests: 1,204\n",
      "- Number of batches: 1\n",
      "- Average batch size: 1204 requests\n",
      "- Estimated processing time: 2-10 minutes\n",
      "üîç Validating batch structure for research quality...\n",
      "üìä Batch Validation Results:\n",
      "- Total batches: 1\n",
      "- Total requests: 1,204\n",
      "‚úÖ Batch 1: Structure valid (1,204 requests)\n",
      "‚úÖ Batch validation completed\n",
      "\n",
      "üìã Video Title Batch Processing Configuration:\n",
      "- Content type: YouTube video titles (short text)\n",
      "- Model: gpt-4.1-mini\n",
      "- Temperature: 0.0 (deterministic)\n",
      "- Total requests: 1,204\n",
      "- Batch configuration: 1 batches\n",
      "- Research focus: Weight stigma detection in title content\n",
      "\n",
      "üéØ Ready for batch file export and API submission\n",
      "üìù Next step: Export batch files and submit to OpenAI API\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the batch requests with comprehensive validation\n",
    "jsonl_data = create_batch_requests_for_titles(input_texts, df)\n",
    "\n",
    "# Split into optimally-sized batches\n",
    "chunks = split_into_batches(jsonl_data, TitleClassificationConfig.BATCH_SIZE)\n",
    "\n",
    "# Validate batch structure\n",
    "validate_batch_structure(chunks)\n",
    "\n",
    "# Research documentation summary\n",
    "print(f\"\\nüìã Video Title Batch Processing Configuration:\")\n",
    "print(f\"- Content type: YouTube video titles (short text)\")\n",
    "print(f\"- Model: {TitleClassificationConfig.MODEL_NAME}\")\n",
    "print(f\"- Temperature: {TitleClassificationConfig.TEMPERATURE} (deterministic)\")\n",
    "print(f\"- Total requests: {len(jsonl_data):,}\")\n",
    "print(f\"- Batch configuration: {len(chunks)} batches\")\n",
    "print(f\"- Research focus: Weight stigma detection in title content\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for batch file export and API submission\")\n",
    "print(\"üìù Next step: Export batch files and submit to OpenAI API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb3ab765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Exporting 1 batch files for video title classification...\n",
      "‚úÖ Created 20250417_youtube_titles_batch_api_0.jsonl: 3.60 MB (1,204 requests)\n",
      "\n",
      "üìä Batch File Export Summary:\n",
      "- Total files created: 1\n",
      "- Total file size: 3.60 MB\n",
      "- Average file size: 3.60 MB\n",
      "- Export directory: ../data/intermediate/jsonl\n",
      "\n",
      "üìÅ Video Title Batch Files Ready:\n",
      "- 20250417_youtube_titles_batch_api_0.jsonl: 3.60 MB (1,204 requests)\n",
      "\n",
      "üéØ Submitting 1 batch files to OpenAI API...\n",
      "üöÄ Initializing batch processors for 1 video title files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a7b0bb01ae40f5bff968f422175464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Submitting title classification batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted batch titles_e2e74fd9 with id batch_6883b30ad0e88190a2ca45f321c264f2\n",
      "Batch info saved to ../data/intermediate/jsonl/20250417_youtube_titles_batch_api_0_20250725_133835.txt\n",
      "‚úÖ Submitted batch: 20250417_youtube_titles_batch_api_0.jsonl (ID: titles_e2e74fd9)\n",
      "\n",
      "üìä Batch Submission Summary:\n",
      "- Total files processed: 1\n",
      "- Successful submissions: 1\n",
      "- Failed submissions: 0\n",
      "- Active batch processors: 1\n",
      "‚úÖ All video title batches submitted successfully\n",
      "üìä Checking initial video title batch status...\n",
      "- 20250417_youtube_titles_batch_api_0.jsonl: validating\n",
      "\n",
      "üìä Batch Status Summary:\n",
      "- validating: 1 batch(es)\n"
     ]
    }
   ],
   "source": [
    "## 8. Batch File Export and API Submission\n",
    "\n",
    "\"\"\"\n",
    "This section handles the export of batch files and submission to OpenAI's batch API\n",
    "for efficient large-scale video title classification processing.\n",
    "\"\"\"\n",
    "\n",
    "def export_batch_files(chunks: List[List[Dict[str, Any]]], base_filename: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Export batch requests to JSONL files for API processing with comprehensive validation.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of batch chunks to export\n",
    "        base_filename: Base filename for the batch files\n",
    "        \n",
    "    Returns:\n",
    "        List of created file paths for API submission\n",
    "    \"\"\"\n",
    "    print(f\"üíæ Exporting {len(chunks)} batch files for video title classification...\")\n",
    "    \n",
    "    created_files = []\n",
    "    total_size_mb = 0\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        filename = f\"{base_filename}_{idx}.jsonl\"\n",
    "        filepath = TitleClassificationConfig.JSONL_DIR / filename\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in chunk:\n",
    "                    # Ensure proper JSON formatting\n",
    "                    json_line = json.dumps(item, ensure_ascii=False, separators=(',', ':'))\n",
    "                    f.write(json_line + \"\\n\")\n",
    "            \n",
    "            # Verify file creation and calculate statistics\n",
    "            file_size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "            total_size_mb += file_size_mb\n",
    "            \n",
    "            print(f\"‚úÖ Created {filename}: {file_size_mb:.2f} MB ({len(chunk):,} requests)\")\n",
    "            created_files.append(str(filepath))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating {filename}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    print(f\"\\nüìä Batch File Export Summary:\")\n",
    "    print(f\"- Total files created: {len(created_files)}\")\n",
    "    print(f\"- Total file size: {total_size_mb:.2f} MB\")\n",
    "    print(f\"- Average file size: {total_size_mb/len(created_files):.2f} MB\")\n",
    "    print(f\"- Export directory: {TitleClassificationConfig.JSONL_DIR}\")\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "def get_file_hash(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate MD5 hash of a file for unique batch naming and tracking.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the file\n",
    "        \n",
    "    Returns:\n",
    "        MD5 hash string for unique identification\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return hashlib.md5(f.read()).hexdigest()\n",
    "\n",
    "def initialize_batch_processors(file_paths: List[str]) -> Dict[str, OpenAIBatchProcessor]:\n",
    "    \"\"\"\n",
    "    Initialize and submit batch jobs for video title processing with comprehensive tracking.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of JSONL file paths to process\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping file paths to batch processors for monitoring\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Initializing batch processors for {len(file_paths)} video title files...\")\n",
    "    \n",
    "    batch_processors = {}\n",
    "    successful_submissions = 0\n",
    "    \n",
    "    for file_path in tqdm(file_paths, desc=\"Submitting title classification batches\"):\n",
    "        try:\n",
    "            # Create processor and submit job\n",
    "            processor = OpenAIBatchProcessor()\n",
    "            batch_name = f\"titles_{get_file_hash(file_path)[:8]}\"\n",
    "            \n",
    "            processor.submit_batch_job(\n",
    "                input_jsonl_path=file_path,\n",
    "                batch_name=batch_name\n",
    "            )\n",
    "            \n",
    "            batch_processors[file_path] = processor\n",
    "            successful_submissions += 1\n",
    "            \n",
    "            filename = Path(file_path).name\n",
    "            print(f\"‚úÖ Submitted batch: {filename} (ID: {batch_name})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error submitting batch for {Path(file_path).name}: {e}\")\n",
    "            # Continue with other files rather than failing completely\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nüìä Batch Submission Summary:\")\n",
    "    print(f\"- Total files processed: {len(file_paths)}\")\n",
    "    print(f\"- Successful submissions: {successful_submissions}\")\n",
    "    print(f\"- Failed submissions: {len(file_paths) - successful_submissions}\")\n",
    "    print(f\"- Active batch processors: {len(batch_processors)}\")\n",
    "    \n",
    "    if successful_submissions < len(file_paths):\n",
    "        print(\"‚ö†Ô∏è Some batch submissions failed - check error messages above\")\n",
    "    else:\n",
    "        print(\"‚úÖ All video title batches submitted successfully\")\n",
    "    \n",
    "    return batch_processors\n",
    "\n",
    "def monitor_initial_batch_status(processors: Dict[str, OpenAIBatchProcessor]) -> None:\n",
    "    \"\"\"\n",
    "    Monitor initial status of all submitted batch jobs.\n",
    "    \n",
    "    Args:\n",
    "        processors: Dictionary of batch processors to monitor\n",
    "    \"\"\"\n",
    "    print(\"üìä Checking initial video title batch status...\")\n",
    "    \n",
    "    status_summary = {}\n",
    "    \n",
    "    for file_path, processor in processors.items():\n",
    "        try:\n",
    "            batch_info = processor.get_batch_info()\n",
    "            filename = Path(file_path).name\n",
    "            status = batch_info.status\n",
    "            \n",
    "            print(f\"- {filename}: {status}\")\n",
    "            \n",
    "            # Track status for summary\n",
    "            status_summary[status] = status_summary.get(status, 0) + 1\n",
    "            \n",
    "            # Show progress if available\n",
    "            if hasattr(batch_info, 'request_counts') and batch_info.request_counts:\n",
    "                counts = batch_info.request_counts\n",
    "                total = counts.get('total', 0)\n",
    "                completed = counts.get('completed', 0)\n",
    "                if total > 0:\n",
    "                    progress = completed / total * 100\n",
    "                    print(f\"  üìà Progress: {completed}/{total} titles ({progress:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error checking status for {Path(file_path).name}: {e}\")\n",
    "            status_summary['error'] = status_summary.get('error', 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìä Batch Status Summary:\")\n",
    "    for status, count in status_summary.items():\n",
    "        print(f\"- {status}: {count} batch(es)\")\n",
    "\n",
    "# Export batch files for video title classification\n",
    "base_filename = TitleClassificationConfig.BATCH_NAME_PREFIX\n",
    "created_files = export_batch_files(chunks, base_filename)\n",
    "\n",
    "print(f\"\\nüìÅ Video Title Batch Files Ready:\")\n",
    "for file_path in created_files:\n",
    "    file_size = Path(file_path).stat().st_size / (1024 * 1024)\n",
    "    request_count = sum(1 for _ in open(file_path, 'r'))\n",
    "    print(f\"- {Path(file_path).name}: {file_size:.2f} MB ({request_count:,} requests)\")\n",
    "\n",
    "# Initialize batch processors and submit to OpenAI\n",
    "if created_files:\n",
    "    print(f\"\\nüéØ Submitting {len(created_files)} batch files to OpenAI API...\")\n",
    "    batch_processors = initialize_batch_processors(created_files)\n",
    "    \n",
    "    # Monitor initial status\n",
    "    if batch_processors:\n",
    "        monitor_initial_batch_status(batch_processors)\n",
    "    else:\n",
    "        print(\"‚ùå No batch processors created - check API credentials and file format\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No batch files found. Please check batch creation process.\")\n",
    "    batch_processors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a0674af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Video Title Batch Processing Status (19.1 min elapsed):\n",
      "Completed: 1/1 | Failed: 0 | In Progress: 0\n",
      "\n",
      "- 20250417_youtube_titles_batch_api_0.jsonl: completed\n",
      "\n",
      "‚úÖ All video title batches finished!\n",
      "- Completed successfully: 1\n",
      "- Failed: 0\n",
      "- Total processing time: 19.1 minutes\n",
      "üîç Final video title batch completion check:\n",
      "- 20250417_youtube_titles_batch_api_0.jsonl: completed\n",
      "  ‚úÖ Ready for result processing\n",
      "\n",
      "üìä Video Title Processing Summary:\n",
      "- Total batches: 1\n",
      "- Completed successfully: 1\n",
      "- Failed: 0\n",
      "- Other status: 0\n",
      "- Success rate: 100.0%\n",
      "‚úÖ All video title batches completed - ready for results processing\n"
     ]
    }
   ],
   "source": [
    "## 9. Batch Monitoring and Completion\n",
    "\n",
    "\"\"\"\n",
    "This section handles batch job monitoring and completion tracking for video title classification.\n",
    "\"\"\"\n",
    "\n",
    "def wait_for_completion(processors: Dict[str, OpenAIBatchProcessor], check_interval: int = 60) -> None:\n",
    "    \"\"\"\n",
    "    Wait for all video title batch jobs to complete with periodic status updates.\n",
    "    \n",
    "    Args:\n",
    "        processors: Dictionary of batch processors to monitor\n",
    "        check_interval: Seconds between status checks\n",
    "    \"\"\"\n",
    "    if not processors:\n",
    "        print(\"‚ö†Ô∏è No batch processors to monitor\")\n",
    "        return\n",
    "        \n",
    "    print(f\"‚è≥ Waiting for video title batch completion (checking every {check_interval}s)...\")\n",
    "    print(\"‚ÑπÔ∏è Video titles typically process faster than comments or transcriptions due to shorter content\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Check completion status\n",
    "            statuses = []\n",
    "            progress_info = []\n",
    "            \n",
    "            for file_path, processor in processors.items():\n",
    "                batch_info = processor.get_batch_info()\n",
    "                status = batch_info.status\n",
    "                statuses.append(status)\n",
    "                \n",
    "                filename = Path(file_path).name\n",
    "                progress_info.append(f\"- {filename}: {status}\")\n",
    "            \n",
    "            # Calculate completion metrics\n",
    "            completed_count = sum(1 for status in statuses if status == \"completed\")\n",
    "            failed_count = sum(1 for status in statuses if status == \"failed\")\n",
    "            total_count = len(statuses)\n",
    "            \n",
    "            # Clear output and show current status\n",
    "            clear_output(wait=True)\n",
    "            elapsed_time = (time.time() - start_time) / 60\n",
    "            \n",
    "            print(f\"üîÑ Video Title Batch Processing Status ({elapsed_time:.1f} min elapsed):\")\n",
    "            print(f\"Completed: {completed_count}/{total_count} | Failed: {failed_count} | In Progress: {total_count - completed_count - failed_count}\")\n",
    "            print()\n",
    "            \n",
    "            # Show detailed status\n",
    "            for info in progress_info:\n",
    "                print(info)\n",
    "            \n",
    "            # Check if all completed or failed\n",
    "            if completed_count + failed_count == total_count:\n",
    "                print(f\"\\n‚úÖ All video title batches finished!\")\n",
    "                print(f\"- Completed successfully: {completed_count}\")\n",
    "                print(f\"- Failed: {failed_count}\")\n",
    "                print(f\"- Total processing time: {elapsed_time:.1f} minutes\")\n",
    "                break\n",
    "            \n",
    "            # Wait before next check\n",
    "            time.sleep(check_interval)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚ö†Ô∏è Monitoring interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during monitoring: {e}\")\n",
    "            break\n",
    "\n",
    "def check_final_completion_status(processors: Dict[str, OpenAIBatchProcessor]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Check final completion status of all video title batches.\n",
    "    \n",
    "    Args:\n",
    "        processors: Dictionary of batch processors\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with completion statistics\n",
    "    \"\"\"\n",
    "    print(\"üîç Final video title batch completion check:\")\n",
    "    \n",
    "    completed_batches = 0\n",
    "    failed_batches = 0\n",
    "    other_status = 0\n",
    "    \n",
    "    for file_path, processor in processors.items():\n",
    "        try:\n",
    "            batch_info = processor.get_batch_info()\n",
    "            filename = Path(file_path).name\n",
    "            status = batch_info.status\n",
    "            \n",
    "            print(f\"- {filename}: {status}\")\n",
    "            \n",
    "            if status == \"completed\":\n",
    "                completed_batches += 1\n",
    "                print(f\"  ‚úÖ Ready for result processing\")\n",
    "            elif status == \"failed\":\n",
    "                failed_batches += 1\n",
    "                print(f\"  ‚ùå Batch failed - check error details\")\n",
    "            else:\n",
    "                other_status += 1\n",
    "                print(f\"  ‚è≥ Status: {status}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error checking {filename}: {e}\")\n",
    "            failed_batches += 1\n",
    "    \n",
    "    total_batches = len(processors)\n",
    "    success_rate = completed_batches / total_batches * 100 if total_batches > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Video Title Processing Summary:\")\n",
    "    print(f\"- Total batches: {total_batches}\")\n",
    "    print(f\"- Completed successfully: {completed_batches}\")\n",
    "    print(f\"- Failed: {failed_batches}\")\n",
    "    print(f\"- Other status: {other_status}\")\n",
    "    print(f\"- Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    if completed_batches == total_batches:\n",
    "        print(\"‚úÖ All video title batches completed - ready for results processing\")\n",
    "    elif completed_batches > 0:\n",
    "        print(\"‚ö†Ô∏è Partial completion - can process available results\")\n",
    "    else:\n",
    "        print(\"‚ùå No completed batches - check for processing errors\")\n",
    "    \n",
    "    return {\n",
    "        'total': total_batches,\n",
    "        'completed': completed_batches,\n",
    "        'failed': failed_batches,\n",
    "        'other': other_status\n",
    "    }\n",
    "\n",
    "# Start comprehensive batch monitoring\n",
    "print(\"\\nüéØ Starting video title batch monitoring...\")\n",
    "print(\"Note: This will run until all batches are completed or failed.\")\n",
    "print(\"You can interrupt with Ctrl+C if needed.\")\n",
    "\n",
    "# Run monitoring\n",
    "wait_for_completion(batch_processors)\n",
    "\n",
    "# Check final completion status\n",
    "completion_stats = check_final_completion_status(batch_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5875460",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10. Results Processing and Data Integration\n",
    "\n",
    "\"\"\"\n",
    "This section processes batch results and integrates video title classifications \n",
    "with the original dataset for research analysis.\n",
    "\"\"\"\n",
    "\n",
    "def process_batch_results(processors: Dict[str, OpenAIBatchProcessor]) -> Tuple[List[Any], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Process and parse results from completed video title batch jobs.\n",
    "    \n",
    "    Args:\n",
    "        processors: Dictionary of batch processors\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (parsed_results, raw_results) for research analysis\n",
    "    \"\"\"\n",
    "    print(\"üîÑ Processing video title batch results...\")\n",
    "    \n",
    "    parsed_results = []\n",
    "    raw_results = []\n",
    "    error_count = 0\n",
    "    total_processed = 0\n",
    "    \n",
    "    for file_path, processor in processors.items():\n",
    "        filename = Path(file_path).name\n",
    "        print(f\"üìÇ Processing results from {filename}...\")\n",
    "        \n",
    "        try:\n",
    "            # Check batch status first\n",
    "            batch_info = processor.get_batch_info()\n",
    "            if batch_info.status != \"completed\":\n",
    "                print(f\"‚ö†Ô∏è Skipping {filename} - status: {batch_info.status}\")\n",
    "                continue\n",
    "            \n",
    "            # Get batch output\n",
    "            file_response = processor.get_batch_output()\n",
    "            if not file_response:\n",
    "                print(f\"‚ö†Ô∏è No response data for {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Process each response\n",
    "            batch_parsed = 0\n",
    "            batch_errors = 0\n",
    "            \n",
    "            for output in file_response:\n",
    "                total_processed += 1\n",
    "                try:\n",
    "                    # Parse the JSON response\n",
    "                    json_output = json.loads(output)\n",
    "                    function_args = json_output[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"tool_calls\"][0][\"function\"][\"arguments\"]\n",
    "                    parsed_json = json.loads(function_args)\n",
    "                    \n",
    "                    # Validate with Pydantic model\n",
    "                    validated_obj = RespostaAnaliseSentimento.model_validate(parsed_json)\n",
    "                    parsed_results.append(validated_obj)\n",
    "                    raw_results.append(validated_obj.model_dump())\n",
    "                    batch_parsed += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Handle parsing errors gracefully\n",
    "                    parsed_results.append(None)\n",
    "                    raw_results.append(None)\n",
    "                    batch_errors += 1\n",
    "                    error_count += 1\n",
    "                    \n",
    "                    if batch_errors <= 3:  # Show first few errors per batch\n",
    "                        print(f\"‚ö†Ô∏è Parsing error: {str(e)[:100]}\")\n",
    "            \n",
    "            print(f\"  ‚úÖ Parsed: {batch_parsed}, Errors: {batch_errors}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    success_rate = (total_processed - error_count) / total_processed * 100 if total_processed > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Video Title Results Processing Summary:\")\n",
    "    print(f\"- Total responses processed: {total_processed:,}\")\n",
    "    print(f\"- Successfully parsed: {total_processed - error_count:,}\")\n",
    "    print(f\"- Parse errors: {error_count:,}\")\n",
    "    print(f\"- Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"- Final dataset size: {len(parsed_results):,} records\")\n",
    "    \n",
    "    return parsed_results, raw_results\n",
    "\n",
    "def save_intermediate_results(parsed_results: List[Any], raw_results: List[Dict]) -> None:\n",
    "    \"\"\"\n",
    "    Save intermediate results for backup and debugging.\n",
    "    \n",
    "    Args:\n",
    "        parsed_results: List of parsed Pydantic objects\n",
    "        raw_results: List of raw result dictionaries\n",
    "    \"\"\"\n",
    "    print(\"üíæ Saving intermediate video title results...\")\n",
    "    \n",
    "    try:\n",
    "        # Save parsed results\n",
    "        joblib.dump(parsed_results, TitleClassificationConfig.PARSED_RESULTS_FILE)\n",
    "        print(f\"‚úÖ Parsed results saved: {TitleClassificationConfig.PARSED_RESULTS_FILE}\")\n",
    "        \n",
    "        # Save raw results\n",
    "        joblib.dump(raw_results, TitleClassificationConfig.RESULTS_FILE)\n",
    "        print(f\"‚úÖ Raw results saved: {TitleClassificationConfig.RESULTS_FILE}\")\n",
    "        \n",
    "        # Calculate and display file sizes\n",
    "        parsed_size = TitleClassificationConfig.PARSED_RESULTS_FILE.stat().st_size / (1024 * 1024)\n",
    "        raw_size = TitleClassificationConfig.RESULTS_FILE.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"üìä Backup file sizes:\")\n",
    "        print(f\"- Parsed results: {parsed_size:.2f} MB\")\n",
    "        print(f\"- Raw results: {raw_size:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving intermediate results: {e}\")\n",
    "        raise\n",
    "\n",
    "def create_results_dataframe(parsed_results: List[Any]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert parsed results to a structured DataFrame for research analysis.\n",
    "    \n",
    "    Args:\n",
    "        parsed_results: List of parsed Pydantic objects\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with classification results\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating structured results DataFrame...\")\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    for i, parsed_document in enumerate(tqdm(parsed_results, desc=\"Converting results to DataFrame\")):\n",
    "        if parsed_document is not None:\n",
    "            # Extract validated classification data\n",
    "            parsed_dict = parsed_document.model_dump()\n",
    "            outputs.append(parsed_dict)\n",
    "        else:\n",
    "            # Handle null results with default values\n",
    "            outputs.append({\n",
    "                \"sentimento\": None,\n",
    "                \"gordofobia_implicita\": None,\n",
    "                \"gordofobia_explicita\": None,\n",
    "                \"idioma\": None,\n",
    "                \"obesidade\": None,\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame with proper column names\n",
    "    df_results = pd.DataFrame(outputs)\n",
    "    df_results.columns = [\"sentimento\", \"gordofobia_implicita\", \"gordofobia_explicita\", \"idioma\", \"obesidade\"]\n",
    "    \n",
    "    print(f\"‚úÖ Created results DataFrame with {len(df_results):,} records\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nüìà Classification Results Summary:\")\n",
    "    if 'obesidade' in df_results.columns:\n",
    "        obesity_counts = df_results['obesidade'].value_counts()\n",
    "        print(f\"- Obesity content: {obesity_counts.to_dict()}\")\n",
    "    \n",
    "    if 'idioma' in df_results.columns:\n",
    "        language_counts = df_results['idioma'].value_counts()\n",
    "        print(f\"- Languages detected: {language_counts.to_dict()}\")\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a62ff417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing video title batch results...\n",
      "üìÇ Processing results from 20250417_youtube_titles_batch_api_0.jsonl...\n",
      "  ‚úÖ Parsed: 1204, Errors: 0\n",
      "\n",
      "üìä Video Title Results Processing Summary:\n",
      "- Total responses processed: 1,204\n",
      "- Successfully parsed: 1,204\n",
      "- Parse errors: 0\n",
      "- Success rate: 100.0%\n",
      "- Final dataset size: 1,204 records\n",
      "üíæ Saving intermediate video title results...\n",
      "‚úÖ Parsed results saved: ../data/tmp/parsed_results_titles.joblib\n",
      "‚úÖ Raw results saved: ../data/tmp/results_titles.joblib\n",
      "üìä Backup file sizes:\n",
      "- Parsed results: 0.07 MB\n",
      "- Raw results: 0.03 MB\n",
      "üìä Creating structured results DataFrame...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0705c8481b5740ee9bf5879b630229e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting results to DataFrame:   0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created results DataFrame with 1,204 records\n",
      "\n",
      "üìà Classification Results Summary:\n",
      "- Obesity content: {True: 801, False: 403}\n",
      "- Languages detected: {'pt': 1180, 'es': 21, 'en': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process all video title batch results\n",
    "parsed_results, raw_results = process_batch_results(batch_processors)\n",
    "\n",
    "# Save intermediate results for backup\n",
    "save_intermediate_results(parsed_results, raw_results)\n",
    "\n",
    "# Create structured results DataFrame\n",
    "df_results = create_results_dataframe(parsed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4428fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11. Data Integration and Final Export\n",
    "\n",
    "\"\"\"\n",
    "This section integrates the video title classification results with the original dataset\n",
    "and exports the final research-ready dataset with language filtering and validation.\n",
    "\"\"\"\n",
    "\n",
    "def validate_results_consistency(df_results: pd.DataFrame, original_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Validate that results match the original video title data structure.\n",
    "    \n",
    "    Args:\n",
    "        df_results: Classification results DataFrame\n",
    "        original_df: Original video DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üîç Validating video title results consistency...\")\n",
    "    \n",
    "    print(f\"- Original video records: {len(original_df):,}\")\n",
    "    print(f\"- Classification results: {len(df_results):,}\")\n",
    "    \n",
    "    if len(df_results) == len(original_df):\n",
    "        print(\"‚úÖ Result count matches original video title data\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Result count mismatch - check for processing errors\")\n",
    "        print(f\"  Difference: {abs(len(df_results) - len(original_df)):,} records\")\n",
    "    \n",
    "    # Check for null results\n",
    "    null_count = df_results.isnull().any(axis=1).sum()\n",
    "    if null_count > 0:\n",
    "        print(f\"‚ö†Ô∏è Found {null_count} records with null values ({null_count/len(df_results)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚úÖ No null results found\")\n",
    "\n",
    "def integrate_title_data(original_df: pd.DataFrame, results_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Integrate original video data with title classification results.\n",
    "    \n",
    "    Args:\n",
    "        original_df: Original video DataFrame\n",
    "        results_df: Classification results DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with all video and classification data\n",
    "    \"\"\"\n",
    "    print(\"üîó Integrating video data with title classification results...\")\n",
    "    \n",
    "    # Ensure DataFrames have matching indices\n",
    "    original_df_reset = original_df.reset_index(drop=True)\n",
    "    results_df_reset = results_df.reset_index(drop=True)\n",
    "    \n",
    "    # Combine original data with results\n",
    "    df_combined = pd.concat([original_df_reset, results_df_reset], axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Integrated DataFrame with {len(df_combined):,} video title records\")\n",
    "    print(f\"üìä Total columns: {len(df_combined.columns)}\")\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "def apply_language_filter(df_combined: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter dataset to Portuguese content for Brazilian research focus.\n",
    "    \n",
    "    Args:\n",
    "        df_combined: Combined DataFrame with classification results\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame with Portuguese content only\n",
    "    \"\"\"\n",
    "    print(\"üåç Applying language filter for Brazilian Portuguese content...\")\n",
    "    \n",
    "    # Display language distribution before filtering\n",
    "    if 'idioma' in df_combined.columns:\n",
    "        language_dist = df_combined['idioma'].value_counts()\n",
    "        print(f\"üìä Language distribution before filtering:\")\n",
    "        for lang, count in language_dist.items():\n",
    "            percentage = count / len(df_combined) * 100\n",
    "            print(f\"- {lang}: {count:,} records ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Apply Portuguese filter\n",
    "        pt_filter = df_combined['idioma'] == TitleClassificationConfig.LANGUAGE_FILTER\n",
    "        df_filtered = df_combined[pt_filter].copy()\n",
    "        df_filtered.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        filtered_count = len(df_filtered)\n",
    "        original_count = len(df_combined)\n",
    "        retention_rate = filtered_count / original_count * 100\n",
    "        \n",
    "        print(f\"\\nüéØ Language filtering results:\")\n",
    "        print(f\"- Original records: {original_count:,}\")\n",
    "        print(f\"- Portuguese records: {filtered_count:,}\")\n",
    "        print(f\"- Retention rate: {retention_rate:.1f}%\")\n",
    "        print(f\"- Filtered out: {original_count - filtered_count:,} non-Portuguese records\")\n",
    "        \n",
    "        return df_filtered\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No language column found - returning original dataset\")\n",
    "        return df_combined\n",
    "\n",
    "def export_final_dataset(df_final: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Export final video title classification dataset for research use.\n",
    "    \n",
    "    Args:\n",
    "        df_final: Final filtered and integrated DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üíæ Exporting final video title classification dataset...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = TitleClassificationConfig.FINAL_OUTPUT_FILE.parent\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Export to Parquet format\n",
    "        df_final.to_parquet(TitleClassificationConfig.FINAL_OUTPUT_FILE, index=False)\n",
    "        \n",
    "        # Calculate file size\n",
    "        file_size_mb = TitleClassificationConfig.FINAL_OUTPUT_FILE.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"‚úÖ Final dataset exported successfully\")\n",
    "        print(f\"üìÅ File: {TitleClassificationConfig.FINAL_OUTPUT_FILE}\")\n",
    "        print(f\"üìä Size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"üìà Records: {len(df_final):,}\")\n",
    "        print(f\"üìã Columns: {len(df_final.columns)}\")\n",
    "        \n",
    "        # Generate dataset summary\n",
    "        print(f\"\\nüìã Final Dataset Summary:\")\n",
    "        if 'obesidade' in df_final.columns:\n",
    "            obesity_dist = df_final['obesidade'].value_counts()\n",
    "            print(f\"- Obesity content distribution: {obesity_dist.to_dict()}\")\n",
    "        \n",
    "        if 'sentimento' in df_final.columns:\n",
    "            sentiment_dist = df_final['sentimento'].value_counts()\n",
    "            print(f\"- Sentiment distribution: {sentiment_dist.to_dict()}\")\n",
    "        \n",
    "        if 'gordofobia_explicita' in df_final.columns:\n",
    "            explicit_stigma = df_final['gordofobia_explicita'].sum()\n",
    "            print(f\"- Videos with explicit weight stigma: {explicit_stigma:,}\")\n",
    "        \n",
    "        if 'gordofobia_implicita' in df_final.columns:\n",
    "            implicit_stigma = df_final['gordofobia_implicita'].sum()\n",
    "            print(f\"- Videos with implicit weight stigma: {implicit_stigma:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting final dataset: {e}\")\n",
    "        raise\n",
    "\n",
    "def generate_research_summary() -> None:\n",
    "    \"\"\"Generate comprehensive summary for research documentation.\"\"\"\n",
    "    print(\"\\nüéØ Video Title Classification Pipeline Summary\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nüìÅ Research Outputs:\")\n",
    "    print(f\"- Final dataset: {TitleClassificationConfig.FINAL_OUTPUT_FILE}\")\n",
    "    print(f\"- Classification schema: 5 dimensions (sentiment, implicit/explicit stigma, language, obesity content)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c6dcd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating video title results consistency...\n",
      "- Original video records: 1,204\n",
      "- Classification results: 1,204\n",
      "‚úÖ Result count matches original video title data\n",
      "‚úÖ No null results found\n",
      "üîó Integrating video data with title classification results...\n",
      "‚úÖ Integrated DataFrame with 1,204 video title records\n",
      "üìä Total columns: 25\n",
      "üåç Applying language filter for Brazilian Portuguese content...\n",
      "üìä Language distribution before filtering:\n",
      "- pt: 1,180 records (98.0%)\n",
      "- es: 21 records (1.7%)\n",
      "- en: 3 records (0.2%)\n",
      "\n",
      "üéØ Language filtering results:\n",
      "- Original records: 1,204\n",
      "- Portuguese records: 1,180\n",
      "- Retention rate: 98.0%\n",
      "- Filtered out: 24 non-Portuguese records\n",
      "üíæ Exporting final video title classification dataset...\n",
      "‚úÖ Final dataset exported successfully\n",
      "üìÅ File: ../data/intermediate/20250417_youtube_titles_yes_labels.parquet\n",
      "üìä Size: 0.42 MB\n",
      "üìà Records: 1,180\n",
      "üìã Columns: 25\n",
      "\n",
      "üìã Final Dataset Summary:\n",
      "- Obesity content distribution: {True: 797, False: 383}\n",
      "- Sentiment distribution: {'neutro': 640, 'negativo': 288, 'positivo': 252}\n",
      "- Videos with explicit weight stigma: 191\n",
      "- Videos with implicit weight stigma: 124\n",
      "\n",
      "üéØ Video Title Classification Pipeline Summary\n",
      "============================================================\n",
      "\n",
      "üìÅ Research Outputs:\n",
      "- Final dataset: ../data/intermediate/20250417_youtube_titles_yes_labels.parquet\n",
      "- Classification schema: 5 dimensions (sentiment, implicit/explicit stigma, language, obesity content)\n",
      "\n",
      "üìã Sample of Final Video Title Dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "video_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "channelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "videoId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textDisplay",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "textOriginal",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorProfileImageUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelUrl",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "authorChannelId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "canRate",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "viewerRating",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "likeCount",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "publishedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "updatedAt",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "author",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "comment",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "likes",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "video_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "language",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentimento",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "gordofobia_implicita",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "gordofobia_explicita",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "idioma",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "obesidade",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "1c44eab1-e51d-4e31-857c-b30b08ccfdb0",
       "rows": [
        [
         "0",
         "--tK3SaYWr4",
         "UCiV6zQocW4CvWRyXcKDZZmQ",
         "--tK3SaYWr4",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "Haahahahahahahahhahh o pol√≠cia chupando a buda do Romer",
         "@evelynsoares4467",
         "https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29BDzJ8BdXanzXFlgRnNfSJpNAeUx0BZ4=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@evelynsoares4467",
         "UCNhXx9ev5RtEiyGsVjMuTOA",
         "True",
         "none",
         "0.0",
         "2024-12-28 21:38:37+00:00",
         "2024-12-28 21:38:37+00:00",
         null,
         null,
         null,
         null,
         "Tony Gordo √© Incriminado #simpsons",
         "pt",
         "neutro",
         "False",
         "False",
         "pt",
         "False"
        ],
        [
         "1",
         "-1DN4904BQw",
         "UCbDy7ap3Ixk45DILe4O6Tbw",
         "-1DN4904BQw",
         "Aula gratuita: https://bit.ly/3RLbmWq",
         "Aula gratuita: https://bit.ly/3RLbmWq",
         "@sejasaudavel5167",
         "https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJnhxv_fnBhMBnW0RAs0EbYwGirxg_O5haaogRSzXIAEmE5bJkObg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@sejasaudavel5167",
         "UCbDy7ap3Ixk45DILe4O6Tbw",
         "True",
         "none",
         "1.0",
         "2022-10-06 20:41:16+00:00",
         "2022-10-06 20:41:16+00:00",
         null,
         null,
         null,
         null,
         "O pa√≠s mais obeso do mundo #shorts",
         "pt",
         "neutro",
         "False",
         "False",
         "pt",
         "True"
        ],
        [
         "2",
         "-4xj_teI1EQ",
         "UCVIpR5_iHUkkpAPBkw24yDQ",
         "-4xj_teI1EQ",
         "Vc √© linda e sua auto-estima √© contagiante. Seu testemunho vai dar for√ßa pra muitas pessoas! Quem v√™ cara ou corpo n√£o v√™ talento ou supera√ß√£o!",
         "Vc √© linda e sua auto-estima √© contagiante. Seu testemunho vai dar for√ßa pra muitas pessoas! Quem v√™ cara ou corpo n√£o v√™ talento ou supera√ß√£o!",
         "@isabelitacorrea2611",
         "https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkXrkG1-bxGyLdN5Kw-Vb4IQMCNkSM=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@isabelitacorrea2611",
         "UCnxzchRu-oFH4H4SKeF2C-Q",
         "True",
         "none",
         "176.0",
         "2024-05-02 01:58:41+00:00",
         "2024-05-02 01:58:41+00:00",
         null,
         null,
         null,
         null,
         "Preconceitos que eu j√° sofri por ser uma bailarina gorda.",
         "pt",
         "negativo",
         "True",
         "False",
         "pt",
         "False"
        ],
        [
         "3",
         "-6Qxw7CpQvQ",
         "UC6cALLZLWQGilBFBB0PWAog",
         "-6Qxw7CpQvQ",
         "V√≠deo completo: https://youtu.be/hnetjD-gje4",
         "V√≠deo completo: https://youtu.be/hnetjD-gje4",
         "",
         "https://yt3.ggpht.com/xXlZxbOOYCKigMGIaVKMpvi1kLiFsU7fSYMPrJeyS2qNyne1r4SA5iZxjS3mRB5ZWg_IDP8zvbg=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/c/MaiconK%C3%BCster",
         "UC6cALLZLWQGilBFBB0PWAog",
         "True",
         "none",
         "436.0",
         "2023-01-26 00:25:05+00:00",
         "2023-01-26 00:25:05+00:00",
         null,
         null,
         null,
         null,
         "esse milion√°rio de 18 anos n√£o quer pegar mulher gorda",
         "pt",
         "negativo",
         "False",
         "True",
         "pt",
         "False"
        ],
        [
         "4",
         "-7fJRjz1BCM",
         "UC9mdw2mmn49ZuqGOpSri7Fw",
         "-7fJRjz1BCM",
         "Reuni√£o pra saber como roubar mais, desgoverno vergonhoso,cpilantra mentiroso! S√≥ engana trouxa",
         "Reuni√£o pra saber como roubar mais, desgoverno vergonhoso,cpilantra mentiroso! S√≥ engana trouxa",
         "@andersoncustodiooliveira1515",
         "https://yt3.ggpht.com/ytc/AIdro_m5pluPevjReHKOJz4FTIetVouLOTx7SHNXOqjqli0=s48-c-k-c0x00ffffff-no-rj",
         "http://www.youtube.com/@andersoncustodiooliveira1515",
         "UCc7hcy4jmptDescrLGRHkeA",
         "True",
         "none",
         "0.0",
         "2023-06-15 21:35:26+00:00",
         "2023-06-15 21:35:26+00:00",
         null,
         null,
         null,
         null,
         "Lula volta a fazer piada com obesidade de Fl√°vio Dino: ‚ÄúTraz pouca comida para ele‚Äù",
         "pt",
         "negativo",
         "False",
         "True",
         "pt",
         "True"
        ]
       ],
       "shape": {
        "columns": 25,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>textDisplay</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>authorDisplayName</th>\n",
       "      <th>authorProfileImageUrl</th>\n",
       "      <th>authorChannelUrl</th>\n",
       "      <th>authorChannelId</th>\n",
       "      <th>canRate</th>\n",
       "      <th>...</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>video_title</th>\n",
       "      <th>language</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>gordofobia_implicita</th>\n",
       "      <th>gordofobia_explicita</th>\n",
       "      <th>idioma</th>\n",
       "      <th>obesidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>UCiV6zQocW4CvWRyXcKDZZmQ</td>\n",
       "      <td>--tK3SaYWr4</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>Haahahahahahahahhahh o pol√≠cia chupando a buda...</td>\n",
       "      <td>@evelynsoares4467</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...</td>\n",
       "      <td>http://www.youtube.com/@evelynsoares4467</td>\n",
       "      <td>UCNhXx9ev5RtEiyGsVjMuTOA</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Tony Gordo √© Incriminado #simpsons</td>\n",
       "      <td>pt</td>\n",
       "      <td>neutro</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1DN4904BQw</td>\n",
       "      <td>UCbDy7ap3Ixk45DILe4O6Tbw</td>\n",
       "      <td>-1DN4904BQw</td>\n",
       "      <td>Aula gratuita: https://bit.ly/3RLbmWq</td>\n",
       "      <td>Aula gratuita: https://bit.ly/3RLbmWq</td>\n",
       "      <td>@sejasaudavel5167</td>\n",
       "      <td>https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJn...</td>\n",
       "      <td>http://www.youtube.com/@sejasaudavel5167</td>\n",
       "      <td>UCbDy7ap3Ixk45DILe4O6Tbw</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>O pa√≠s mais obeso do mundo #shorts</td>\n",
       "      <td>pt</td>\n",
       "      <td>neutro</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4xj_teI1EQ</td>\n",
       "      <td>UCVIpR5_iHUkkpAPBkw24yDQ</td>\n",
       "      <td>-4xj_teI1EQ</td>\n",
       "      <td>Vc √© linda e sua auto-estima √© contagiante. Se...</td>\n",
       "      <td>Vc √© linda e sua auto-estima √© contagiante. Se...</td>\n",
       "      <td>@isabelitacorrea2611</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkX...</td>\n",
       "      <td>http://www.youtube.com/@isabelitacorrea2611</td>\n",
       "      <td>UCnxzchRu-oFH4H4SKeF2C-Q</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Preconceitos que eu j√° sofri por ser uma baila...</td>\n",
       "      <td>pt</td>\n",
       "      <td>negativo</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6Qxw7CpQvQ</td>\n",
       "      <td>UC6cALLZLWQGilBFBB0PWAog</td>\n",
       "      <td>-6Qxw7CpQvQ</td>\n",
       "      <td>V√≠deo completo: https://youtu.be/hnetjD-gje4</td>\n",
       "      <td>V√≠deo completo: https://youtu.be/hnetjD-gje4</td>\n",
       "      <td></td>\n",
       "      <td>https://yt3.ggpht.com/xXlZxbOOYCKigMGIaVKMpvi1...</td>\n",
       "      <td>http://www.youtube.com/c/MaiconK%C3%BCster</td>\n",
       "      <td>UC6cALLZLWQGilBFBB0PWAog</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>esse milion√°rio de 18 anos n√£o quer pegar mulh...</td>\n",
       "      <td>pt</td>\n",
       "      <td>negativo</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>pt</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7fJRjz1BCM</td>\n",
       "      <td>UC9mdw2mmn49ZuqGOpSri7Fw</td>\n",
       "      <td>-7fJRjz1BCM</td>\n",
       "      <td>Reuni√£o pra saber como roubar mais, desgoverno...</td>\n",
       "      <td>Reuni√£o pra saber como roubar mais, desgoverno...</td>\n",
       "      <td>@andersoncustodiooliveira1515</td>\n",
       "      <td>https://yt3.ggpht.com/ytc/AIdro_m5pluPevjReHKO...</td>\n",
       "      <td>http://www.youtube.com/@andersoncustodioolivei...</td>\n",
       "      <td>UCc7hcy4jmptDescrLGRHkeA</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Lula volta a fazer piada com obesidade de Fl√°v...</td>\n",
       "      <td>pt</td>\n",
       "      <td>negativo</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>pt</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                 channelId      videoId  \\\n",
       "0  --tK3SaYWr4  UCiV6zQocW4CvWRyXcKDZZmQ  --tK3SaYWr4   \n",
       "1  -1DN4904BQw  UCbDy7ap3Ixk45DILe4O6Tbw  -1DN4904BQw   \n",
       "2  -4xj_teI1EQ  UCVIpR5_iHUkkpAPBkw24yDQ  -4xj_teI1EQ   \n",
       "3  -6Qxw7CpQvQ  UC6cALLZLWQGilBFBB0PWAog  -6Qxw7CpQvQ   \n",
       "4  -7fJRjz1BCM  UC9mdw2mmn49ZuqGOpSri7Fw  -7fJRjz1BCM   \n",
       "\n",
       "                                         textDisplay  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1              Aula gratuita: https://bit.ly/3RLbmWq   \n",
       "2  Vc √© linda e sua auto-estima √© contagiante. Se...   \n",
       "3       V√≠deo completo: https://youtu.be/hnetjD-gje4   \n",
       "4  Reuni√£o pra saber como roubar mais, desgoverno...   \n",
       "\n",
       "                                        textOriginal  \\\n",
       "0  Haahahahahahahahhahh o pol√≠cia chupando a buda...   \n",
       "1              Aula gratuita: https://bit.ly/3RLbmWq   \n",
       "2  Vc √© linda e sua auto-estima √© contagiante. Se...   \n",
       "3       V√≠deo completo: https://youtu.be/hnetjD-gje4   \n",
       "4  Reuni√£o pra saber como roubar mais, desgoverno...   \n",
       "\n",
       "               authorDisplayName  \\\n",
       "0              @evelynsoares4467   \n",
       "1              @sejasaudavel5167   \n",
       "2           @isabelitacorrea2611   \n",
       "3                                  \n",
       "4  @andersoncustodiooliveira1515   \n",
       "\n",
       "                               authorProfileImageUrl  \\\n",
       "0  https://yt3.ggpht.com/ytc/AIdro_kTUhLtO25GYE29...   \n",
       "1  https://yt3.ggpht.com/3Uk9AXlL4DHwwOhPTVsJIKJn...   \n",
       "2  https://yt3.ggpht.com/ytc/AIdro_nE2ZHEpUNJCTkX...   \n",
       "3  https://yt3.ggpht.com/xXlZxbOOYCKigMGIaVKMpvi1...   \n",
       "4  https://yt3.ggpht.com/ytc/AIdro_m5pluPevjReHKO...   \n",
       "\n",
       "                                    authorChannelUrl  \\\n",
       "0           http://www.youtube.com/@evelynsoares4467   \n",
       "1           http://www.youtube.com/@sejasaudavel5167   \n",
       "2        http://www.youtube.com/@isabelitacorrea2611   \n",
       "3         http://www.youtube.com/c/MaiconK%C3%BCster   \n",
       "4  http://www.youtube.com/@andersoncustodioolivei...   \n",
       "\n",
       "            authorChannelId  canRate  ... comment  date likes  \\\n",
       "0  UCNhXx9ev5RtEiyGsVjMuTOA     True  ...    None  None  None   \n",
       "1  UCbDy7ap3Ixk45DILe4O6Tbw     True  ...    None  None  None   \n",
       "2  UCnxzchRu-oFH4H4SKeF2C-Q     True  ...    None  None  None   \n",
       "3  UC6cALLZLWQGilBFBB0PWAog     True  ...    None  None  None   \n",
       "4  UCc7hcy4jmptDescrLGRHkeA     True  ...    None  None  None   \n",
       "\n",
       "                                         video_title language sentimento  \\\n",
       "0                 Tony Gordo √© Incriminado #simpsons       pt     neutro   \n",
       "1                 O pa√≠s mais obeso do mundo #shorts       pt     neutro   \n",
       "2  Preconceitos que eu j√° sofri por ser uma baila...       pt   negativo   \n",
       "3  esse milion√°rio de 18 anos n√£o quer pegar mulh...       pt   negativo   \n",
       "4  Lula volta a fazer piada com obesidade de Fl√°v...       pt   negativo   \n",
       "\n",
       "  gordofobia_implicita gordofobia_explicita idioma obesidade  \n",
       "0                False                False     pt     False  \n",
       "1                False                False     pt      True  \n",
       "2                 True                False     pt     False  \n",
       "3                False                 True     pt     False  \n",
       "4                False                 True     pt      True  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Validate results consistency\n",
    "validate_results_consistency(df_results, df)\n",
    "\n",
    "# Integrate original data with classification results\n",
    "df_integrated = integrate_title_data(df, df_results)\n",
    "\n",
    "# Apply language filter for Brazilian Portuguese content\n",
    "df_final = apply_language_filter(df_integrated)\n",
    "\n",
    "# Export final research dataset\n",
    "export_final_dataset(df_final)\n",
    "\n",
    "# Generate research summary\n",
    "generate_research_summary()\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(f\"\\nüìã Sample of Final Video Title Dataset:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed1206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2b98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c13ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper_youtube_weight_stigma_1e733e1bf2b6c34f6bcb8483dce2a479",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
